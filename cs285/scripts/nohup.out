/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:42: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-21 01:11:06.257498: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-21 01:11:06.262429: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-21 01:11:06.396272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:06.396911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563df60d35c0 executing computations on platform CUDA. Devices:
2019-10-21 01:11:06.396954: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-10-21 01:11:06.417745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499615000 Hz
2019-10-21 01:11:06.419256: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563df61e6da0 executing computations on platform Host. Devices:
2019-10-21 01:11:06.419290: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-21 01:11:06.419560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:06.420334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:41:00.0
2019-10-21 01:11:06.420624: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 01:11:06.422006: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 01:11:06.423270: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-21 01:11:06.423560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-21 01:11:06.425198: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-21 01:11:06.426412: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-21 01:11:06.430210: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-21 01:11:06.430399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:06.431086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:06.431576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-21 01:11:06.431620: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 01:11:06.432407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-21 01:11:06.432417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-21 01:11:06.432422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-21 01:11:06.432627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:06.433162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:06.433681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4769 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:41:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py:39: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f53404ddbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f53404ddbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f53404ddbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f53404ddbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f532f022cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f532f022cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:39: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:40: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f53404dd6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f53404dd6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f533b446978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f533b446978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f533b446978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f533b446978>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/critics/bootstrapped_continuous_critic.py:55: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.

WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-10-21 01:11:08.447172: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0



LOGGING TO:  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/ac_10_10_HalfCheetah-v2_21-10-2019_01-11-06 



########################
logging outputs to  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/ac_10_10_HalfCheetah-v2_21-10-2019_01-11-06
########################

Collecting data to be used for training...
Traceback (most recent call last):
  File "run_hw3_actor_critic.py", line 131, in <module>
    main()
  File "run_hw3_actor_critic.py", line 127, in main
    trainer.run_training_loop()
  File "run_hw3_actor_critic.py", line 56, in run_training_loop
    eval_policy = self.rl_trainer.agent.actor,
  File "/home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py", line 139, in run_training_loop
    paths, envsteps_this_batch, train_video_paths = self.collect_training_trajectories(itr, initial_expertdata, collect_policy, self.params['batch_size'])
  File "/home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py", line 201, in collect_training_trajectories
    paths, envsteps_this_batch = sample_trajectories(self.env, collect_policy, batch_size, self.params['ep_len'])
  File "/home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/utils.py", line 78, in sample_trajectories
    path = sample_trajectory(env, policy, max_path_length, render, render_mode)
  File "/home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/utils.py", line 34, in sample_trajectory
    ac = policy.get_action(ob) # TODO: GETTHIS from HW1
  File "/home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py", line 121, in get_action
    return self.sess.run(self.sample_ac, feed_dict={self.observations_pl : observation})
  File "/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 950, in run
    run_metadata_ptr)
  File "/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1350, in _do_run
    run_metadata)
  File "/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1356, in _do_call
    return fn(*args)
  File "/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1429, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:42: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-21 01:11:27.915602: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-21 01:11:27.920654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-21 01:11:28.061040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:28.061695: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55faf74a6670 executing computations on platform CUDA. Devices:
2019-10-21 01:11:28.061719: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-10-21 01:11:28.081727: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499615000 Hz
2019-10-21 01:11:28.083119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55faf75b9de0 executing computations on platform Host. Devices:
2019-10-21 01:11:28.083169: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-21 01:11:28.083395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:28.084033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:41:00.0
2019-10-21 01:11:28.084248: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 01:11:28.085152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 01:11:28.086163: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-21 01:11:28.086369: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-21 01:11:28.087391: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-21 01:11:28.088149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-21 01:11:28.090559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-21 01:11:28.090700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:28.091338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:28.091890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-21 01:11:28.091937: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 01:11:28.092790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-21 01:11:28.092801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-21 01:11:28.092806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-21 01:11:28.093005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:28.093606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:11:28.094170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4769 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:41:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py:39: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6705a15c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6705a15c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6705a15c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6705a15c18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66f29a8cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66f29a8cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:39: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:40: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66fe5c59b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66fe5c59b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66fe5c59b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66fe5c59b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66fe5c59b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f66fe5c59b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/critics/bootstrapped_continuous_critic.py:55: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.

WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-10-21 01:11:30.094427: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:42: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-21 01:13:25.399141: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-21 01:13:25.404060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-21 01:13:25.485820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:13:25.486719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560dc5528b10 executing computations on platform CUDA. Devices:
2019-10-21 01:13:25.486750: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-10-21 01:13:25.509633: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499615000 Hz
2019-10-21 01:13:25.511008: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560dc563c320 executing computations on platform Host. Devices:
2019-10-21 01:13:25.511065: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-21 01:13:25.511353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:13:25.512153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:41:00.0
2019-10-21 01:13:25.512527: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 01:13:25.514414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 01:13:25.516101: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-21 01:13:25.516503: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-21 01:13:25.518662: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-21 01:13:25.520154: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-21 01:13:25.523528: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-21 01:13:25.523745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:13:25.524505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:13:25.525143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-21 01:13:25.525212: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 01:13:25.526311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-21 01:13:25.526325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-21 01:13:25.526330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-21 01:13:25.526613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:13:25.527310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 01:13:25.527978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4769 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:41:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py:39: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d7354e630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d7354e630>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d6a80dc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d6a80dc18>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e34e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e34e0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:39: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/policies/MLP_policy.py:40: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3d734e32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/critics/bootstrapped_continuous_critic.py:55: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.

WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-10-21 01:13:27.613737: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0



LOGGING TO:  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/ac_10_10_HalfCheetah-v2_21-10-2019_01-11-27 



########################
logging outputs to  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/ac_10_10_HalfCheetah-v2_21-10-2019_01-11-27
########################

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -103.5672378540039
Eval_StdReturn : 40.74188995361328
Eval_MaxReturn : -38.04983139038086
Eval_MinReturn : -170.88192749023438
Eval_AverageEpLen : 151.0
Train_AverageReturn : -145.1819610595703
Train_StdReturn : 42.973201751708984
Train_MaxReturn : -44.54351806640625
Train_MinReturn : -279.71429443359375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 30049
TimeSinceStart : 17.578917026519775
Critic_Loss : 1.5633006465435029
Actor_Loss : -13795.6259765625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -76.21788024902344
Eval_StdReturn : 23.50004768371582
Eval_MaxReturn : -32.99031448364258
Eval_MinReturn : -109.59667205810547
Eval_AverageEpLen : 151.0
Train_AverageReturn : -106.33426666259766
Train_StdReturn : 38.04395294189453
Train_MaxReturn : -14.530417442321777
Train_MinReturn : -234.54653930664062
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 60098
TimeSinceStart : 34.78961730003357
Critic_Loss : 1.4022178328037262
Actor_Loss : -13597.5341796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -86.2359619140625
Eval_StdReturn : 34.3157844543457
Eval_MaxReturn : -37.168670654296875
Eval_MinReturn : -152.81150817871094
Eval_AverageEpLen : 151.0
Train_AverageReturn : -88.2629623413086
Train_StdReturn : 34.684486389160156
Train_MaxReturn : -5.631071090698242
Train_MinReturn : -199.513671875
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 90147
TimeSinceStart : 52.07129740715027
Critic_Loss : 1.315552660226822
Actor_Loss : -14445.015625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -71.63164520263672
Eval_StdReturn : 30.64268684387207
Eval_MaxReturn : -21.25295639038086
Eval_MinReturn : -125.09904479980469
Eval_AverageEpLen : 151.0
Train_AverageReturn : -84.39942932128906
Train_StdReturn : 34.97280502319336
Train_MaxReturn : 27.519718170166016
Train_MinReturn : -214.0259246826172
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 120196
TimeSinceStart : 68.76412725448608
Critic_Loss : 1.3407221508026124
Actor_Loss : -13873.263671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -60.766456604003906
Eval_StdReturn : 29.117874145507812
Eval_MaxReturn : -25.9688777923584
Eval_MinReturn : -128.45892333984375
Eval_AverageEpLen : 151.0
Train_AverageReturn : -73.91971588134766
Train_StdReturn : 31.109737396240234
Train_MaxReturn : -0.2780632972717285
Train_MinReturn : -166.5413055419922
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 150245
TimeSinceStart : 85.28983664512634
Critic_Loss : 1.3610399556159973
Actor_Loss : -13635.38671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -70.17965698242188
Eval_StdReturn : 29.454784393310547
Eval_MaxReturn : -4.663627624511719
Eval_MinReturn : -103.4561767578125
Eval_AverageEpLen : 151.0
Train_AverageReturn : -70.11083984375
Train_StdReturn : 32.979835510253906
Train_MaxReturn : 6.763128280639648
Train_MinReturn : -179.79605102539062
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 180294
TimeSinceStart : 101.95978140830994
Critic_Loss : 1.407332297563553
Actor_Loss : -13175.4765625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -51.8753547668457
Eval_StdReturn : 28.23737335205078
Eval_MaxReturn : -15.866759300231934
Eval_MinReturn : -116.78412628173828
Eval_AverageEpLen : 151.0
Train_AverageReturn : -67.865478515625
Train_StdReturn : 28.882373809814453
Train_MaxReturn : 3.937105178833008
Train_MinReturn : -174.76211547851562
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 210343
TimeSinceStart : 118.8583550453186
Critic_Loss : 1.2213788080215453
Actor_Loss : -13483.998046875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -57.72673797607422
Eval_StdReturn : 32.203800201416016
Eval_MaxReturn : -3.317227363586426
Eval_MinReturn : -113.6114501953125
Eval_AverageEpLen : 151.0
Train_AverageReturn : -62.24098205566406
Train_StdReturn : 27.36069679260254
Train_MaxReturn : 2.1334896087646484
Train_MinReturn : -146.43728637695312
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 240392
TimeSinceStart : 139.35779547691345
Critic_Loss : 1.1724279820919037
Actor_Loss : -12723.9658203125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -48.84638214111328
Eval_StdReturn : 16.36901092529297
Eval_MaxReturn : -18.58568000793457
Eval_MinReturn : -81.32908630371094
Eval_AverageEpLen : 151.0
Train_AverageReturn : -54.323978424072266
Train_StdReturn : 24.870113372802734
Train_MaxReturn : 0.44457149505615234
Train_MinReturn : -123.05518341064453
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 270441
TimeSinceStart : 159.8733229637146
Critic_Loss : 1.0297876238822936
Actor_Loss : -13464.701171875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -39.819602966308594
Eval_StdReturn : 19.142356872558594
Eval_MaxReturn : -1.2543643712997437
Eval_MinReturn : -70.35897827148438
Eval_AverageEpLen : 151.0
Train_AverageReturn : -52.173397064208984
Train_StdReturn : 24.073698043823242
Train_MaxReturn : 31.51482582092285
Train_MinReturn : -148.22677612304688
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 300490
TimeSinceStart : 180.11454367637634
Critic_Loss : 0.9555025660991668
Actor_Loss : -12803.224609375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -44.462074279785156
Eval_StdReturn : 12.502490043640137
Eval_MaxReturn : -19.371307373046875
Eval_MinReturn : -66.73250579833984
Eval_AverageEpLen : 151.0
Train_AverageReturn : -46.957252502441406
Train_StdReturn : 25.1678524017334
Train_MaxReturn : 24.912113189697266
Train_MinReturn : -159.78314208984375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 330539
TimeSinceStart : 200.19417881965637
Critic_Loss : 0.9572079080343247
Actor_Loss : -13046.435546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -40.41801452636719
Eval_StdReturn : 18.512006759643555
Eval_MaxReturn : -1.842116355895996
Eval_MinReturn : -71.354736328125
Eval_AverageEpLen : 151.0
Train_AverageReturn : -44.55998229980469
Train_StdReturn : 24.060007095336914
Train_MaxReturn : 36.22835922241211
Train_MinReturn : -113.0351333618164
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 360588
TimeSinceStart : 220.27096104621887
Critic_Loss : 0.8628293216228485
Actor_Loss : -12951.40625
Initial_DataCollection_AverageReturn : -145.1819610595703


LOGGING TO:  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/ac_10_10_InvertedPendulum-v2_21-10-2019_01-13-25 



########################
logging outputs to  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/ac_10_10_InvertedPendulum-v2_21-10-2019_01-13-25
########################

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 19.619047164916992
Eval_StdReturn : 11.265101432800293
Eval_MaxReturn : 46.0
Eval_MinReturn : 4.0
Eval_AverageEpLen : 19.61904761904762
Train_AverageReturn : 7.952305316925049
Train_StdReturn : 4.7417521476745605
Train_MaxReturn : 32.0
Train_MinReturn : 3.0
Train_AverageEpLen : 7.952305246422894
Train_EnvstepsSoFar : 5002
TimeSinceStart : 3.9069440364837646
Critic_Loss : 0.8734982095658779
Actor_Loss : -843.891845703125
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 214.3333282470703
Eval_StdReturn : 41.26607131958008
Eval_MaxReturn : 245.0
Eval_MinReturn : 156.0
Eval_AverageEpLen : 214.33333333333334
Train_AverageReturn : 132.55262756347656
Train_StdReturn : 51.107574462890625
Train_MaxReturn : 334.0
Train_MinReturn : 56.0
Train_AverageEpLen : 132.55263157894737
Train_EnvstepsSoFar : 55203
TimeSinceStart : 37.13963460922241
Critic_Loss : 0.256257226318121
Actor_Loss : -517.142333984375
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 244.6666717529297
Eval_StdReturn : 205.31655883789062
Eval_MaxReturn : 535.0
Eval_MinReturn : 96.0
Eval_AverageEpLen : 244.66666666666666
Train_AverageReturn : 311.8823547363281
Train_StdReturn : 248.22189331054688
Train_MaxReturn : 1000.0
Train_MinReturn : 40.0
Train_AverageEpLen : 311.88235294117646
Train_EnvstepsSoFar : 106851
TimeSinceStart : 70.65028357505798
Critic_Loss : 0.7984635782241821
Actor_Loss : -383.4412841796875
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 614.0
Eval_StdReturn : 386.0
Eval_MaxReturn : 1000.0
Eval_MinReturn : 228.0
Eval_AverageEpLen : 614.0
Train_AverageReturn : 279.5
Train_StdReturn : 84.19900512695312
Train_MaxReturn : 499.0
Train_MinReturn : 175.0
Train_AverageEpLen : 279.5
Train_EnvstepsSoFar : 157959
TimeSinceStart : 104.07862448692322
Critic_Loss : 0.15029294162988663
Actor_Loss : -90.88911437988281
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 1000.0
Eval_StdReturn : 0.0
Eval_MaxReturn : 1000.0
Eval_MinReturn : 1000.0
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1000.0
Train_StdReturn : 0.0
Train_MaxReturn : 1000.0
Train_MinReturn : 1000.0
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 209493
TimeSinceStart : 137.34568095207214
Critic_Loss : 0.3470028644800186
Actor_Loss : -64.69591522216797
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 1000.0
Eval_StdReturn : 0.0
Eval_MaxReturn : 1000.0
Eval_MinReturn : 1000.0
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1000.0
Train_StdReturn : 0.0
Train_MaxReturn : 1000.0
Train_MinReturn : 1000.0
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 259493
TimeSinceStart : 170.19268798828125
Critic_Loss : 0.3441236823797226
Actor_Loss : 50.07241439819336
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 820.0
Eval_StdReturn : 0.0
Eval_MaxReturn : 820.0
Eval_MinReturn : 820.0
Eval_AverageEpLen : 820.0
Train_AverageReturn : 512.5999755859375
Train_StdReturn : 248.6544647216797
Train_MaxReturn : 1000.0
Train_MinReturn : 264.0
Train_AverageEpLen : 512.6
Train_EnvstepsSoFar : 310579
TimeSinceStart : 203.51327395439148
Critic_Loss : 0.14534836381673813
Actor_Loss : -99.08723449707031
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 1000.0
Eval_StdReturn : 0.0
Eval_MaxReturn : 1000.0
Eval_MinReturn : 1000.0
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1000.0
Train_StdReturn : 0.0
Train_MaxReturn : 1000.0
Train_MinReturn : 1000.0
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 360694
TimeSinceStart : 235.232346534729
Critic_Loss : 0.3540536978840828
Actor_Loss : 20.413715362548828
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 1000.0
Eval_StdReturn : 0.0
Eval_MaxReturn : 1000.0
Eval_MinReturn : 1000.0
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1000.0
Train_StdReturn : 0.0
Train_MaxReturn : 1000.0
Train_MinReturn : 1000.0
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 410694
TimeSinceStart : 267.1134395599365
Critic_Loss : 0.34760764360427854
Actor_Loss : 9.300334930419922
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 1000.0
Eval_StdReturn : 0.0
Eval_MaxReturn : 1000.0
Eval_MinReturn : 1000.0
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1000.0
Train_StdReturn : 0.0
Train_MaxReturn : 1000.0
Train_MinReturn : 1000.0
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 460694
TimeSinceStart : 298.931289434433
Critic_Loss : 0.34585054844617846
Actor_Loss : 6.201811790466309
Initial_DataCollection_AverageReturn : 7.952305316925049
Done logging...



Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Collecting data to be used for training...

Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -43.7201042175293
Eval_StdReturn : 32.40104675292969
Eval_MaxReturn : -5.977863311767578
Eval_MinReturn : -125.12712860107422
Eval_AverageEpLen : 151.0
Train_AverageReturn : -40.976802825927734
Train_StdReturn : 22.894203186035156
Train_MaxReturn : 8.794596672058105
Train_MinReturn : -136.13677978515625
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 390637
TimeSinceStart : 240.43392395973206
Critic_Loss : 0.8396414893865586
Actor_Loss : -12934.0126953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -26.14767837524414
Eval_StdReturn : 21.558231353759766
Eval_MaxReturn : -3.8156485557556152
Eval_MinReturn : -74.59685516357422
Eval_AverageEpLen : 151.0
Train_AverageReturn : -32.86625289916992
Train_StdReturn : 20.6549015045166
Train_MaxReturn : 8.643564224243164
Train_MinReturn : -113.3501205444336
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 420686
TimeSinceStart : 260.40275287628174
Critic_Loss : 0.8693638640642166
Actor_Loss : -12799.76953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -38.37858963012695
Eval_StdReturn : 15.980118751525879
Eval_MaxReturn : -21.844980239868164
Eval_MinReturn : -78.49051666259766
Eval_AverageEpLen : 151.0
Train_AverageReturn : -29.5979061126709
Train_StdReturn : 23.62326431274414
Train_MaxReturn : 41.42515182495117
Train_MinReturn : -111.85248565673828
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 450735
TimeSinceStart : 280.87726283073425
Critic_Loss : 0.8186011809110642
Actor_Loss : -11686.263671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -25.005800247192383
Eval_StdReturn : 13.678037643432617
Eval_MaxReturn : -10.71781063079834
Eval_MinReturn : -59.250709533691406
Eval_AverageEpLen : 151.0
Train_AverageReturn : -33.42267990112305
Train_StdReturn : 22.974748611450195
Train_MaxReturn : 34.931983947753906
Train_MinReturn : -102.43775939941406
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 480784
TimeSinceStart : 301.5292503833771
Critic_Loss : 0.8191615962982177
Actor_Loss : -10714.892578125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -30.4915771484375
Eval_StdReturn : 14.683658599853516
Eval_MaxReturn : -3.8070926666259766
Eval_MinReturn : -52.71770477294922
Eval_AverageEpLen : 151.0
Train_AverageReturn : -26.70514678955078
Train_StdReturn : 22.75501823425293
Train_MaxReturn : 33.7828483581543
Train_MinReturn : -76.8673095703125
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 510833
TimeSinceStart : 321.7680130004883
Critic_Loss : 0.7927826005220413
Actor_Loss : -11694.1064453125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -25.747695922851562
Eval_StdReturn : 25.92275619506836
Eval_MaxReturn : 10.099713325500488
Eval_MinReturn : -87.35291290283203
Eval_AverageEpLen : 151.0
Train_AverageReturn : -29.72307777404785
Train_StdReturn : 22.868661880493164
Train_MaxReturn : 32.21015548706055
Train_MinReturn : -103.52006530761719
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 540882
TimeSinceStart : 341.5363132953644
Critic_Loss : 0.7872925424575805
Actor_Loss : -10436.955078125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -27.9693546295166
Eval_StdReturn : 13.838610649108887
Eval_MaxReturn : -0.5692615509033203
Eval_MinReturn : -51.52473449707031
Eval_AverageEpLen : 151.0
Train_AverageReturn : -26.21711540222168
Train_StdReturn : 20.60169219970703
Train_MaxReturn : 42.18821334838867
Train_MinReturn : -80.29789733886719
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 570931
TimeSinceStart : 361.39059710502625
Critic_Loss : 0.7487985879182816
Actor_Loss : -11486.5048828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -37.984901428222656
Eval_StdReturn : 7.164128303527832
Eval_MaxReturn : -22.55694580078125
Eval_MinReturn : -48.21539306640625
Eval_AverageEpLen : 151.0
Train_AverageReturn : -27.853294372558594
Train_StdReturn : 19.225584030151367
Train_MaxReturn : 42.2169303894043
Train_MinReturn : -101.79733276367188
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 600980
TimeSinceStart : 381.47935128211975
Critic_Loss : 0.7263901197910309
Actor_Loss : -11501.583984375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -31.4964542388916
Eval_StdReturn : 14.99544620513916
Eval_MaxReturn : -9.462663650512695
Eval_MinReturn : -55.99449920654297
Eval_AverageEpLen : 151.0
Train_AverageReturn : -26.86489486694336
Train_StdReturn : 16.734106063842773
Train_MaxReturn : 13.873751640319824
Train_MinReturn : -81.29353332519531
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 631029
TimeSinceStart : 401.2441952228546
Critic_Loss : 0.6354183804988861
Actor_Loss : -11388.318359375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -15.817245483398438
Eval_StdReturn : 16.93468475341797
Eval_MaxReturn : 16.40298843383789
Eval_MinReturn : -36.01700210571289
Eval_AverageEpLen : 151.0
Train_AverageReturn : -25.048425674438477
Train_StdReturn : 18.077510833740234
Train_MaxReturn : 18.699129104614258
Train_MinReturn : -108.84808349609375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 661078
TimeSinceStart : 421.1623795032501
Critic_Loss : 0.5936108869314194
Actor_Loss : -11418.083984375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -25.081317901611328
Eval_StdReturn : 10.2614107131958
Eval_MaxReturn : -5.295162200927734
Eval_MinReturn : -39.86518859863281
Eval_AverageEpLen : 151.0
Train_AverageReturn : -20.836671829223633
Train_StdReturn : 16.77788543701172
Train_MaxReturn : 24.095157623291016
Train_MinReturn : -75.4877700805664
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 691127
TimeSinceStart : 440.87658190727234
Critic_Loss : 0.5926279258728028
Actor_Loss : -10845.271484375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -11.240208625793457
Eval_StdReturn : 13.99832820892334
Eval_MaxReturn : 8.308223724365234
Eval_MinReturn : -39.77588653564453
Eval_AverageEpLen : 151.0
Train_AverageReturn : -19.214834213256836
Train_StdReturn : 15.246444702148438
Train_MaxReturn : 19.653518676757812
Train_MinReturn : -63.116600036621094
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 721176
TimeSinceStart : 457.9065821170807
Critic_Loss : 0.5649364471435547
Actor_Loss : -10956.75
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -11.065169334411621
Eval_StdReturn : 11.377388954162598
Eval_MaxReturn : 4.704092025756836
Eval_MinReturn : -38.055877685546875
Eval_AverageEpLen : 151.0
Train_AverageReturn : -18.525022506713867
Train_StdReturn : 15.47645092010498
Train_MaxReturn : 18.03664779663086
Train_MinReturn : -57.441078186035156
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 751225
TimeSinceStart : 474.39255380630493
Critic_Loss : 0.49573179960250857
Actor_Loss : -11005.51171875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -11.122000694274902
Eval_StdReturn : 10.78426456451416
Eval_MaxReturn : -0.5664243698120117
Eval_MinReturn : -35.16838836669922
Eval_AverageEpLen : 151.0
Train_AverageReturn : -12.525606155395508
Train_StdReturn : 13.908863067626953
Train_MaxReturn : 18.868247985839844
Train_MinReturn : -59.166812896728516
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 781274
TimeSinceStart : 490.58600664138794
Critic_Loss : 0.4778039291501045
Actor_Loss : -11694.376953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -6.3354010581970215
Eval_StdReturn : 12.665985107421875
Eval_MaxReturn : 17.904747009277344
Eval_MinReturn : -22.01261329650879
Eval_AverageEpLen : 151.0
Train_AverageReturn : -11.822395324707031
Train_StdReturn : 13.351400375366211
Train_MaxReturn : 23.221967697143555
Train_MinReturn : -46.172176361083984
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 811323
TimeSinceStart : 506.9246599674225
Critic_Loss : 0.4566512727737427
Actor_Loss : -11698.03125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -5.480459690093994
Eval_StdReturn : 8.019618034362793
Eval_MaxReturn : 8.623658180236816
Eval_MinReturn : -17.721899032592773
Eval_AverageEpLen : 151.0
Train_AverageReturn : -8.849580764770508
Train_StdReturn : 13.052207946777344
Train_MaxReturn : 31.12998390197754
Train_MinReturn : -39.941497802734375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 841372
TimeSinceStart : 523.5630328655243
Critic_Loss : 0.4239823186397553
Actor_Loss : -11185.091796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -7.767753601074219
Eval_StdReturn : 10.380547523498535
Eval_MaxReturn : 7.469485282897949
Eval_MinReturn : -24.424766540527344
Eval_AverageEpLen : 151.0
Train_AverageReturn : -5.344992637634277
Train_StdReturn : 13.771023750305176
Train_MaxReturn : 27.12778663635254
Train_MinReturn : -51.38233184814453
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 871421
TimeSinceStart : 540.2458801269531
Critic_Loss : 0.37784604728221893
Actor_Loss : -11493.4716796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -2.7042977809906006
Eval_StdReturn : 12.752018928527832
Eval_MaxReturn : 15.089139938354492
Eval_MinReturn : -29.942138671875
Eval_AverageEpLen : 151.0
Train_AverageReturn : -6.638546943664551
Train_StdReturn : 14.240323066711426
Train_MaxReturn : 26.844711303710938
Train_MinReturn : -84.72421264648438
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 901470
TimeSinceStart : 556.9846785068512
Critic_Loss : 0.35574808716773987
Actor_Loss : -12689.546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -2.297757387161255
Eval_StdReturn : 13.30632209777832
Eval_MaxReturn : 19.435317993164062
Eval_MinReturn : -28.876638412475586
Eval_AverageEpLen : 151.0
Train_AverageReturn : -4.314573287963867
Train_StdReturn : 11.544258117675781
Train_MaxReturn : 25.34626579284668
Train_MinReturn : -38.100589752197266
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 931519
TimeSinceStart : 573.626617193222
Critic_Loss : 0.3462671387195587
Actor_Loss : -12035.2802734375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -1.0244449377059937
Eval_StdReturn : 16.14404296875
Eval_MaxReturn : 18.980121612548828
Eval_MinReturn : -28.397140502929688
Eval_AverageEpLen : 151.0
Train_AverageReturn : -0.06537996232509613
Train_StdReturn : 13.961657524108887
Train_MaxReturn : 29.558469772338867
Train_MinReturn : -64.97988891601562
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 961568
TimeSinceStart : 590.2690780162811
Critic_Loss : 0.36339405506849287
Actor_Loss : -11652.8583984375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : -0.19907407462596893
Eval_StdReturn : 13.61953067779541
Eval_MaxReturn : 15.639209747314453
Eval_MinReturn : -19.938621520996094
Eval_AverageEpLen : 151.0
Train_AverageReturn : 3.549516439437866
Train_StdReturn : 13.817791938781738
Train_MaxReturn : 37.8736572265625
Train_MinReturn : -38.54474639892578
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 991617
TimeSinceStart : 606.8578565120697
Critic_Loss : 0.4123862892389297
Actor_Loss : -10910.8564453125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 13.855142593383789
Eval_StdReturn : 17.418079376220703
Eval_MaxReturn : 38.47981262207031
Eval_MinReturn : -16.697219848632812
Eval_AverageEpLen : 151.0
Train_AverageReturn : 8.176984786987305
Train_StdReturn : 15.5858793258667
Train_MaxReturn : 40.777503967285156
Train_MinReturn : -39.92936325073242
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1021666
TimeSinceStart : 623.5711438655853
Critic_Loss : 0.4848643082380295
Actor_Loss : -9527.2265625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 14.397287368774414
Eval_StdReturn : 13.46100902557373
Eval_MaxReturn : 27.979806900024414
Eval_MinReturn : -19.02118682861328
Eval_AverageEpLen : 151.0
Train_AverageReturn : 12.431890487670898
Train_StdReturn : 16.843692779541016
Train_MaxReturn : 51.677146911621094
Train_MinReturn : -49.29290008544922
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1051715
TimeSinceStart : 640.2477777004242
Critic_Loss : 0.46163924038410187
Actor_Loss : -9553.87890625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 20.981393814086914
Eval_StdReturn : 15.446063995361328
Eval_MaxReturn : 42.35676193237305
Eval_MinReturn : -5.073812484741211
Eval_AverageEpLen : 151.0
Train_AverageReturn : 19.06650733947754
Train_StdReturn : 15.72122859954834
Train_MaxReturn : 56.839378356933594
Train_MinReturn : -50.570404052734375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1081764
TimeSinceStart : 656.8881063461304
Critic_Loss : 0.4067342960834503
Actor_Loss : -10830.9248046875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 29.741687774658203
Eval_StdReturn : 10.047067642211914
Eval_MaxReturn : 51.864891052246094
Eval_MinReturn : 12.753888130187988
Eval_AverageEpLen : 151.0
Train_AverageReturn : 22.5925235748291
Train_StdReturn : 14.480491638183594
Train_MaxReturn : 57.32908248901367
Train_MinReturn : -24.6626033782959
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1111813
TimeSinceStart : 673.5599603652954
Critic_Loss : 0.4647002214193344
Actor_Loss : -10880.55859375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 26.618793487548828
Eval_StdReturn : 20.603422164916992
Eval_MaxReturn : 50.750553131103516
Eval_MinReturn : -21.848417282104492
Eval_AverageEpLen : 151.0
Train_AverageReturn : 28.40087127685547
Train_StdReturn : 16.902219772338867
Train_MaxReturn : 84.50836944580078
Train_MinReturn : -26.93686294555664
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1141862
TimeSinceStart : 690.0219552516937
Critic_Loss : 0.4340598744153976
Actor_Loss : -10679.466796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 36.46868896484375
Eval_StdReturn : 28.015981674194336
Eval_MaxReturn : 71.83612060546875
Eval_MinReturn : -37.73222732543945
Eval_AverageEpLen : 151.0
Train_AverageReturn : 44.461753845214844
Train_StdReturn : 20.93535804748535
Train_MaxReturn : 87.16175842285156
Train_MinReturn : -40.86266326904297
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1171911
TimeSinceStart : 706.4141592979431
Critic_Loss : 0.7743492221832275
Actor_Loss : -7395.1396484375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 38.26181411743164
Eval_StdReturn : 24.124223709106445
Eval_MaxReturn : 68.89701843261719
Eval_MinReturn : -10.695270538330078
Eval_AverageEpLen : 151.0
Train_AverageReturn : 42.81157684326172
Train_StdReturn : 25.19313621520996
Train_MaxReturn : 103.48680114746094
Train_MinReturn : -62.129127502441406
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1201960
TimeSinceStart : 722.9456143379211
Critic_Loss : 0.6083816850185394
Actor_Loss : -7518.35986328125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 44.144981384277344
Eval_StdReturn : 16.688396453857422
Eval_MaxReturn : 85.1594467163086
Eval_MinReturn : 23.2523250579834
Eval_AverageEpLen : 151.0
Train_AverageReturn : 40.355712890625
Train_StdReturn : 23.739377975463867
Train_MaxReturn : 88.29463958740234
Train_MinReturn : -33.19446563720703
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1232009
TimeSinceStart : 739.3954286575317
Critic_Loss : 0.6683078145980835
Actor_Loss : -7742.1044921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 61.47362518310547
Eval_StdReturn : 10.177847862243652
Eval_MaxReturn : 77.79299926757812
Eval_MinReturn : 47.44212341308594
Eval_AverageEpLen : 151.0
Train_AverageReturn : 41.17632293701172
Train_StdReturn : 24.011022567749023
Train_MaxReturn : 94.99360656738281
Train_MinReturn : -46.55287170410156
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1262058
TimeSinceStart : 755.8707597255707
Critic_Loss : 0.6922070568799973
Actor_Loss : -8408.08984375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 69.74446868896484
Eval_StdReturn : 23.199012756347656
Eval_MaxReturn : 92.14104461669922
Eval_MinReturn : 30.295942306518555
Eval_AverageEpLen : 151.0
Train_AverageReturn : 50.66547393798828
Train_StdReturn : 25.71183967590332
Train_MaxReturn : 105.97476196289062
Train_MinReturn : -52.94550704956055
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1292107
TimeSinceStart : 772.326239824295
Critic_Loss : 0.6967932224273682
Actor_Loss : -8109.986328125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 60.1947021484375
Eval_StdReturn : 23.51670265197754
Eval_MaxReturn : 89.06986999511719
Eval_MinReturn : 9.687066078186035
Eval_AverageEpLen : 151.0
Train_AverageReturn : 61.12593078613281
Train_StdReturn : 24.3475284576416
Train_MaxReturn : 108.77685546875
Train_MinReturn : -55.14500045776367
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1322156
TimeSinceStart : 788.7518820762634
Critic_Loss : 0.6286552917957305
Actor_Loss : -8416.435546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 64.15446472167969
Eval_StdReturn : 32.79113006591797
Eval_MaxReturn : 103.0595703125
Eval_MinReturn : -6.571676254272461
Eval_AverageEpLen : 151.0
Train_AverageReturn : 63.31731033325195
Train_StdReturn : 22.84547233581543
Train_MaxReturn : 107.93610382080078
Train_MinReturn : -20.322330474853516
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1352205
TimeSinceStart : 805.3155505657196
Critic_Loss : 0.6913878440856933
Actor_Loss : -7409.8056640625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 81.56999969482422
Eval_StdReturn : 19.13996696472168
Eval_MaxReturn : 98.84954833984375
Eval_MinReturn : 44.562965393066406
Eval_AverageEpLen : 151.0
Train_AverageReturn : 69.65718078613281
Train_StdReturn : 25.10807991027832
Train_MaxReturn : 116.345458984375
Train_MinReturn : -22.224302291870117
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1382254
TimeSinceStart : 821.8212795257568
Critic_Loss : 0.6618449163436889
Actor_Loss : -7871.11376953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 88.48136901855469
Eval_StdReturn : 13.443611145019531
Eval_MaxReturn : 108.83099365234375
Eval_MinReturn : 69.31132507324219
Eval_AverageEpLen : 151.0
Train_AverageReturn : 76.10722351074219
Train_StdReturn : 24.165002822875977
Train_MaxReturn : 128.6411895751953
Train_MinReturn : -29.760107040405273
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1412303
TimeSinceStart : 838.4783246517181
Critic_Loss : 0.6593656653165817
Actor_Loss : -6879.03369140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 75.12153625488281
Eval_StdReturn : 11.114087104797363
Eval_MaxReturn : 96.68887329101562
Eval_MinReturn : 55.91327667236328
Eval_AverageEpLen : 151.0
Train_AverageReturn : 79.95840454101562
Train_StdReturn : 20.57135009765625
Train_MaxReturn : 116.84222412109375
Train_MinReturn : -1.2683849334716797
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1442352
TimeSinceStart : 854.8587734699249
Critic_Loss : 0.6588900458812713
Actor_Loss : -7766.51513671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 98.92034912109375
Eval_StdReturn : 15.249061584472656
Eval_MaxReturn : 119.50376892089844
Eval_MinReturn : 77.65142822265625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 83.58295440673828
Train_StdReturn : 22.14382553100586
Train_MaxReturn : 125.01164245605469
Train_MinReturn : -43.04863739013672
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1472401
TimeSinceStart : 871.3779020309448
Critic_Loss : 0.7372184348106384
Actor_Loss : -7499.890625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 93.98490142822266
Eval_StdReturn : 17.218318939208984
Eval_MaxReturn : 126.60780334472656
Eval_MinReturn : 65.3504638671875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 92.26813507080078
Train_StdReturn : 19.936914443969727
Train_MaxReturn : 137.9808349609375
Train_MinReturn : 11.863924026489258
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1502450
TimeSinceStart : 888.0064835548401
Critic_Loss : 0.7754042035341263
Actor_Loss : -6764.1005859375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 85.58326721191406
Eval_StdReturn : 20.658212661743164
Eval_MaxReturn : 123.76533508300781
Eval_MinReturn : 56.67009353637695
Eval_AverageEpLen : 151.0
Train_AverageReturn : 89.9824447631836
Train_StdReturn : 26.754348754882812
Train_MaxReturn : 141.0820770263672
Train_MinReturn : -31.419078826904297
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1532499
TimeSinceStart : 904.355571269989
Critic_Loss : 0.92595084130764
Actor_Loss : -4653.9873046875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 95.60599517822266
Eval_StdReturn : 12.380637168884277
Eval_MaxReturn : 113.88754272460938
Eval_MinReturn : 76.45380401611328
Eval_AverageEpLen : 151.0
Train_AverageReturn : 93.76054382324219
Train_StdReturn : 16.761167526245117
Train_MaxReturn : 140.0321044921875
Train_MinReturn : 21.001012802124023
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1562548
TimeSinceStart : 920.9105882644653
Critic_Loss : 0.6550904768705368
Actor_Loss : -6898.6142578125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 91.63279724121094
Eval_StdReturn : 21.55391502380371
Eval_MaxReturn : 127.73368835449219
Eval_MinReturn : 60.22478485107422
Eval_AverageEpLen : 151.0
Train_AverageReturn : 91.79649353027344
Train_StdReturn : 16.16379737854004
Train_MaxReturn : 130.98497009277344
Train_MinReturn : 38.345947265625
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1592597
TimeSinceStart : 937.5117154121399
Critic_Loss : 0.5863786202669143
Actor_Loss : -6177.0126953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 47.931026458740234
Eval_StdReturn : 29.212406158447266
Eval_MaxReturn : 85.53768920898438
Eval_MinReturn : -18.232601165771484
Eval_AverageEpLen : 151.0
Train_AverageReturn : 86.59963989257812
Train_StdReturn : 19.922266006469727
Train_MaxReturn : 128.08334350585938
Train_MinReturn : 10.044809341430664
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1622646
TimeSinceStart : 954.2674148082733
Critic_Loss : 0.7865924352407455
Actor_Loss : -6098.306640625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 74.30076599121094
Eval_StdReturn : 11.80791187286377
Eval_MaxReturn : 89.99140930175781
Eval_MinReturn : 45.87678146362305
Eval_AverageEpLen : 151.0
Train_AverageReturn : 51.85116195678711
Train_StdReturn : 27.83037757873535
Train_MaxReturn : 110.56671142578125
Train_MinReturn : -62.5420036315918
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1652695
TimeSinceStart : 971.3575196266174
Critic_Loss : 1.0277837276458741
Actor_Loss : -4354.21728515625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 91.54745483398438
Eval_StdReturn : 16.841581344604492
Eval_MaxReturn : 126.64773559570312
Eval_MinReturn : 71.05980682373047
Eval_AverageEpLen : 151.0
Train_AverageReturn : 71.60568237304688
Train_StdReturn : 23.213584899902344
Train_MaxReturn : 115.03487396240234
Train_MinReturn : -59.439613342285156
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1682744
TimeSinceStart : 988.244300365448
Critic_Loss : 0.7470010781288147
Actor_Loss : -5406.5703125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 107.47076416015625
Eval_StdReturn : 11.784746170043945
Eval_MaxReturn : 119.11964416503906
Eval_MinReturn : 75.45710754394531
Eval_AverageEpLen : 151.0
Train_AverageReturn : 86.25595092773438
Train_StdReturn : 21.16843605041504
Train_MaxReturn : 131.78744506835938
Train_MinReturn : -42.77742385864258
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1712793
TimeSinceStart : 1004.8235721588135
Critic_Loss : 0.6768860584497451
Actor_Loss : -5800.37646484375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 96.46028137207031
Eval_StdReturn : 14.472737312316895
Eval_MaxReturn : 118.48043823242188
Eval_MinReturn : 66.03783416748047
Eval_AverageEpLen : 151.0
Train_AverageReturn : 101.17623138427734
Train_StdReturn : 17.989404678344727
Train_MaxReturn : 147.4451446533203
Train_MinReturn : -1.6855130195617676
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1742842
TimeSinceStart : 1021.481374502182
Critic_Loss : 0.8156131458282471
Actor_Loss : -4595.71923828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 64.0038833618164
Eval_StdReturn : 19.844552993774414
Eval_MaxReturn : 106.53377532958984
Eval_MinReturn : 37.03578186035156
Eval_AverageEpLen : 151.0
Train_AverageReturn : 94.91484832763672
Train_StdReturn : 16.36434555053711
Train_MaxReturn : 139.04827880859375
Train_MinReturn : 50.817176818847656
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1772891
TimeSinceStart : 1038.1256248950958
Critic_Loss : 0.7391880857944488
Actor_Loss : -5321.80615234375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 74.5054702758789
Eval_StdReturn : 15.548060417175293
Eval_MaxReturn : 94.62301635742188
Eval_MinReturn : 44.378639221191406
Eval_AverageEpLen : 151.0
Train_AverageReturn : 58.52067947387695
Train_StdReturn : 22.679981231689453
Train_MaxReturn : 135.28115844726562
Train_MinReturn : 0.4128251075744629
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1802940
TimeSinceStart : 1054.6771228313446
Critic_Loss : 0.8178060346841812
Actor_Loss : -3595.73291015625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 85.6992416381836
Eval_StdReturn : 15.059886932373047
Eval_MaxReturn : 113.021484375
Eval_MinReturn : 61.80601119995117
Eval_AverageEpLen : 151.0
Train_AverageReturn : 71.40817260742188
Train_StdReturn : 17.952510833740234
Train_MaxReturn : 115.32383728027344
Train_MinReturn : 19.949909210205078
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1832989
TimeSinceStart : 1071.17014336586
Critic_Loss : 0.7410181057453156
Actor_Loss : -4776.4912109375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 85.79263305664062
Eval_StdReturn : 36.37699508666992
Eval_MaxReturn : 116.99999237060547
Eval_MinReturn : -17.68117904663086
Eval_AverageEpLen : 151.0
Train_AverageReturn : 89.04113006591797
Train_StdReturn : 13.89284896850586
Train_MaxReturn : 129.81983947753906
Train_MinReturn : -11.0338773727417
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1863038
TimeSinceStart : 1087.7255363464355
Critic_Loss : 0.5193375611305237
Actor_Loss : -5671.841796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 95.53858947753906
Eval_StdReturn : 42.647850036621094
Eval_MaxReturn : 136.51548767089844
Eval_MinReturn : -6.788778305053711
Eval_AverageEpLen : 151.0
Train_AverageReturn : 89.67849731445312
Train_StdReturn : 27.45810890197754
Train_MaxReturn : 121.62865447998047
Train_MinReturn : -49.78704833984375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1893087
TimeSinceStart : 1104.3693795204163
Critic_Loss : 0.5257965204119682
Actor_Loss : -5285.3310546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 125.69498443603516
Eval_StdReturn : 20.563426971435547
Eval_MaxReturn : 165.6021728515625
Eval_MinReturn : 95.28500366210938
Eval_AverageEpLen : 151.0
Train_AverageReturn : 108.20970153808594
Train_StdReturn : 15.32144546508789
Train_MaxReturn : 151.93765258789062
Train_MinReturn : 36.44137954711914
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1923136
TimeSinceStart : 1120.8838262557983
Critic_Loss : 0.6015090984106064
Actor_Loss : -4156.3115234375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 105.74610900878906
Eval_StdReturn : 18.67300796508789
Eval_MaxReturn : 142.51544189453125
Eval_MinReturn : 75.82830810546875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 120.5058364868164
Train_StdReturn : 18.292285919189453
Train_MaxReturn : 154.37118530273438
Train_MinReturn : 26.215742111206055
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1953185
TimeSinceStart : 1137.4557085037231
Critic_Loss : 0.8629116630554199
Actor_Loss : -3785.750244140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 117.1346664428711
Eval_StdReturn : 22.328590393066406
Eval_MaxReturn : 141.077880859375
Eval_MinReturn : 68.1178207397461
Eval_AverageEpLen : 151.0
Train_AverageReturn : 114.96957397460938
Train_StdReturn : 30.5062255859375
Train_MaxReturn : 160.48715209960938
Train_MinReturn : -19.292593002319336
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 1983234
TimeSinceStart : 1154.0524508953094
Critic_Loss : 0.8788875550031662
Actor_Loss : -3644.102294921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 104.113525390625
Eval_StdReturn : 36.207664489746094
Eval_MaxReturn : 149.8166046142578
Eval_MinReturn : 32.4218864440918
Eval_AverageEpLen : 151.0
Train_AverageReturn : 111.28097534179688
Train_StdReturn : 25.320690155029297
Train_MaxReturn : 182.43557739257812
Train_MinReturn : -1.3442344665527344
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2013283
TimeSinceStart : 1170.4034280776978
Critic_Loss : 0.9652240157127381
Actor_Loss : -4247.443359375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 110.67999267578125
Eval_StdReturn : 20.436525344848633
Eval_MaxReturn : 140.69985961914062
Eval_MinReturn : 72.7036361694336
Eval_AverageEpLen : 151.0
Train_AverageReturn : 112.09451293945312
Train_StdReturn : 26.058788299560547
Train_MaxReturn : 165.64996337890625
Train_MinReturn : -0.5299168229103088
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2043332
TimeSinceStart : 1186.9896252155304
Critic_Loss : 0.8603432661294937
Actor_Loss : -4696.8701171875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 108.8812484741211
Eval_StdReturn : 11.3963623046875
Eval_MaxReturn : 119.58770751953125
Eval_MinReturn : 80.16449737548828
Eval_AverageEpLen : 151.0
Train_AverageReturn : 112.74301147460938
Train_StdReturn : 19.04969596862793
Train_MaxReturn : 151.73489379882812
Train_MinReturn : 20.982128143310547
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2073381
TimeSinceStart : 1203.5577442646027
Critic_Loss : 0.8706629818677902
Actor_Loss : -3945.8427734375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 120.59916687011719
Eval_StdReturn : 13.389372825622559
Eval_MaxReturn : 148.41481018066406
Eval_MinReturn : 105.17234802246094
Eval_AverageEpLen : 151.0
Train_AverageReturn : 117.76360321044922
Train_StdReturn : 17.729455947875977
Train_MaxReturn : 160.97100830078125
Train_MinReturn : 38.206539154052734
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2103430
TimeSinceStart : 1219.9854061603546
Critic_Loss : 0.7674420320987702
Actor_Loss : -5106.38671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 119.69947814941406
Eval_StdReturn : 16.46611213684082
Eval_MaxReturn : 139.91476440429688
Eval_MinReturn : 96.35211181640625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 124.4977035522461
Train_StdReturn : 16.7506103515625
Train_MaxReturn : 160.22409057617188
Train_MinReturn : 51.369964599609375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2133479
TimeSinceStart : 1236.4267818927765
Critic_Loss : 0.7414705139398575
Actor_Loss : -4752.51513671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 119.2191162109375
Eval_StdReturn : 19.885496139526367
Eval_MaxReturn : 145.16201782226562
Eval_MinReturn : 75.50602722167969
Eval_AverageEpLen : 151.0
Train_AverageReturn : 122.36404418945312
Train_StdReturn : 18.19476890563965
Train_MaxReturn : 164.29212951660156
Train_MinReturn : 53.14869689941406
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2163528
TimeSinceStart : 1252.8389563560486
Critic_Loss : 0.779134293794632
Actor_Loss : -3932.26953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 95.2572021484375
Eval_StdReturn : 21.09917449951172
Eval_MaxReturn : 135.59600830078125
Eval_MinReturn : 66.82740020751953
Eval_AverageEpLen : 151.0
Train_AverageReturn : 119.75273132324219
Train_StdReturn : 17.56693458557129
Train_MaxReturn : 168.33071899414062
Train_MinReturn : 32.536224365234375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2193577
TimeSinceStart : 1269.2604146003723
Critic_Loss : 1.0534959542751312
Actor_Loss : -3061.09521484375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 128.77236938476562
Eval_StdReturn : 10.312363624572754
Eval_MaxReturn : 148.79183959960938
Eval_MinReturn : 115.0535888671875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 106.25305938720703
Train_StdReturn : 18.5079288482666
Train_MaxReturn : 148.30728149414062
Train_MinReturn : 9.898614883422852
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2223626
TimeSinceStart : 1285.6420204639435
Critic_Loss : 0.9564093738794327
Actor_Loss : -2476.48291015625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 125.32490539550781
Eval_StdReturn : 12.427033424377441
Eval_MaxReturn : 136.2890625
Eval_MinReturn : 92.49982452392578
Eval_AverageEpLen : 151.0
Train_AverageReturn : 123.40837097167969
Train_StdReturn : 16.915075302124023
Train_MaxReturn : 161.62887573242188
Train_MinReturn : 14.787284851074219
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2253675
TimeSinceStart : 1302.1522574424744
Critic_Loss : 0.804174832701683
Actor_Loss : -3125.494140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 125.5488510131836
Eval_StdReturn : 11.22036361694336
Eval_MaxReturn : 141.82452392578125
Eval_MinReturn : 109.52821350097656
Eval_AverageEpLen : 151.0
Train_AverageReturn : 125.24044036865234
Train_StdReturn : 13.0103120803833
Train_MaxReturn : 161.01922607421875
Train_MinReturn : 68.39364624023438
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2283724
TimeSinceStart : 1318.9403331279755
Critic_Loss : 0.739732974767685
Actor_Loss : -4204.1240234375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 133.5845489501953
Eval_StdReturn : 23.22086524963379
Eval_MaxReturn : 180.51641845703125
Eval_MinReturn : 103.13916015625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 133.9488067626953
Train_StdReturn : 14.143523216247559
Train_MaxReturn : 173.01036071777344
Train_MinReturn : 84.96102142333984
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2313773
TimeSinceStart : 1335.4861416816711
Critic_Loss : 0.767619326710701
Actor_Loss : -3927.569580078125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 140.73104858398438
Eval_StdReturn : 11.16887378692627
Eval_MaxReturn : 159.39529418945312
Eval_MinReturn : 121.80007934570312
Eval_AverageEpLen : 151.0
Train_AverageReturn : 137.0093231201172
Train_StdReturn : 16.89154815673828
Train_MaxReturn : 173.37051391601562
Train_MinReturn : 65.35953521728516
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2343822
TimeSinceStart : 1352.0262458324432
Critic_Loss : 0.8241143292188644
Actor_Loss : -3854.524169921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 142.12857055664062
Eval_StdReturn : 16.492176055908203
Eval_MaxReturn : 168.40086364746094
Eval_MinReturn : 106.16443634033203
Eval_AverageEpLen : 151.0
Train_AverageReturn : 131.97418212890625
Train_StdReturn : 22.172971725463867
Train_MaxReturn : 173.99427795410156
Train_MinReturn : 9.48318099975586
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2373871
TimeSinceStart : 1368.515109539032
Critic_Loss : 1.004575698375702
Actor_Loss : -3689.986328125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 149.21337890625
Eval_StdReturn : 12.218597412109375
Eval_MaxReturn : 175.5478057861328
Eval_MinReturn : 131.95599365234375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 136.83059692382812
Train_StdReturn : 20.153980255126953
Train_MaxReturn : 178.91641235351562
Train_MinReturn : 56.30741882324219
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2403920
TimeSinceStart : 1384.8694047927856
Critic_Loss : 1.0419800543785096
Actor_Loss : -2866.03173828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 134.14395141601562
Eval_StdReturn : 18.280925750732422
Eval_MaxReturn : 159.3979034423828
Eval_MinReturn : 94.47672271728516
Eval_AverageEpLen : 151.0
Train_AverageReturn : 141.84715270996094
Train_StdReturn : 16.768726348876953
Train_MaxReturn : 183.54710388183594
Train_MinReturn : 85.97700500488281
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2433969
TimeSinceStart : 1401.2528591156006
Critic_Loss : 1.025916839838028
Actor_Loss : -2516.515869140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 147.29241943359375
Eval_StdReturn : 12.429941177368164
Eval_MaxReturn : 160.46780395507812
Eval_MinReturn : 118.89669799804688
Eval_AverageEpLen : 151.0
Train_AverageReturn : 139.8674774169922
Train_StdReturn : 22.13539695739746
Train_MaxReturn : 179.89456176757812
Train_MinReturn : 12.44578742980957
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2464018
TimeSinceStart : 1417.8076431751251
Critic_Loss : 1.0304968988895415
Actor_Loss : -3437.1279296875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 141.26248168945312
Eval_StdReturn : 14.283636093139648
Eval_MaxReturn : 167.23629760742188
Eval_MinReturn : 124.83226776123047
Eval_AverageEpLen : 151.0
Train_AverageReturn : 144.145751953125
Train_StdReturn : 17.50295639038086
Train_MaxReturn : 182.364501953125
Train_MinReturn : 52.43886184692383
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2494067
TimeSinceStart : 1434.0858836174011
Critic_Loss : 0.9138559925556183
Actor_Loss : -3392.888671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 141.39892578125
Eval_StdReturn : 17.700672149658203
Eval_MaxReturn : 157.80545043945312
Eval_MinReturn : 111.10084533691406
Eval_AverageEpLen : 151.0
Train_AverageReturn : 145.1343994140625
Train_StdReturn : 19.730775833129883
Train_MaxReturn : 195.31736755371094
Train_MinReturn : 44.98788070678711
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2524116
TimeSinceStart : 1450.7507820129395
Critic_Loss : 0.9444901061058044
Actor_Loss : -3190.7060546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 136.41970825195312
Eval_StdReturn : 14.950285911560059
Eval_MaxReturn : 175.2373046875
Eval_MinReturn : 112.83409118652344
Eval_AverageEpLen : 151.0
Train_AverageReturn : 136.78762817382812
Train_StdReturn : 20.786367416381836
Train_MaxReturn : 177.05474853515625
Train_MinReturn : 29.760671615600586
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2554165
TimeSinceStart : 1467.1669278144836
Critic_Loss : 1.1655310422182084
Actor_Loss : -3211.415283203125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 140.7694549560547
Eval_StdReturn : 29.774187088012695
Eval_MaxReturn : 160.54177856445312
Eval_MinReturn : 54.97392272949219
Eval_AverageEpLen : 151.0
Train_AverageReturn : 139.2008819580078
Train_StdReturn : 19.916013717651367
Train_MaxReturn : 191.39639282226562
Train_MinReturn : 44.343841552734375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2584214
TimeSinceStart : 1483.51047372818
Critic_Loss : 0.9995759052038192
Actor_Loss : -3175.0625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 137.81204223632812
Eval_StdReturn : 35.96704864501953
Eval_MaxReturn : 166.9578857421875
Eval_MinReturn : 41.94573974609375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 144.0264434814453
Train_StdReturn : 19.504146575927734
Train_MaxReturn : 183.98770141601562
Train_MinReturn : 35.792724609375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2614263
TimeSinceStart : 1500.060361623764
Critic_Loss : 1.0380371981859207
Actor_Loss : -3422.58837890625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 129.26678466796875
Eval_StdReturn : 17.02305793762207
Eval_MaxReturn : 149.567626953125
Eval_MinReturn : 94.81470489501953
Eval_AverageEpLen : 151.0
Train_AverageReturn : 131.79257202148438
Train_StdReturn : 37.20885467529297
Train_MaxReturn : 180.50332641601562
Train_MinReturn : -24.25163459777832
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2644312
TimeSinceStart : 1516.6131052970886
Critic_Loss : 1.232319085597992
Actor_Loss : -2271.739013671875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 145.039306640625
Eval_StdReturn : 18.458581924438477
Eval_MaxReturn : 172.51339721679688
Eval_MinReturn : 104.412353515625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 131.80758666992188
Train_StdReturn : 30.103599548339844
Train_MaxReturn : 186.61392211914062
Train_MinReturn : 0.2308497428894043
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2674361
TimeSinceStart : 1533.1568059921265
Critic_Loss : 1.0694147723913192
Actor_Loss : -2478.296875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 138.02386474609375
Eval_StdReturn : 15.264277458190918
Eval_MaxReturn : 154.5966796875
Eval_MinReturn : 108.4197769165039
Eval_AverageEpLen : 151.0
Train_AverageReturn : 138.66636657714844
Train_StdReturn : 18.189912796020508
Train_MaxReturn : 184.3369140625
Train_MinReturn : 38.122161865234375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2704410
TimeSinceStart : 1549.6813468933105
Critic_Loss : 1.0840480893850326
Actor_Loss : -2697.1796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 127.26448059082031
Eval_StdReturn : 14.99586009979248
Eval_MaxReturn : 149.781494140625
Eval_MinReturn : 102.97708129882812
Eval_AverageEpLen : 151.0
Train_AverageReturn : 132.30514526367188
Train_StdReturn : 23.275142669677734
Train_MaxReturn : 179.57305908203125
Train_MinReturn : 22.261781692504883
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2734459
TimeSinceStart : 1566.1138951778412
Critic_Loss : 1.0667237365245819
Actor_Loss : -2617.126220703125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 116.76722717285156
Eval_StdReturn : 12.345892906188965
Eval_MaxReturn : 144.5225067138672
Eval_MinReturn : 95.18120574951172
Eval_AverageEpLen : 151.0
Train_AverageReturn : 118.59239196777344
Train_StdReturn : 27.05034637451172
Train_MaxReturn : 164.62355041503906
Train_MinReturn : -1.7428035736083984
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2764508
TimeSinceStart : 1582.5618674755096
Critic_Loss : 1.0132149457931519
Actor_Loss : -1782.74169921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 137.15945434570312
Eval_StdReturn : 4.9496564865112305
Eval_MaxReturn : 147.80796813964844
Eval_MinReturn : 128.67469787597656
Eval_AverageEpLen : 151.0
Train_AverageReturn : 115.29080963134766
Train_StdReturn : 19.085966110229492
Train_MaxReturn : 153.18536376953125
Train_MinReturn : 44.0874137878418
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2794557
TimeSinceStart : 1598.8466529846191
Critic_Loss : 0.7973508638143539
Actor_Loss : -3048.706787109375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 145.5413360595703
Eval_StdReturn : 13.060628890991211
Eval_MaxReturn : 163.22705078125
Eval_MinReturn : 121.11091613769531
Eval_AverageEpLen : 151.0
Train_AverageReturn : 129.49473571777344
Train_StdReturn : 15.983490943908691
Train_MaxReturn : 158.6166229248047
Train_MinReturn : 61.02106475830078
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2824606
TimeSinceStart : 1615.1153149604797
Critic_Loss : 0.7725049102306366
Actor_Loss : -3309.965087890625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 154.85626220703125
Eval_StdReturn : 11.651556968688965
Eval_MaxReturn : 174.6173095703125
Eval_MinReturn : 135.09426879882812
Eval_AverageEpLen : 151.0
Train_AverageReturn : 146.03512573242188
Train_StdReturn : 14.565942764282227
Train_MaxReturn : 179.2943115234375
Train_MinReturn : 83.23147583007812
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2854655
TimeSinceStart : 1631.4777250289917
Critic_Loss : 0.893428236246109
Actor_Loss : -2461.294921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 151.994140625
Eval_StdReturn : 12.944014549255371
Eval_MaxReturn : 170.89419555664062
Eval_MinReturn : 131.68087768554688
Eval_AverageEpLen : 151.0
Train_AverageReturn : 152.56024169921875
Train_StdReturn : 14.589029312133789
Train_MaxReturn : 182.8814697265625
Train_MinReturn : 108.20759582519531
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2884704
TimeSinceStart : 1647.9537539482117
Critic_Loss : 0.9040728509426117
Actor_Loss : -3109.03759765625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 149.35618591308594
Eval_StdReturn : 14.853055953979492
Eval_MaxReturn : 177.12225341796875
Eval_MinReturn : 128.07032775878906
Eval_AverageEpLen : 151.0
Train_AverageReturn : 148.48597717285156
Train_StdReturn : 18.829849243164062
Train_MaxReturn : 184.16241455078125
Train_MinReturn : 60.048133850097656
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2914753
TimeSinceStart : 1664.509813785553
Critic_Loss : 0.9274493420124054
Actor_Loss : -2668.399169921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 143.38990783691406
Eval_StdReturn : 17.796497344970703
Eval_MaxReturn : 170.0548095703125
Eval_MinReturn : 114.17704772949219
Eval_AverageEpLen : 151.0
Train_AverageReturn : 154.31715393066406
Train_StdReturn : 15.048799514770508
Train_MaxReturn : 191.50033569335938
Train_MinReturn : 84.12677001953125
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2944802
TimeSinceStart : 1681.1485822200775
Critic_Loss : 0.9513894248008729
Actor_Loss : -2717.11474609375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 136.6063232421875
Eval_StdReturn : 16.45124626159668
Eval_MaxReturn : 162.53271484375
Eval_MinReturn : 112.11058044433594
Eval_AverageEpLen : 151.0
Train_AverageReturn : 150.48373413085938
Train_StdReturn : 16.92359733581543
Train_MaxReturn : 190.8590087890625
Train_MinReturn : 64.54363250732422
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 2974851
TimeSinceStart : 1697.6195619106293
Critic_Loss : 0.9609744220972061
Actor_Loss : -2622.78076171875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 138.16043090820312
Eval_StdReturn : 14.037535667419434
Eval_MaxReturn : 160.71817016601562
Eval_MinReturn : 117.61566162109375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 142.68682861328125
Train_StdReturn : 17.837732315063477
Train_MaxReturn : 178.93392944335938
Train_MinReturn : 36.165470123291016
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3004900
TimeSinceStart : 1714.0439562797546
Critic_Loss : 0.9102387845516204
Actor_Loss : -2617.96044921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 149.21273803710938
Eval_StdReturn : 7.238054275512695
Eval_MaxReturn : 162.03939819335938
Eval_MinReturn : 139.19635009765625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 134.70130920410156
Train_StdReturn : 21.456344604492188
Train_MaxReturn : 179.57540893554688
Train_MinReturn : 26.834697723388672
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3034949
TimeSinceStart : 1730.4532525539398
Critic_Loss : 0.9135216176509857
Actor_Loss : -2252.25390625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 136.844482421875
Eval_StdReturn : 10.959468841552734
Eval_MaxReturn : 161.31033325195312
Eval_MinReturn : 116.83755493164062
Eval_AverageEpLen : 151.0
Train_AverageReturn : 133.65667724609375
Train_StdReturn : 16.88693618774414
Train_MaxReturn : 168.60235595703125
Train_MinReturn : 46.6998405456543
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3064998
TimeSinceStart : 1746.9241812229156
Critic_Loss : 0.8363235682249069
Actor_Loss : -2338.120361328125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 135.55975341796875
Eval_StdReturn : 15.474162101745605
Eval_MaxReturn : 151.90426635742188
Eval_MinReturn : 104.80326843261719
Eval_AverageEpLen : 151.0
Train_AverageReturn : 133.69996643066406
Train_StdReturn : 19.476404190063477
Train_MaxReturn : 169.7757568359375
Train_MinReturn : 46.96665573120117
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3095047
TimeSinceStart : 1763.333344221115
Critic_Loss : 0.8481271082162857
Actor_Loss : -3072.02294921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 141.78872680664062
Eval_StdReturn : 17.350963592529297
Eval_MaxReturn : 170.9569091796875
Eval_MinReturn : 116.79743957519531
Eval_AverageEpLen : 151.0
Train_AverageReturn : 138.88040161132812
Train_StdReturn : 17.46726417541504
Train_MaxReturn : 179.42124938964844
Train_MinReturn : 55.23650360107422
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3125096
TimeSinceStart : 1779.923231601715
Critic_Loss : 0.8404888844490052
Actor_Loss : -2480.909423828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 154.97634887695312
Eval_StdReturn : 17.33751678466797
Eval_MaxReturn : 178.109130859375
Eval_MinReturn : 120.82109832763672
Eval_AverageEpLen : 151.0
Train_AverageReturn : 147.64366149902344
Train_StdReturn : 21.339950561523438
Train_MaxReturn : 192.64712524414062
Train_MinReturn : 45.560546875
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3155145
TimeSinceStart : 1796.3599166870117
Critic_Loss : 1.0114735502004624
Actor_Loss : -1941.806396484375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 153.98318481445312
Eval_StdReturn : 19.535131454467773
Eval_MaxReturn : 185.8075714111328
Eval_MinReturn : 128.87777709960938
Eval_AverageEpLen : 151.0
Train_AverageReturn : 151.01121520996094
Train_StdReturn : 23.38611602783203
Train_MaxReturn : 196.36892700195312
Train_MinReturn : 61.832969665527344
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3185194
TimeSinceStart : 1812.715345621109
Critic_Loss : 1.135901311635971
Actor_Loss : -2366.867431640625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 138.17568969726562
Eval_StdReturn : 19.338777542114258
Eval_MaxReturn : 168.9473876953125
Eval_MinReturn : 97.70146942138672
Eval_AverageEpLen : 151.0
Train_AverageReturn : 143.70143127441406
Train_StdReturn : 28.487092971801758
Train_MaxReturn : 187.54150390625
Train_MinReturn : 4.792832851409912
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3215243
TimeSinceStart : 1828.9787249565125
Critic_Loss : 1.10519540309906
Actor_Loss : -2149.84619140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 149.55462646484375
Eval_StdReturn : 17.584556579589844
Eval_MaxReturn : 184.14883422851562
Eval_MinReturn : 119.657958984375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 150.04449462890625
Train_StdReturn : 21.647441864013672
Train_MaxReturn : 193.34239196777344
Train_MinReturn : 86.67508697509766
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3245292
TimeSinceStart : 1845.569334745407
Critic_Loss : 1.022889654636383
Actor_Loss : -2380.50244140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 151.1000518798828
Eval_StdReturn : 16.95683479309082
Eval_MaxReturn : 186.88742065429688
Eval_MinReturn : 125.32220458984375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 160.2725372314453
Train_StdReturn : 16.254716873168945
Train_MaxReturn : 196.49884033203125
Train_MinReturn : 57.314693450927734
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3275341
TimeSinceStart : 1861.9698693752289
Critic_Loss : 0.9504918485879899
Actor_Loss : -2250.22802734375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 139.5693359375
Eval_StdReturn : 19.79574203491211
Eval_MaxReturn : 172.8372039794922
Eval_MinReturn : 100.65768432617188
Eval_AverageEpLen : 151.0
Train_AverageReturn : 155.5215301513672
Train_StdReturn : 17.68202018737793
Train_MaxReturn : 191.15260314941406
Train_MinReturn : 54.1431884765625
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3305390
TimeSinceStart : 1878.334745645523
Critic_Loss : 0.987316049337387
Actor_Loss : -2131.45458984375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 146.5094757080078
Eval_StdReturn : 17.912424087524414
Eval_MaxReturn : 163.462158203125
Eval_MinReturn : 106.76687622070312
Eval_AverageEpLen : 151.0
Train_AverageReturn : 137.83499145507812
Train_StdReturn : 22.88580322265625
Train_MaxReturn : 177.5238037109375
Train_MinReturn : 35.976715087890625
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3335439
TimeSinceStart : 1894.9061670303345
Critic_Loss : 1.002734581232071
Actor_Loss : -1745.976318359375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 144.87332153320312
Eval_StdReturn : 13.90526008605957
Eval_MaxReturn : 163.76058959960938
Eval_MinReturn : 122.3814468383789
Eval_AverageEpLen : 151.0
Train_AverageReturn : 135.59326171875
Train_StdReturn : 22.705089569091797
Train_MaxReturn : 181.07650756835938
Train_MinReturn : 15.60822868347168
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3365488
TimeSinceStart : 1911.358705997467
Critic_Loss : 0.8892438870668411
Actor_Loss : -2821.92822265625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 153.6975860595703
Eval_StdReturn : 17.332599639892578
Eval_MaxReturn : 183.11599731445312
Eval_MinReturn : 136.399169921875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 151.45509338378906
Train_StdReturn : 13.352399826049805
Train_MaxReturn : 183.9353485107422
Train_MinReturn : 103.26910400390625
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3395537
TimeSinceStart : 1927.7398326396942
Critic_Loss : 0.8166368931531907
Actor_Loss : -2776.7578125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 159.76919555664062
Eval_StdReturn : 11.328398704528809
Eval_MaxReturn : 179.7086639404297
Eval_MinReturn : 141.37527465820312
Eval_AverageEpLen : 151.0
Train_AverageReturn : 156.0066680908203
Train_StdReturn : 15.620824813842773
Train_MaxReturn : 193.78802490234375
Train_MinReturn : 87.63030242919922
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3425586
TimeSinceStart : 1944.0645530223846
Critic_Loss : 0.9270459514856338
Actor_Loss : -1383.964111328125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 158.04811096191406
Eval_StdReturn : 16.069202423095703
Eval_MaxReturn : 176.69155883789062
Eval_MinReturn : 122.58043670654297
Eval_AverageEpLen : 151.0
Train_AverageReturn : 155.6820831298828
Train_StdReturn : 19.15846824645996
Train_MaxReturn : 201.42320251464844
Train_MinReturn : 40.755184173583984
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3455635
TimeSinceStart : 1960.4922733306885
Critic_Loss : 1.1015592247247696
Actor_Loss : -1697.03759765625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 155.480224609375
Eval_StdReturn : 13.35412311553955
Eval_MaxReturn : 171.5538330078125
Eval_MinReturn : 136.2887420654297
Eval_AverageEpLen : 151.0
Train_AverageReturn : 159.07925415039062
Train_StdReturn : 16.354450225830078
Train_MaxReturn : 198.99301147460938
Train_MinReturn : 90.00711822509766
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3485684
TimeSinceStart : 1976.906500339508
Critic_Loss : 1.1962747418880462
Actor_Loss : -1288.1640625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 152.63601684570312
Eval_StdReturn : 39.32205581665039
Eval_MaxReturn : 184.27366638183594
Eval_MinReturn : 40.61513137817383
Eval_AverageEpLen : 151.0
Train_AverageReturn : 164.62998962402344
Train_StdReturn : 16.060161590576172
Train_MaxReturn : 194.56643676757812
Train_MinReturn : 70.2936019897461
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3515733
TimeSinceStart : 1993.3857641220093
Critic_Loss : 1.005407331585884
Actor_Loss : -1779.5308837890625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 163.97021484375
Eval_StdReturn : 18.481765747070312
Eval_MaxReturn : 192.50917053222656
Eval_MinReturn : 138.0863800048828
Eval_AverageEpLen : 151.0
Train_AverageReturn : 160.4435272216797
Train_StdReturn : 19.083049774169922
Train_MaxReturn : 203.0153045654297
Train_MinReturn : 48.25859069824219
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3545782
TimeSinceStart : 2010.0020549297333
Critic_Loss : 1.0347853183746338
Actor_Loss : -1798.60107421875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 151.07550048828125
Eval_StdReturn : 12.93307876586914
Eval_MaxReturn : 170.7102813720703
Eval_MinReturn : 133.58407592773438
Eval_AverageEpLen : 151.0
Train_AverageReturn : 159.3640899658203
Train_StdReturn : 19.089920043945312
Train_MaxReturn : 202.4656219482422
Train_MinReturn : 45.98518371582031
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3575831
TimeSinceStart : 2026.2194011211395
Critic_Loss : 1.0497102189064025
Actor_Loss : -1331.0440673828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 156.39419555664062
Eval_StdReturn : 19.498220443725586
Eval_MaxReturn : 189.41815185546875
Eval_MinReturn : 131.4478302001953
Eval_AverageEpLen : 151.0
Train_AverageReturn : 156.57901000976562
Train_StdReturn : 20.064599990844727
Train_MaxReturn : 191.25949096679688
Train_MinReturn : 43.114051818847656
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3605880
TimeSinceStart : 2042.434714794159
Critic_Loss : 0.9978759169578553
Actor_Loss : -1493.581298828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 147.41659545898438
Eval_StdReturn : 11.745712280273438
Eval_MaxReturn : 165.330322265625
Eval_MinReturn : 128.330810546875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 157.17189025878906
Train_StdReturn : 17.516271591186523
Train_MaxReturn : 218.69073486328125
Train_MinReturn : 75.29374694824219
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3635929
TimeSinceStart : 2058.4760909080505
Critic_Loss : 1.112107338309288
Actor_Loss : -2116.294921875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 159.17269897460938
Eval_StdReturn : 17.471986770629883
Eval_MaxReturn : 188.26956176757812
Eval_MinReturn : 130.76263427734375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 154.60829162597656
Train_StdReturn : 13.370659828186035
Train_MaxReturn : 190.4876708984375
Train_MinReturn : 108.39979553222656
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3665978
TimeSinceStart : 2074.5644705295563
Critic_Loss : 1.010777696967125
Actor_Loss : -1975.6759033203125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 141.01754760742188
Eval_StdReturn : 11.289103507995605
Eval_MaxReturn : 157.14881896972656
Eval_MinReturn : 120.65924835205078
Eval_AverageEpLen : 151.0
Train_AverageReturn : 148.86083984375
Train_StdReturn : 17.065921783447266
Train_MaxReturn : 184.99542236328125
Train_MinReturn : 67.66848754882812
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3696027
TimeSinceStart : 2090.672108888626
Critic_Loss : 0.9052030891180038
Actor_Loss : -2004.990966796875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 151.1560516357422
Eval_StdReturn : 20.8807315826416
Eval_MaxReturn : 177.7169189453125
Eval_MinReturn : 114.54783630371094
Eval_AverageEpLen : 151.0
Train_AverageReturn : 147.29090881347656
Train_StdReturn : 13.076303482055664
Train_MaxReturn : 175.33998107910156
Train_MinReturn : 112.62139129638672
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3726076
TimeSinceStart : 2106.8822236061096
Critic_Loss : 0.8932216709852219
Actor_Loss : -1933.4525146484375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 144.40609741210938
Eval_StdReturn : 15.270674705505371
Eval_MaxReturn : 173.73974609375
Eval_MinReturn : 115.90084838867188
Eval_AverageEpLen : 151.0
Train_AverageReturn : 146.84686279296875
Train_StdReturn : 14.33584213256836
Train_MaxReturn : 185.76730346679688
Train_MinReturn : 101.24518585205078
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3756125
TimeSinceStart : 2123.0257484912872
Critic_Loss : 0.8703454357385635
Actor_Loss : -2323.1220703125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 166.3036346435547
Eval_StdReturn : 15.81594467163086
Eval_MaxReturn : 189.3626708984375
Eval_MinReturn : 142.2180633544922
Eval_AverageEpLen : 151.0
Train_AverageReturn : 154.3356170654297
Train_StdReturn : 16.401758193969727
Train_MaxReturn : 198.184814453125
Train_MinReturn : 98.72837829589844
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3786174
TimeSinceStart : 2139.3211867809296
Critic_Loss : 0.9755351626873017
Actor_Loss : -2279.4384765625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 154.24838256835938
Eval_StdReturn : 29.70220375061035
Eval_MaxReturn : 200.67596435546875
Eval_MinReturn : 88.20893859863281
Eval_AverageEpLen : 151.0
Train_AverageReturn : 161.0903778076172
Train_StdReturn : 20.80841064453125
Train_MaxReturn : 206.1558837890625
Train_MinReturn : 63.33012390136719
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3816223
TimeSinceStart : 2155.6724524497986
Critic_Loss : 1.0435536307096482
Actor_Loss : -1945.857177734375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 161.45101928710938
Eval_StdReturn : 19.775922775268555
Eval_MaxReturn : 187.31027221679688
Eval_MinReturn : 129.01998901367188
Eval_AverageEpLen : 151.0
Train_AverageReturn : 158.0616912841797
Train_StdReturn : 23.633869171142578
Train_MaxReturn : 199.21395874023438
Train_MinReturn : 7.150148391723633
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3846272
TimeSinceStart : 2171.7815680503845
Critic_Loss : 1.3714487731456757
Actor_Loss : -1495.533447265625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 147.6333770751953
Eval_StdReturn : 21.312171936035156
Eval_MaxReturn : 182.12930297851562
Eval_MinReturn : 97.85513305664062
Eval_AverageEpLen : 151.0
Train_AverageReturn : 153.2744903564453
Train_StdReturn : 26.77543067932129
Train_MaxReturn : 192.64425659179688
Train_MinReturn : 13.861515045166016
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3876321
TimeSinceStart : 2188.3741822242737
Critic_Loss : 1.0732300931215286
Actor_Loss : -1075.585205078125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 156.684814453125
Eval_StdReturn : 19.034481048583984
Eval_MaxReturn : 186.3358154296875
Eval_MinReturn : 120.6919937133789
Eval_AverageEpLen : 151.0
Train_AverageReturn : 145.98878479003906
Train_StdReturn : 29.743928909301758
Train_MaxReturn : 189.96737670898438
Train_MinReturn : -3.1724348068237305
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3906370
TimeSinceStart : 2205.0094785690308
Critic_Loss : 0.9441075831651687
Actor_Loss : -1567.83349609375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 164.8304443359375
Eval_StdReturn : 11.956958770751953
Eval_MaxReturn : 185.3134002685547
Eval_MinReturn : 143.64605712890625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 151.54359436035156
Train_StdReturn : 25.773618698120117
Train_MaxReturn : 189.18984985351562
Train_MinReturn : -1.7363471984863281
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3936419
TimeSinceStart : 2221.603657245636
Critic_Loss : 0.9806810194253921
Actor_Loss : -1799.705322265625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 142.82958984375
Eval_StdReturn : 55.31126022338867
Eval_MaxReturn : 195.17869567871094
Eval_MinReturn : -14.837690353393555
Eval_AverageEpLen : 151.0
Train_AverageReturn : 158.54515075683594
Train_StdReturn : 22.59876823425293
Train_MaxReturn : 193.9068145751953
Train_MinReturn : 3.27825927734375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3966468
TimeSinceStart : 2238.6001963615417
Critic_Loss : 0.9968595838546753
Actor_Loss : -2181.034423828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 155.75790405273438
Eval_StdReturn : 18.55511474609375
Eval_MaxReturn : 188.30935668945312
Eval_MinReturn : 127.13831329345703
Eval_AverageEpLen : 151.0
Train_AverageReturn : 157.18115234375
Train_StdReturn : 22.260364532470703
Train_MaxReturn : 191.5642547607422
Train_MinReturn : 13.483734130859375
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 3996517
TimeSinceStart : 2255.4828355312347
Critic_Loss : 1.017833840250969
Actor_Loss : -1356.981689453125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 165.05856323242188
Eval_StdReturn : 14.675334930419922
Eval_MaxReturn : 197.7193603515625
Eval_MinReturn : 134.03286743164062
Eval_AverageEpLen : 151.0
Train_AverageReturn : 161.19827270507812
Train_StdReturn : 20.62422752380371
Train_MaxReturn : 203.07125854492188
Train_MinReturn : 41.331817626953125
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4026566
TimeSinceStart : 2272.600911617279
Critic_Loss : 1.0980223059654235
Actor_Loss : -1851.685791015625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 165.48374938964844
Eval_StdReturn : 21.163938522338867
Eval_MaxReturn : 200.63710021972656
Eval_MinReturn : 126.81990814208984
Eval_AverageEpLen : 151.0
Train_AverageReturn : 160.2158660888672
Train_StdReturn : 19.022518157958984
Train_MaxReturn : 203.06393432617188
Train_MinReturn : 43.90055847167969
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4056615
TimeSinceStart : 2289.4437804222107
Critic_Loss : 1.197702009677887
Actor_Loss : -1734.392822265625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 154.52488708496094
Eval_StdReturn : 20.143768310546875
Eval_MaxReturn : 186.50054931640625
Eval_MinReturn : 107.69763946533203
Eval_AverageEpLen : 151.0
Train_AverageReturn : 151.55296325683594
Train_StdReturn : 28.760805130004883
Train_MaxReturn : 204.89974975585938
Train_MinReturn : 17.852699279785156
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4086664
TimeSinceStart : 2306.011979341507
Critic_Loss : 1.209005960226059
Actor_Loss : -2021.8369140625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 158.00108337402344
Eval_StdReturn : 10.461419105529785
Eval_MaxReturn : 173.7270050048828
Eval_MinReturn : 133.0614776611328
Eval_AverageEpLen : 151.0
Train_AverageReturn : 152.13829040527344
Train_StdReturn : 30.479703903198242
Train_MaxReturn : 203.6866455078125
Train_MinReturn : 4.844657897949219
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4116713
TimeSinceStart : 2322.7547895908356
Critic_Loss : 1.045939788222313
Actor_Loss : -1880.1383056640625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 160.53823852539062
Eval_StdReturn : 16.96038246154785
Eval_MaxReturn : 192.183837890625
Eval_MinReturn : 128.23727416992188
Eval_AverageEpLen : 151.0
Train_AverageReturn : 155.9930419921875
Train_StdReturn : 21.445322036743164
Train_MaxReturn : 208.71182250976562
Train_MinReturn : 8.076417922973633
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4146762
TimeSinceStart : 2339.5516328811646
Critic_Loss : 0.9411902832984924
Actor_Loss : -2138.790283203125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 135.45993041992188
Eval_StdReturn : 22.848386764526367
Eval_MaxReturn : 183.9008331298828
Eval_MinReturn : 104.15242004394531
Eval_AverageEpLen : 151.0
Train_AverageReturn : 151.30809020996094
Train_StdReturn : 22.937406539916992
Train_MaxReturn : 189.1219024658203
Train_MinReturn : 32.727840423583984
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4176811
TimeSinceStart : 2356.2151217460632
Critic_Loss : 1.0391474360227584
Actor_Loss : -1517.576416015625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 118.6851806640625
Eval_StdReturn : 18.888063430786133
Eval_MaxReturn : 148.60997009277344
Eval_MinReturn : 84.88592529296875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 132.20635986328125
Train_StdReturn : 25.078014373779297
Train_MaxReturn : 178.40179443359375
Train_MinReturn : -2.5905895233154297
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4206860
TimeSinceStart : 2372.692163705826
Critic_Loss : 1.2044913911819457
Actor_Loss : -1020.7908935546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 127.7330322265625
Eval_StdReturn : 22.376813888549805
Eval_MaxReturn : 162.82022094726562
Eval_MinReturn : 87.95195007324219
Eval_AverageEpLen : 151.0
Train_AverageReturn : 125.17680358886719
Train_StdReturn : 24.00665855407715
Train_MaxReturn : 174.00538635253906
Train_MinReturn : 17.966726303100586
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4236909
TimeSinceStart : 2389.327456712723
Critic_Loss : 1.144741358757019
Actor_Loss : -810.373779296875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 158.22972106933594
Eval_StdReturn : 17.55569076538086
Eval_MaxReturn : 184.180908203125
Eval_MinReturn : 117.02567291259766
Eval_AverageEpLen : 151.0
Train_AverageReturn : 132.397705078125
Train_StdReturn : 21.63625717163086
Train_MaxReturn : 182.5201873779297
Train_MinReturn : 23.04883575439453
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4266958
TimeSinceStart : 2406.0830512046814
Critic_Loss : 1.0271170246601105
Actor_Loss : -1564.4886474609375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 158.71322631835938
Eval_StdReturn : 11.593413352966309
Eval_MaxReturn : 176.4418487548828
Eval_MinReturn : 141.61895751953125
Eval_AverageEpLen : 151.0
Train_AverageReturn : 150.30796813964844
Train_StdReturn : 17.1967830657959
Train_MaxReturn : 183.29669189453125
Train_MinReturn : 83.54605102539062
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4297007
TimeSinceStart : 2422.7715299129486
Critic_Loss : 0.9461724746227265
Actor_Loss : -1617.9014892578125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 158.43624877929688
Eval_StdReturn : 33.03121566772461
Eval_MaxReturn : 200.14205932617188
Eval_MinReturn : 89.92450714111328
Eval_AverageEpLen : 151.0
Train_AverageReturn : 159.04112243652344
Train_StdReturn : 14.63493824005127
Train_MaxReturn : 203.6209259033203
Train_MinReturn : 112.57249450683594
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4327056
TimeSinceStart : 2439.502458333969
Critic_Loss : 0.91333999812603
Actor_Loss : -1982.052734375
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 163.2951202392578
Eval_StdReturn : 26.890336990356445
Eval_MaxReturn : 205.048828125
Eval_MinReturn : 95.10578918457031
Eval_AverageEpLen : 151.0
Train_AverageReturn : 155.86521911621094
Train_StdReturn : 29.369571685791016
Train_MaxReturn : 192.21133422851562
Train_MinReturn : 29.584157943725586
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4357105
TimeSinceStart : 2456.0796666145325
Critic_Loss : 1.0053598606586456
Actor_Loss : -1694.685546875
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 165.8755645751953
Eval_StdReturn : 10.16819953918457
Eval_MaxReturn : 186.5924835205078
Eval_MinReturn : 151.784912109375
Eval_AverageEpLen : 151.0
Train_AverageReturn : 157.80233764648438
Train_StdReturn : 22.269989013671875
Train_MaxReturn : 197.21234130859375
Train_MinReturn : 26.692073822021484
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4387154
TimeSinceStart : 2472.738105535507
Critic_Loss : 0.956784343123436
Actor_Loss : -2064.88330078125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 161.2794189453125
Eval_StdReturn : 11.731743812561035
Eval_MaxReturn : 183.6729736328125
Eval_MinReturn : 145.13528442382812
Eval_AverageEpLen : 151.0
Train_AverageReturn : 156.894287109375
Train_StdReturn : 13.345894813537598
Train_MaxReturn : 191.66738891601562
Train_MinReturn : 110.94775390625
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4417203
TimeSinceStart : 2489.472916126251
Critic_Loss : 1.1180869132280349
Actor_Loss : -1389.080322265625
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 160.30624389648438
Eval_StdReturn : 12.59410285949707
Eval_MaxReturn : 192.63243103027344
Eval_MinReturn : 144.12908935546875
Eval_AverageEpLen : 151.0
Train_AverageReturn : 158.06927490234375
Train_StdReturn : 13.520384788513184
Train_MaxReturn : 186.8860626220703
Train_MinReturn : 120.61009216308594
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4447252
TimeSinceStart : 2506.214364051819
Critic_Loss : 0.9667339438199997
Actor_Loss : -1600.31298828125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 163.29185485839844
Eval_StdReturn : 19.870622634887695
Eval_MaxReturn : 189.1654815673828
Eval_MinReturn : 122.56968688964844
Eval_AverageEpLen : 151.0
Train_AverageReturn : 161.63467407226562
Train_StdReturn : 19.310556411743164
Train_MaxReturn : 200.364990234375
Train_MinReturn : 60.135047912597656
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4477301
TimeSinceStart : 2522.978754758835
Critic_Loss : 0.9560387474298477
Actor_Loss : -1308.2130126953125
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...



Collecting data to be used for training...

Beginning logging procedure...

Collecting data for eval...
Eval_AverageReturn : 171.5006103515625
Eval_StdReturn : 16.575212478637695
Eval_MaxReturn : 196.8111572265625
Eval_MinReturn : 137.73297119140625
Eval_AverageEpLen : 151.0
Train_AverageReturn : 162.06556701660156
Train_StdReturn : 18.63765525817871
Train_MaxReturn : 210.78245544433594
Train_MinReturn : 81.62582397460938
Train_AverageEpLen : 151.0
Train_EnvstepsSoFar : 4507350
TimeSinceStart : 2539.8743522167206
Critic_Loss : 1.0423750537633896
Actor_Loss : -837.5581665039062
Initial_DataCollection_AverageReturn : -145.1819610595703
Done logging...


/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/dqn_utils.py:125: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:42: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-21 17:26:18.657311: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-21 17:26:18.662207: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-21 17:26:18.811712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:26:18.812393: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d230ac17c0 executing computations on platform CUDA. Devices:
2019-10-21 17:26:18.812427: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-10-21 17:26:18.833715: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499615000 Hz
2019-10-21 17:26:18.834751: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d230bdc1c0 executing computations on platform Host. Devices:
2019-10-21 17:26:18.834783: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-21 17:26:18.835084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:26:18.835910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:41:00.0
2019-10-21 17:26:18.836195: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 17:26:18.837574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 17:26:18.838823: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-21 17:26:18.839113: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-21 17:26:18.840754: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-21 17:26:18.842002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-21 17:26:18.845833: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-21 17:26:18.846024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:26:18.846829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:26:18.847371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-21 17:26:18.847415: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 17:26:18.848260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-21 17:26:18.848270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-21 17:26:18.848275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-21 17:26:18.848473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:26:18.849068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:26:18.849647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4769 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:41:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py:39: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

/home/nathan/.local/lib/python3.5/site-packages/gym/logger.py:30: UserWarning: [33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/critics/dqn_critic.py:100: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6d902c4940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6d902c4940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6d902c4898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6d902c4898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6d902c4668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6d902c4668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6d902e7128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6d902e7128>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6d902c4668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6d902c4668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6e26f344a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6e26f344a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6e26ed0048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6e26ed0048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6e26ed0cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6e26ed0cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6e26ed0d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f6e26ed0d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6e26faff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6e26faff28>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6e26ed0828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6e26ed0828>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6e26eca9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f6e26eca9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/dqn_utils.py:170: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where



LOGGING TO:  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/dqn_constant_lr_PongNoFrameskip-v4_21-10-2019_17-26-18 



########################
logging outputs to  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/dqn_constant_lr_PongNoFrameskip-v4_21-10-2019_17-26-18
########################

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002523
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0025229454040527344
Done logging...



Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -20.300000
best mean reward -inf
running time 19.266174
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -20.3
TimeSinceStart : 19.266173839569092
Done logging...



Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -20.250000
best mean reward -inf
running time 35.498100
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -20.25
TimeSinceStart : 35.49809980392456
Done logging...



Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -20.225806
best mean reward -inf
running time 52.774117
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -20.225806451612904
TimeSinceStart : 52.77411723136902
Done logging...



Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -20.238095
best mean reward -inf
running time 69.007484
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -20.238095238095237
TimeSinceStart : 69.00748372077942
Done logging...



Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -20.226415
best mean reward -inf
running time 85.396398
2019-10-21 17:27:46.224383: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 17:27:46.397775: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/dqn_utils.py:141: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:42: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-21 17:28:04.699367: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-21 17:28:04.704458: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-21 17:28:04.783755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:28:04.784588: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5577f2818ee0 executing computations on platform CUDA. Devices:
2019-10-21 17:28:04.784630: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-10-21 17:28:04.805705: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499615000 Hz
2019-10-21 17:28:04.807245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5577f2933970 executing computations on platform Host. Devices:
2019-10-21 17:28:04.807268: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-21 17:28:04.807546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:28:04.808335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:41:00.0
2019-10-21 17:28:04.808618: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 17:28:04.810039: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 17:28:04.811288: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-21 17:28:04.811564: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-21 17:28:04.813106: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-21 17:28:04.814284: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-21 17:28:04.818056: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-21 17:28:04.818262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:28:04.818943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:28:04.819506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-21 17:28:04.819552: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 17:28:04.820393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-21 17:28:04.820403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-21 17:28:04.820408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-21 17:28:04.820602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:28:04.821180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:28:04.821753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4769 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:41:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py:39: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

/home/nathan/.local/lib/python3.5/site-packages/gym/logger.py:30: UserWarning: [33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/critics/dqn_critic.py:100: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efc4c136940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efc4c136940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efc4c136898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efc4c136898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efc4c136668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efc4c136668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7efc4c1581d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7efc4c1581d0>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc4c136da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc4c136da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc4c136da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efc4c136da0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efce43bc0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efce43bc0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efce43bc6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efce43bc6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efce43bc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7efce43bc198>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7efce43dff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7efce43dff28>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efce43bc3c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efce43bc3c8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efce43885c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efce43885c0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/dqn_utils.py:170: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where



LOGGING TO:  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/dqn_increasing_lr_PongNoFrameskip-v4_21-10-2019_17-28-04 



########################
logging outputs to  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/dqn_increasing_lr_PongNoFrameskip-v4_21-10-2019_17-28-04
########################

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.002840
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.002840280532836914
Done logging...



Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -20.300000
best mean reward -inf
running time 19.414527
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -20.22641509433962
TimeSinceStart : 85.39639782905579
Done logging...



Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -20.285714
best mean reward -inf
running time 128.352505
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -20.3
TimeSinceStart : 19.41452717781067
Done logging...



Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -20.250000
best mean reward -inf
running time 36.364767
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -20.25
TimeSinceStart : 36.36476731300354
Done logging...



Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -20.225806
best mean reward -inf
running time 54.007032
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/dqn_utils.py:133: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:42: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/tf_utils.py:59: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-10-21 17:29:03.119105: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-21 17:29:03.124366: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-21 17:29:03.216109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:29:03.216812: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b594061f0 executing computations on platform CUDA. Devices:
2019-10-21 17:29:03.216834: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-10-21 17:29:03.237773: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3499615000 Hz
2019-10-21 17:29:03.239235: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b59520910 executing computations on platform Host. Devices:
2019-10-21 17:29:03.239292: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-21 17:29:03.239574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:29:03.240320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:41:00.0
2019-10-21 17:29:03.240606: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 17:29:03.241936: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 17:29:03.243119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-21 17:29:03.243359: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-21 17:29:03.244702: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-21 17:29:03.245755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-21 17:29:03.248977: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-21 17:29:03.249208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:29:03.250038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:29:03.250810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-21 17:29:03.250902: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-21 17:29:03.251990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-21 17:29:03.252007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-21 17:29:03.252016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-21 17:29:03.252316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:29:03.253020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-21 17:29:03.253738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4769 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:41:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/rl_trainer.py:39: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

/home/nathan/.local/lib/python3.5/site-packages/gym/logger.py:30: UserWarning: [33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/critics/dqn_critic.py:100: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f17941b1940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f17941b1940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f17941b1668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f17941b1668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f17941b1ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f17941b1ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/anaconda3/envs/cs285_env/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f17941d3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f17941d3198>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17941b1940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17941b1940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17941b1940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f17941b1940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f182bc37c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f182bc37c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f182bc73cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f182bc73cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f182bb9db38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7f182bb9db38>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f182bb9bac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f182bb9bac8>>: AttributeError: module 'gast' has no attribute 'Num'
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f182bb9d048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f182bb9d048>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f182bc266a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f182bc266a0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:From /home/nathan/projects/homework_fall2019/hw3/cs285/infrastructure/dqn_utils.py:170: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where



LOGGING TO:  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/dqn_decreasing_lr_PongNoFrameskip-v4_21-10-2019_17-29-03 



########################
logging outputs to  /home/nathan/projects/homework_fall2019/hw3/cs285/scripts/../data/dqn_decreasing_lr_PongNoFrameskip-v4_21-10-2019_17-29-03
########################

Beginning logging procedure...
Timestep 1
mean reward (100 episodes) nan
best mean reward -inf
running time 0.003200
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -20.285714285714285
TimeSinceStart : 128.35250544548035
Done logging...



Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -20.178082
best mean reward -inf
running time 171.612799
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -20.225806451612904
TimeSinceStart : 54.00703191757202
Done logging...



Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -20.238095
best mean reward -inf
running time 71.167521
Train_EnvstepsSoFar : 1
TimeSinceStart : 0.0031995773315429688
Done logging...



Beginning logging procedure...
Timestep 10001
mean reward (100 episodes) -20.300000
best mean reward -inf
running time 20.163551
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -20.238095238095237
TimeSinceStart : 71.16752099990845
Done logging...



Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -20.226415
best mean reward -inf
running time 88.375973
2019-10-21 17:29:35.252326: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 17:29:35.456851: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
Train_EnvstepsSoFar : 10001
Train_AverageReturn : -20.3
TimeSinceStart : 20.163551092147827
Done logging...



Beginning logging procedure...
Timestep 20001
mean reward (100 episodes) -20.250000
best mean reward -inf
running time 37.213917
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -20.17808219178082
TimeSinceStart : 171.61279916763306
Done logging...



Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -20.238095
best mean reward -inf
running time 214.701957
Train_EnvstepsSoFar : 20001
Train_AverageReturn : -20.25
TimeSinceStart : 37.21391701698303
Done logging...



Beginning logging procedure...
Timestep 30001
mean reward (100 episodes) -20.225806
best mean reward -inf
running time 54.941057
Train_EnvstepsSoFar : 30001
Train_AverageReturn : -20.225806451612904
TimeSinceStart : 54.941056966781616
Done logging...



Beginning logging procedure...
Timestep 40001
mean reward (100 episodes) -20.238095
best mean reward -inf
running time 71.924781
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -20.22641509433962
TimeSinceStart : 88.37597298622131
Done logging...



Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -20.218750
best mean reward -inf
running time 133.398490
Train_EnvstepsSoFar : 40001
Train_AverageReturn : -20.238095238095237
TimeSinceStart : 71.92478060722351
Done logging...



Beginning logging procedure...
Timestep 50001
mean reward (100 episodes) -20.226415
best mean reward -inf
running time 89.436683
2019-10-21 17:30:34.768204: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-21 17:30:34.990890: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -20.238095238095237
TimeSinceStart : 214.7019567489624
Done logging...



Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -20.200000
best mean reward -inf
running time 257.996735
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -20.21875
TimeSinceStart : 133.39849019050598
Done logging...



Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -20.202703
best mean reward -inf
running time 177.228310
Train_EnvstepsSoFar : 50001
Train_AverageReturn : -20.22641509433962
TimeSinceStart : 89.43668341636658
Done logging...



Beginning logging procedure...
Timestep 60001
mean reward (100 episodes) -20.265625
best mean reward -inf
running time 134.419273
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -20.2
TimeSinceStart : 257.99673533439636
Done logging...



Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -20.160000
best mean reward -20.160000
running time 302.217358
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -20.2027027027027
TimeSinceStart : 177.22830963134766
Done logging...



Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -20.188235
best mean reward -inf
running time 220.446868
Train_EnvstepsSoFar : 60001
Train_AverageReturn : -20.265625
TimeSinceStart : 134.41927337646484
Done logging...



Beginning logging procedure...
Timestep 70001
mean reward (100 episodes) -20.283784
best mean reward -inf
running time 178.720355
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -20.16
Train_BestReturn : -20.16
TimeSinceStart : 302.2173583507538
Done logging...



Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -20.210000
best mean reward -20.160000
running time 347.017875
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -20.188235294117646
TimeSinceStart : 220.44686794281006
Done logging...



Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -20.136842
best mean reward -inf
running time 264.113073
Train_EnvstepsSoFar : 70001
Train_AverageReturn : -20.283783783783782
TimeSinceStart : 178.7203552722931
Done logging...



Beginning logging procedure...
Timestep 80001
mean reward (100 episodes) -20.337209
best mean reward -inf
running time 222.830517
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -20.21
Train_BestReturn : -20.16
TimeSinceStart : 347.01787519454956
Done logging...



Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -20.180000
best mean reward -20.160000
running time 392.915486
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -20.13684210526316
TimeSinceStart : 264.11307287216187
Done logging...



Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -20.160000
best mean reward -20.160000
running time 308.032123
Train_EnvstepsSoFar : 80001
Train_AverageReturn : -20.337209302325583
TimeSinceStart : 222.83051705360413
Done logging...



Beginning logging procedure...
Timestep 90001
mean reward (100 episodes) -20.302083
best mean reward -inf
running time 266.719901
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -20.18
Train_BestReturn : -20.16
TimeSinceStart : 392.9154860973358
Done logging...



Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -20.210000
best mean reward -20.160000
running time 437.777167
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -20.16
Train_BestReturn : -20.16
TimeSinceStart : 308.0321226119995
Done logging...



Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -20.190000
best mean reward -20.160000
running time 351.917755
Train_EnvstepsSoFar : 90001
Train_AverageReturn : -20.302083333333332
TimeSinceStart : 266.71990060806274
Done logging...



Beginning logging procedure...
Timestep 100001
mean reward (100 episodes) -20.350000
best mean reward -20.350000
running time 311.336589
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -20.21
Train_BestReturn : -20.16
TimeSinceStart : 437.7771670818329
Done logging...



Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) -20.210000
best mean reward -20.160000
running time 482.914331
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -20.19
Train_BestReturn : -20.16
TimeSinceStart : 351.9177553653717
Done logging...



Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -20.200000
best mean reward -20.160000
running time 397.116843
Train_EnvstepsSoFar : 100001
Train_AverageReturn : -20.35
Train_BestReturn : -20.35
TimeSinceStart : 311.3365886211395
Done logging...



Beginning logging procedure...
Timestep 110001
mean reward (100 episodes) -20.340000
best mean reward -20.340000
running time 356.030267
Train_EnvstepsSoFar : 140001
Train_AverageReturn : -20.21
Train_BestReturn : -20.16
TimeSinceStart : 482.91433119773865
Done logging...



Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) -20.250000
best mean reward -20.160000
running time 528.048818
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -20.2
Train_BestReturn : -20.16
TimeSinceStart : 397.1168429851532
Done logging...



Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -20.190000
best mean reward -20.160000
running time 441.871020
Train_EnvstepsSoFar : 110001
Train_AverageReturn : -20.34
Train_BestReturn : -20.34
TimeSinceStart : 356.0302667617798
Done logging...



Beginning logging procedure...
Timestep 120001
mean reward (100 episodes) -20.320000
best mean reward -20.320000
running time 402.564278
Train_EnvstepsSoFar : 150001
Train_AverageReturn : -20.25
Train_BestReturn : -20.16
TimeSinceStart : 528.0488176345825
Done logging...



Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) -20.310000
best mean reward -20.160000
running time 573.239101
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -20.19
Train_BestReturn : -20.16
TimeSinceStart : 441.87102031707764
Done logging...



Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) -20.190000
best mean reward -20.160000
running time 486.723492
Train_EnvstepsSoFar : 120001
Train_AverageReturn : -20.32
Train_BestReturn : -20.32
TimeSinceStart : 402.5642783641815
Done logging...



Beginning logging procedure...
Timestep 130001
mean reward (100 episodes) -20.330000
best mean reward -20.320000
running time 447.686875
Train_EnvstepsSoFar : 160001
Train_AverageReturn : -20.31
Train_BestReturn : -20.16
TimeSinceStart : 573.2391006946564
Done logging...



Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) -20.290000
best mean reward -20.160000
running time 618.717987
Train_EnvstepsSoFar : 140001
Train_AverageReturn : -20.19
Train_BestReturn : -20.16
TimeSinceStart : 486.7234923839569
Done logging...



Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) -20.220000
best mean reward -20.160000
running time 532.159472
Train_EnvstepsSoFar : 130001
Train_AverageReturn : -20.33
Train_BestReturn : -20.32
TimeSinceStart : 447.6868748664856
Done logging...



Beginning logging procedure...
Timestep 140001
mean reward (100 episodes) -20.310000
best mean reward -20.310000
running time 492.224358
Train_EnvstepsSoFar : 170001
Train_AverageReturn : -20.29
Train_BestReturn : -20.16
TimeSinceStart : 618.7179868221283
Done logging...



Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) -20.340000
best mean reward -20.160000
running time 664.532897
Train_EnvstepsSoFar : 150001
Train_AverageReturn : -20.22
Train_BestReturn : -20.16
TimeSinceStart : 532.1594722270966
Done logging...



Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) -20.290000
best mean reward -20.160000
running time 577.442695
Train_EnvstepsSoFar : 140001
Train_AverageReturn : -20.31
Train_BestReturn : -20.31
TimeSinceStart : 492.2243580818176
Done logging...



Beginning logging procedure...
Timestep 150001
mean reward (100 episodes) -20.300000
best mean reward -20.300000
running time 536.979297
Train_EnvstepsSoFar : 180001
Train_AverageReturn : -20.34
Train_BestReturn : -20.16
TimeSinceStart : 664.5328965187073
Done logging...



Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) -20.310000
best mean reward -20.160000
running time 711.009910
Train_EnvstepsSoFar : 160001
Train_AverageReturn : -20.29
Train_BestReturn : -20.16
TimeSinceStart : 577.4426946640015
Done logging...



Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) -20.270000
best mean reward -20.160000
running time 622.780383
Train_EnvstepsSoFar : 150001
Train_AverageReturn : -20.3
Train_BestReturn : -20.3
TimeSinceStart : 536.9792966842651
Done logging...



Beginning logging procedure...
Timestep 160001
mean reward (100 episodes) -20.300000
best mean reward -20.300000
running time 582.783052
Train_EnvstepsSoFar : 190001
Train_AverageReturn : -20.31
Train_BestReturn : -20.16
TimeSinceStart : 711.0099101066589
Done logging...



Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) -20.330000
best mean reward -20.160000
running time 757.496080
Train_EnvstepsSoFar : 170001
Train_AverageReturn : -20.27
Train_BestReturn : -20.16
TimeSinceStart : 622.7803828716278
Done logging...



Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) -20.270000
best mean reward -20.160000
running time 667.998872
Train_EnvstepsSoFar : 160001
Train_AverageReturn : -20.3
Train_BestReturn : -20.3
TimeSinceStart : 582.7830517292023
Done logging...



Beginning logging procedure...
Timestep 170001
mean reward (100 episodes) -20.290000
best mean reward -20.290000
running time 628.563578
Train_EnvstepsSoFar : 200001
Train_AverageReturn : -20.33
Train_BestReturn : -20.16
TimeSinceStart : 757.4960799217224
Done logging...



Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) -20.310000
best mean reward -20.160000
running time 805.226066
Train_EnvstepsSoFar : 180001
Train_AverageReturn : -20.27
Train_BestReturn : -20.16
TimeSinceStart : 667.9988722801208
Done logging...



Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) -20.310000
best mean reward -20.160000
running time 712.937616
Train_EnvstepsSoFar : 170001
Train_AverageReturn : -20.29
Train_BestReturn : -20.29
TimeSinceStart : 628.5635776519775
Done logging...



Beginning logging procedure...
Timestep 180001
mean reward (100 episodes) -20.290000
best mean reward -20.290000
running time 674.361170
Train_EnvstepsSoFar : 210001
Train_AverageReturn : -20.31
Train_BestReturn : -20.16
TimeSinceStart : 805.2260658740997
Done logging...



Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) -20.300000
best mean reward -20.160000
running time 851.482942
Train_EnvstepsSoFar : 190001
Train_AverageReturn : -20.31
Train_BestReturn : -20.16
TimeSinceStart : 712.9376163482666
Done logging...



Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) -20.320000
best mean reward -20.160000
running time 758.487127
Train_EnvstepsSoFar : 180001
Train_AverageReturn : -20.29
Train_BestReturn : -20.29
TimeSinceStart : 674.361170053482
Done logging...



Beginning logging procedure...
Timestep 190001
mean reward (100 episodes) -20.220000
best mean reward -20.220000
running time 719.804927
Train_EnvstepsSoFar : 220001
Train_AverageReturn : -20.3
Train_BestReturn : -20.16
TimeSinceStart : 851.4829423427582
Done logging...



Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) -20.300000
best mean reward -20.160000
running time 898.066511
Train_EnvstepsSoFar : 200001
Train_AverageReturn : -20.32
Train_BestReturn : -20.16
TimeSinceStart : 758.4871273040771
Done logging...



Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) -20.270000
best mean reward -20.160000
running time 805.169647
Train_EnvstepsSoFar : 190001
Train_AverageReturn : -20.22
Train_BestReturn : -20.22
TimeSinceStart : 719.8049266338348
Done logging...



Beginning logging procedure...
Timestep 200001
mean reward (100 episodes) -20.220000
best mean reward -20.220000
running time 765.750514
Train_EnvstepsSoFar : 230001
Train_AverageReturn : -20.3
Train_BestReturn : -20.16
TimeSinceStart : 898.0665111541748
Done logging...



Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) -20.300000
best mean reward -20.160000
running time 945.010888
Train_EnvstepsSoFar : 210001
Train_AverageReturn : -20.27
Train_BestReturn : -20.16
TimeSinceStart : 805.1696474552155
Done logging...



Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) -20.300000
best mean reward -20.160000
running time 851.201981
Train_EnvstepsSoFar : 200001
Train_AverageReturn : -20.22
Train_BestReturn : -20.22
TimeSinceStart : 765.7505135536194
Done logging...



Beginning logging procedure...
Timestep 210001
mean reward (100 episodes) -20.260000
best mean reward -20.220000
running time 812.974006
Train_EnvstepsSoFar : 240001
Train_AverageReturn : -20.3
Train_BestReturn : -20.16
TimeSinceStart : 945.010888338089
Done logging...



Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) -20.260000
best mean reward -20.160000
running time 992.297603
Train_EnvstepsSoFar : 220001
Train_AverageReturn : -20.3
Train_BestReturn : -20.16
TimeSinceStart : 851.2019808292389
Done logging...



Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) -20.260000
best mean reward -20.160000
running time 897.675448
Train_EnvstepsSoFar : 210001
Train_AverageReturn : -20.26
Train_BestReturn : -20.22
TimeSinceStart : 812.9740064144135
Done logging...



Beginning logging procedure...
Timestep 220001
mean reward (100 episodes) -20.290000
best mean reward -20.220000
running time 859.708935
Train_EnvstepsSoFar : 250001
Train_AverageReturn : -20.26
Train_BestReturn : -20.16
TimeSinceStart : 992.2976026535034
Done logging...



Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) -20.250000
best mean reward -20.160000
running time 1039.275371
Train_EnvstepsSoFar : 230001
Train_AverageReturn : -20.26
Train_BestReturn : -20.16
TimeSinceStart : 897.675448179245
Done logging...



Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) -20.260000
best mean reward -20.160000
running time 943.908324
Train_EnvstepsSoFar : 220001
Train_AverageReturn : -20.29
Train_BestReturn : -20.22
TimeSinceStart : 859.7089354991913
Done logging...



Beginning logging procedure...
Timestep 230001
mean reward (100 episodes) -20.300000
best mean reward -20.220000
running time 906.526592
Train_EnvstepsSoFar : 260001
Train_AverageReturn : -20.25
Train_BestReturn : -20.16
TimeSinceStart : 1039.275371313095
Done logging...



Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) -20.240000
best mean reward -20.160000
running time 1087.015049
Train_EnvstepsSoFar : 240001
Train_AverageReturn : -20.26
Train_BestReturn : -20.16
TimeSinceStart : 943.9083240032196
Done logging...



Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) -20.260000
best mean reward -20.160000
running time 990.570292
Train_EnvstepsSoFar : 230001
Train_AverageReturn : -20.3
Train_BestReturn : -20.22
TimeSinceStart : 906.5265924930573
Done logging...



Beginning logging procedure...
Timestep 240001
mean reward (100 episodes) -20.290000
best mean reward -20.220000
running time 953.446802
Train_EnvstepsSoFar : 270001
Train_AverageReturn : -20.24
Train_BestReturn : -20.16
TimeSinceStart : 1087.015048980713
Done logging...



Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) -20.280000
best mean reward -20.160000
running time 1134.834736
Train_EnvstepsSoFar : 250001
Train_AverageReturn : -20.26
Train_BestReturn : -20.16
TimeSinceStart : 990.5702919960022
Done logging...



Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) -20.240000
best mean reward -20.160000
running time 1037.244099
Train_EnvstepsSoFar : 240001
Train_AverageReturn : -20.29
Train_BestReturn : -20.22
TimeSinceStart : 953.4468021392822
Done logging...



Beginning logging procedure...
Timestep 250001
mean reward (100 episodes) -20.300000
best mean reward -20.220000
running time 1000.650622
Train_EnvstepsSoFar : 280001
Train_AverageReturn : -20.28
Train_BestReturn : -20.16
TimeSinceStart : 1134.8347363471985
Done logging...



Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) -20.220000
best mean reward -20.160000
running time 1182.762936
Train_EnvstepsSoFar : 260001
Train_AverageReturn : -20.24
Train_BestReturn : -20.16
TimeSinceStart : 1037.2440989017487
Done logging...



Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) -20.270000
best mean reward -20.160000
running time 1084.564919
Train_EnvstepsSoFar : 250001
Train_AverageReturn : -20.3
Train_BestReturn : -20.22
TimeSinceStart : 1000.6506216526031
Done logging...



Beginning logging procedure...
Timestep 260001
mean reward (100 episodes) -20.360000
best mean reward -20.220000
running time 1048.091078
Train_EnvstepsSoFar : 290001
Train_AverageReturn : -20.22
Train_BestReturn : -20.16
TimeSinceStart : 1182.762936115265
Done logging...



Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) -20.220000
best mean reward -20.160000
running time 1230.884092
Train_EnvstepsSoFar : 270001
Train_AverageReturn : -20.27
Train_BestReturn : -20.16
TimeSinceStart : 1084.5649185180664
Done logging...



Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) -20.220000
best mean reward -20.160000
running time 1131.859317
Train_EnvstepsSoFar : 260001
Train_AverageReturn : -20.36
Train_BestReturn : -20.22
TimeSinceStart : 1048.091078042984
Done logging...



Beginning logging procedure...
Timestep 270001
mean reward (100 episodes) -20.360000
best mean reward -20.220000
running time 1095.533946
Train_EnvstepsSoFar : 300001
Train_AverageReturn : -20.22
Train_BestReturn : -20.16
TimeSinceStart : 1230.8840918540955
Done logging...



Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) -20.200000
best mean reward -20.160000
running time 1279.122669
Train_EnvstepsSoFar : 280001
Train_AverageReturn : -20.22
Train_BestReturn : -20.16
TimeSinceStart : 1131.8593168258667
Done logging...



Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) -20.200000
best mean reward -20.160000
running time 1180.082173
Train_EnvstepsSoFar : 270001
Train_AverageReturn : -20.36
Train_BestReturn : -20.22
TimeSinceStart : 1095.5339455604553
Done logging...



Beginning logging procedure...
Timestep 280001
mean reward (100 episodes) -20.410000
best mean reward -20.220000
running time 1143.279094
Train_EnvstepsSoFar : 310001
Train_AverageReturn : -20.2
Train_BestReturn : -20.16
TimeSinceStart : 1279.1226687431335
Done logging...



Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) -20.140000
best mean reward -20.140000
running time 1327.430197
Train_EnvstepsSoFar : 290001
Train_AverageReturn : -20.2
Train_BestReturn : -20.16
TimeSinceStart : 1180.0821731090546
Done logging...



Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) -20.240000
best mean reward -20.160000
running time 1228.180252
Train_EnvstepsSoFar : 280001
Train_AverageReturn : -20.41
Train_BestReturn : -20.22
TimeSinceStart : 1143.279093503952
Done logging...



Beginning logging procedure...
Timestep 290001
mean reward (100 episodes) -20.350000
best mean reward -20.220000
running time 1191.249994
Train_EnvstepsSoFar : 320001
Train_AverageReturn : -20.14
Train_BestReturn : -20.14
TimeSinceStart : 1327.4301965236664
Done logging...



Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) -20.130000
best mean reward -20.130000
running time 1376.571023
Train_EnvstepsSoFar : 300001
Train_AverageReturn : -20.24
Train_BestReturn : -20.16
TimeSinceStart : 1228.1802515983582
Done logging...



Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) -20.180000
best mean reward -20.160000
running time 1276.373571
Train_EnvstepsSoFar : 290001
Train_AverageReturn : -20.35
Train_BestReturn : -20.22
TimeSinceStart : 1191.2499935626984
Done logging...



Beginning logging procedure...
Timestep 300001
mean reward (100 episodes) -20.280000
best mean reward -20.220000
running time 1239.186916
Train_EnvstepsSoFar : 330001
Train_AverageReturn : -20.13
Train_BestReturn : -20.13
TimeSinceStart : 1376.5710227489471
Done logging...



Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) -20.050000
best mean reward -20.050000
running time 1424.770609
Train_EnvstepsSoFar : 310001
Train_AverageReturn : -20.18
Train_BestReturn : -20.16
TimeSinceStart : 1276.3735709190369
Done logging...



Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) -20.180000
best mean reward -20.160000
running time 1324.880105
Train_EnvstepsSoFar : 300001
Train_AverageReturn : -20.28
Train_BestReturn : -20.22
TimeSinceStart : 1239.1869161128998
Done logging...



Beginning logging procedure...
Timestep 310001
mean reward (100 episodes) -20.280000
best mean reward -20.220000
running time 1287.365123
Train_EnvstepsSoFar : 340001
Train_AverageReturn : -20.05
Train_BestReturn : -20.05
TimeSinceStart : 1424.7706089019775
Done logging...



Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) -20.000000
best mean reward -20.000000
running time 1473.295980
Train_EnvstepsSoFar : 320001
Train_AverageReturn : -20.18
Train_BestReturn : -20.16
TimeSinceStart : 1324.8801050186157
Done logging...



Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) -20.120000
best mean reward -20.120000
running time 1374.885183
Train_EnvstepsSoFar : 310001
Train_AverageReturn : -20.28
Train_BestReturn : -20.22
TimeSinceStart : 1287.3651232719421
Done logging...



Beginning logging procedure...
Timestep 320001
mean reward (100 episodes) -20.240000
best mean reward -20.220000
running time 1336.790415
Train_EnvstepsSoFar : 350001
Train_AverageReturn : -20.0
Train_BestReturn : -20.0
TimeSinceStart : 1473.2959797382355
Done logging...



Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) -19.930000
best mean reward -19.930000
running time 1522.707109
Train_EnvstepsSoFar : 330001
Train_AverageReturn : -20.12
Train_BestReturn : -20.12
TimeSinceStart : 1374.8851828575134
Done logging...



Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) -20.130000
best mean reward -20.120000
running time 1423.443118
Train_EnvstepsSoFar : 320001
Train_AverageReturn : -20.24
Train_BestReturn : -20.22
TimeSinceStart : 1336.7904148101807
Done logging...



Beginning logging procedure...
Timestep 330001
mean reward (100 episodes) -20.200000
best mean reward -20.200000
running time 1385.843817
Train_EnvstepsSoFar : 360001
Train_AverageReturn : -19.93
Train_BestReturn : -19.93
TimeSinceStart : 1522.707109451294
Done logging...



Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) -19.960000
best mean reward -19.930000
running time 1572.207724
Train_EnvstepsSoFar : 340001
Train_AverageReturn : -20.13
Train_BestReturn : -20.12
TimeSinceStart : 1423.443118095398
Done logging...



Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) -20.160000
best mean reward -20.120000
running time 1472.095542
Train_EnvstepsSoFar : 330001
Train_AverageReturn : -20.2
Train_BestReturn : -20.2
TimeSinceStart : 1385.8438167572021
Done logging...



Beginning logging procedure...
Timestep 340001
mean reward (100 episodes) -20.170000
best mean reward -20.170000
running time 1434.362558
Train_EnvstepsSoFar : 370001
Train_AverageReturn : -19.96
Train_BestReturn : -19.93
TimeSinceStart : 1572.2077243328094
Done logging...



Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) -19.870000
best mean reward -19.870000
running time 1622.455743
Train_EnvstepsSoFar : 350001
Train_AverageReturn : -20.16
Train_BestReturn : -20.12
TimeSinceStart : 1472.0955424308777
Done logging...



Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) -20.080000
best mean reward -20.080000
running time 1521.394290
Train_EnvstepsSoFar : 340001
Train_AverageReturn : -20.17
Train_BestReturn : -20.17
TimeSinceStart : 1434.3625581264496
Done logging...



Beginning logging procedure...
Timestep 350001
mean reward (100 episodes) -20.120000
best mean reward -20.120000
running time 1483.417427
Train_EnvstepsSoFar : 380001
Train_AverageReturn : -19.87
Train_BestReturn : -19.87
TimeSinceStart : 1622.4557428359985
Done logging...



Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) -19.920000
best mean reward -19.870000
running time 1672.335640
Train_EnvstepsSoFar : 360001
Train_AverageReturn : -20.08
Train_BestReturn : -20.08
TimeSinceStart : 1521.3942897319794
Done logging...



Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) -20.040000
best mean reward -20.040000
running time 1571.148597
Train_EnvstepsSoFar : 350001
Train_AverageReturn : -20.12
Train_BestReturn : -20.12
TimeSinceStart : 1483.4174270629883
Done logging...



Beginning logging procedure...
Timestep 360001
mean reward (100 episodes) -20.060000
best mean reward -20.060000
running time 1532.512876
Train_EnvstepsSoFar : 390001
Train_AverageReturn : -19.92
Train_BestReturn : -19.87
TimeSinceStart : 1672.3356404304504
Done logging...



Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) -19.860000
best mean reward -19.860000
running time 1722.457956
Train_EnvstepsSoFar : 370001
Train_AverageReturn : -20.04
Train_BestReturn : -20.04
TimeSinceStart : 1571.148597240448
Done logging...



Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) -20.040000
best mean reward -20.040000
running time 1621.165493
Train_EnvstepsSoFar : 360001
Train_AverageReturn : -20.06
Train_BestReturn : -20.06
TimeSinceStart : 1532.5128757953644
Done logging...



Beginning logging procedure...
Timestep 370001
mean reward (100 episodes) -20.050000
best mean reward -20.050000
running time 1582.158748
Train_EnvstepsSoFar : 400001
Train_AverageReturn : -19.86
Train_BestReturn : -19.86
TimeSinceStart : 1722.4579560756683
Done logging...



Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) -19.910000
best mean reward -19.860000
running time 1772.610281
Train_EnvstepsSoFar : 380001
Train_AverageReturn : -20.04
Train_BestReturn : -20.04
TimeSinceStart : 1621.1654925346375
Done logging...



Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) -20.020000
best mean reward -20.020000
running time 1671.548190
Train_EnvstepsSoFar : 370001
Train_AverageReturn : -20.05
Train_BestReturn : -20.05
TimeSinceStart : 1582.1587476730347
Done logging...



Beginning logging procedure...
Timestep 380001
mean reward (100 episodes) -20.030000
best mean reward -20.030000
running time 1632.053279
Train_EnvstepsSoFar : 410001
Train_AverageReturn : -19.91
Train_BestReturn : -19.86
TimeSinceStart : 1772.6102814674377
Done logging...



Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) -19.870000
best mean reward -19.860000
running time 1823.750629
Train_EnvstepsSoFar : 390001
Train_AverageReturn : -20.02
Train_BestReturn : -20.02
TimeSinceStart : 1671.5481901168823
Done logging...



Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) -19.990000
best mean reward -19.990000
running time 1721.426630
Train_EnvstepsSoFar : 380001
Train_AverageReturn : -20.03
Train_BestReturn : -20.03
TimeSinceStart : 1632.053278684616
Done logging...



Beginning logging procedure...
Timestep 390001
mean reward (100 episodes) -20.170000
best mean reward -20.030000
running time 1681.592554
Train_EnvstepsSoFar : 420001
Train_AverageReturn : -19.87
Train_BestReturn : -19.86
TimeSinceStart : 1823.7506289482117
Done logging...



Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) -19.870000
best mean reward -19.860000
running time 1874.858999
Train_EnvstepsSoFar : 400001
Train_AverageReturn : -19.99
Train_BestReturn : -19.99
TimeSinceStart : 1721.4266295433044
Done logging...



Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) -19.910000
best mean reward -19.910000
running time 1771.644829
Train_EnvstepsSoFar : 390001
Train_AverageReturn : -20.17
Train_BestReturn : -20.03
TimeSinceStart : 1681.5925543308258
Done logging...



Beginning logging procedure...
Timestep 400001
mean reward (100 episodes) -20.210000
best mean reward -20.030000
running time 1731.395232
Train_EnvstepsSoFar : 430001
Train_AverageReturn : -19.87
Train_BestReturn : -19.86
TimeSinceStart : 1874.8589985370636
Done logging...



Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) -19.810000
best mean reward -19.810000
running time 1925.811184
Train_EnvstepsSoFar : 410001
Train_AverageReturn : -19.91
Train_BestReturn : -19.91
TimeSinceStart : 1771.6448287963867
Done logging...



Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) -19.860000
best mean reward -19.860000
running time 1821.971574
Train_EnvstepsSoFar : 400001
Train_AverageReturn : -20.21
Train_BestReturn : -20.03
TimeSinceStart : 1731.395231962204
Done logging...



Beginning logging procedure...
Timestep 410001
mean reward (100 episodes) -20.240000
best mean reward -20.030000
running time 1781.059454
Train_EnvstepsSoFar : 440001
Train_AverageReturn : -19.81
Train_BestReturn : -19.81
TimeSinceStart : 1925.8111839294434
Done logging...



Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) -19.790000
best mean reward -19.790000
running time 1977.221953
Train_EnvstepsSoFar : 420001
Train_AverageReturn : -19.86
Train_BestReturn : -19.86
TimeSinceStart : 1821.9715735912323
Done logging...



Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) -19.840000
best mean reward -19.840000
running time 1872.497679
Train_EnvstepsSoFar : 410001
Train_AverageReturn : -20.24
Train_BestReturn : -20.03
TimeSinceStart : 1781.0594544410706
Done logging...



Beginning logging procedure...
Timestep 420001
mean reward (100 episodes) -20.260000
best mean reward -20.030000
running time 1831.482804
Train_EnvstepsSoFar : 450001
Train_AverageReturn : -19.79
Train_BestReturn : -19.79
TimeSinceStart : 1977.2219533920288
Done logging...



Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) -19.740000
best mean reward -19.740000
running time 2028.616028
Train_EnvstepsSoFar : 430001
Train_AverageReturn : -19.84
Train_BestReturn : -19.84
TimeSinceStart : 1872.4976785182953
Done logging...



Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) -19.750000
best mean reward -19.750000
running time 1923.259581
Train_EnvstepsSoFar : 420001
Train_AverageReturn : -20.26
Train_BestReturn : -20.03
TimeSinceStart : 1831.4828040599823
Done logging...



Beginning logging procedure...
Timestep 430001
mean reward (100 episodes) -20.290000
best mean reward -20.030000
running time 1882.002720
Train_EnvstepsSoFar : 440001
Train_AverageReturn : -19.75
Train_BestReturn : -19.75
TimeSinceStart : 1923.2595813274384
Done logging...



Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) -19.690000
best mean reward -19.690000
running time 1973.686205
Train_EnvstepsSoFar : 460001
Train_AverageReturn : -19.74
Train_BestReturn : -19.74
TimeSinceStart : 2028.6160275936127
Done logging...



Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) -19.710000
best mean reward -19.710000
running time 2080.002390
Train_EnvstepsSoFar : 430001
Train_AverageReturn : -20.29
Train_BestReturn : -20.03
TimeSinceStart : 1882.0027203559875
Done logging...



Beginning logging procedure...
Timestep 440001
mean reward (100 episodes) -20.300000
best mean reward -20.030000
running time 1932.840809
Train_EnvstepsSoFar : 450001
Train_AverageReturn : -19.69
Train_BestReturn : -19.69
TimeSinceStart : 1973.6862049102783
Done logging...



Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) -19.710000
best mean reward -19.690000
running time 2024.093565
Train_EnvstepsSoFar : 470001
Train_AverageReturn : -19.71
Train_BestReturn : -19.71
TimeSinceStart : 2080.0023896694183
Done logging...



Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) -19.700000
best mean reward -19.700000
running time 2131.017347
Train_EnvstepsSoFar : 440001
Train_AverageReturn : -20.3
Train_BestReturn : -20.03
TimeSinceStart : 1932.8408086299896
Done logging...



Beginning logging procedure...
Timestep 450001
mean reward (100 episodes) -20.360000
best mean reward -20.030000
running time 1983.674233
Train_EnvstepsSoFar : 460001
Train_AverageReturn : -19.71
Train_BestReturn : -19.69
TimeSinceStart : 2024.0935652256012
Done logging...



Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) -19.670000
best mean reward -19.670000
running time 2075.157026
Train_EnvstepsSoFar : 480001
Train_AverageReturn : -19.7
Train_BestReturn : -19.7
TimeSinceStart : 2131.0173468589783
Done logging...



Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) -19.700000
best mean reward -19.700000
running time 2182.740991
Train_EnvstepsSoFar : 450001
Train_AverageReturn : -20.36
Train_BestReturn : -20.03
TimeSinceStart : 1983.6742329597473
Done logging...



Beginning logging procedure...
Timestep 460001
mean reward (100 episodes) -20.380000
best mean reward -20.030000
running time 2034.688810
Train_EnvstepsSoFar : 470001
Train_AverageReturn : -19.67
Train_BestReturn : -19.67
TimeSinceStart : 2075.157025575638
Done logging...



Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) -19.620000
best mean reward -19.620000
running time 2126.279955
Train_EnvstepsSoFar : 490001
Train_AverageReturn : -19.7
Train_BestReturn : -19.7
TimeSinceStart : 2182.7409908771515
Done logging...



Beginning logging procedure...
Timestep 500001
mean reward (100 episodes) -19.600000
best mean reward -19.600000
running time 2234.864929
Train_EnvstepsSoFar : 460001
Train_AverageReturn : -20.38
Train_BestReturn : -20.03
TimeSinceStart : 2034.6888101100922
Done logging...



Beginning logging procedure...
Timestep 470001
mean reward (100 episodes) -20.420000
best mean reward -20.030000
running time 2085.783269
Train_EnvstepsSoFar : 480001
Train_AverageReturn : -19.62
Train_BestReturn : -19.62
TimeSinceStart : 2126.2799546718597
Done logging...



Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) -19.510000
best mean reward -19.510000
running time 2178.190419
Train_EnvstepsSoFar : 500001
Train_AverageReturn : -19.6
Train_BestReturn : -19.6
TimeSinceStart : 2234.8649294376373
Done logging...



Beginning logging procedure...
Timestep 510001
mean reward (100 episodes) -19.510000
best mean reward -19.510000
running time 2287.892330
Train_EnvstepsSoFar : 470001
Train_AverageReturn : -20.42
Train_BestReturn : -20.03
TimeSinceStart : 2085.783269405365
Done logging...



Beginning logging procedure...
Timestep 480001
mean reward (100 episodes) -20.430000
best mean reward -20.030000
running time 2137.987196
Train_EnvstepsSoFar : 490001
Train_AverageReturn : -19.51
Train_BestReturn : -19.51
TimeSinceStart : 2178.190418958664
Done logging...



Beginning logging procedure...
Timestep 500001
mean reward (100 episodes) -19.490000
best mean reward -19.490000
running time 2231.127844
Train_EnvstepsSoFar : 510001
Train_AverageReturn : -19.51
Train_BestReturn : -19.51
TimeSinceStart : 2287.8923301696777
Done logging...



Beginning logging procedure...
Timestep 520001
mean reward (100 episodes) -19.510000
best mean reward -19.510000
running time 2340.100395
Train_EnvstepsSoFar : 480001
Train_AverageReturn : -20.43
Train_BestReturn : -20.03
TimeSinceStart : 2137.987196445465
Done logging...



Beginning logging procedure...
Timestep 490001
mean reward (100 episodes) -20.400000
best mean reward -20.030000
running time 2189.714380
Train_EnvstepsSoFar : 500001
Train_AverageReturn : -19.49
Train_BestReturn : -19.49
TimeSinceStart : 2231.1278438568115
Done logging...



Beginning logging procedure...
Timestep 510001
mean reward (100 episodes) -19.350000
best mean reward -19.350000
running time 2282.011112
Train_EnvstepsSoFar : 520001
Train_AverageReturn : -19.51
Train_BestReturn : -19.51
TimeSinceStart : 2340.1003952026367
Done logging...



Beginning logging procedure...
Timestep 530001
mean reward (100 episodes) -19.390000
best mean reward -19.390000
running time 2392.431506
Train_EnvstepsSoFar : 490001
Train_AverageReturn : -20.4
Train_BestReturn : -20.03
TimeSinceStart : 2189.714379787445
Done logging...



Beginning logging procedure...
Timestep 500001
mean reward (100 episodes) -20.480000
best mean reward -20.030000
running time 2241.160074
Train_EnvstepsSoFar : 510001
Train_AverageReturn : -19.35
Train_BestReturn : -19.35
TimeSinceStart : 2282.011111974716
Done logging...



Beginning logging procedure...
Timestep 520001
mean reward (100 episodes) -19.290000
best mean reward -19.290000
running time 2334.162366
Train_EnvstepsSoFar : 530001
Train_AverageReturn : -19.39
Train_BestReturn : -19.39
TimeSinceStart : 2392.4315056800842
Done logging...



Beginning logging procedure...
Timestep 540001
mean reward (100 episodes) -19.410000
best mean reward -19.390000
running time 2445.004438
Train_EnvstepsSoFar : 500001
Train_AverageReturn : -20.48
Train_BestReturn : -20.03
TimeSinceStart : 2241.1600737571716
Done logging...



Beginning logging procedure...
Timestep 510001
mean reward (100 episodes) -20.460000
best mean reward -20.030000
running time 2292.874307
Train_EnvstepsSoFar : 520001
Train_AverageReturn : -19.29
Train_BestReturn : -19.29
TimeSinceStart : 2334.162365913391
Done logging...



Beginning logging procedure...
Timestep 530001
mean reward (100 episodes) -19.260000
best mean reward -19.260000
running time 2386.264807
Train_EnvstepsSoFar : 540001
Train_AverageReturn : -19.41
Train_BestReturn : -19.39
TimeSinceStart : 2445.0044384002686
Done logging...



Beginning logging procedure...
Timestep 550001
mean reward (100 episodes) -19.300000
best mean reward -19.300000
running time 2497.764468
Train_EnvstepsSoFar : 510001
Train_AverageReturn : -20.46
Train_BestReturn : -20.03
TimeSinceStart : 2292.874307155609
Done logging...



Beginning logging procedure...
Timestep 520001
mean reward (100 episodes) -20.380000
best mean reward -20.030000
running time 2344.675441
Train_EnvstepsSoFar : 530001
Train_AverageReturn : -19.26
Train_BestReturn : -19.26
TimeSinceStart : 2386.2648074626923
Done logging...



Beginning logging procedure...
Timestep 540001
mean reward (100 episodes) -19.280000
best mean reward -19.260000
running time 2438.915200
Train_EnvstepsSoFar : 550001
Train_AverageReturn : -19.3
Train_BestReturn : -19.3
TimeSinceStart : 2497.764468193054
Done logging...



Beginning logging procedure...
Timestep 560001
mean reward (100 episodes) -19.340000
best mean reward -19.300000
running time 2550.367175
Train_EnvstepsSoFar : 520001
Train_AverageReturn : -20.38
Train_BestReturn : -20.03
TimeSinceStart : 2344.6754405498505
Done logging...



Beginning logging procedure...
Timestep 530001
mean reward (100 episodes) -20.360000
best mean reward -20.030000
running time 2396.921990
Train_EnvstepsSoFar : 540001
Train_AverageReturn : -19.28
Train_BestReturn : -19.26
TimeSinceStart : 2438.9151997566223
Done logging...



Beginning logging procedure...
Timestep 550001
mean reward (100 episodes) -19.200000
best mean reward -19.200000
running time 2491.717314
Train_EnvstepsSoFar : 560001
Train_AverageReturn : -19.34
Train_BestReturn : -19.3
TimeSinceStart : 2550.3671753406525
Done logging...



Beginning logging procedure...
Timestep 570001
mean reward (100 episodes) -19.240000
best mean reward -19.240000
running time 2603.522034
Train_EnvstepsSoFar : 530001
Train_AverageReturn : -20.36
Train_BestReturn : -20.03
TimeSinceStart : 2396.9219896793365
Done logging...



Beginning logging procedure...
Timestep 540001
mean reward (100 episodes) -20.360000
best mean reward -20.030000
running time 2449.510794
Train_EnvstepsSoFar : 550001
Train_AverageReturn : -19.2
Train_BestReturn : -19.2
TimeSinceStart : 2491.717313528061
Done logging...



Beginning logging procedure...
Timestep 560001
mean reward (100 episodes) -19.230000
best mean reward -19.200000
running time 2544.449274
Train_EnvstepsSoFar : 570001
Train_AverageReturn : -19.24
Train_BestReturn : -19.24
TimeSinceStart : 2603.5220341682434
Done logging...



Beginning logging procedure...
Timestep 580001
mean reward (100 episodes) -19.220000
best mean reward -19.220000
running time 2656.396573
Train_EnvstepsSoFar : 540001
Train_AverageReturn : -20.36
Train_BestReturn : -20.03
TimeSinceStart : 2449.510793685913
Done logging...



Beginning logging procedure...
Timestep 550001
mean reward (100 episodes) -20.350000
best mean reward -20.030000
running time 2502.010018
Train_EnvstepsSoFar : 560001
Train_AverageReturn : -19.23
Train_BestReturn : -19.2
TimeSinceStart : 2544.4492738246918
Done logging...



Beginning logging procedure...
Timestep 570001
mean reward (100 episodes) -19.170000
best mean reward -19.170000
running time 2597.657168
Train_EnvstepsSoFar : 580001
Train_AverageReturn : -19.22
Train_BestReturn : -19.22
TimeSinceStart : 2656.3965730667114
Done logging...



Beginning logging procedure...
Timestep 590001
mean reward (100 episodes) -19.120000
best mean reward -19.120000
running time 2709.495193
Train_EnvstepsSoFar : 550001
Train_AverageReturn : -20.35
Train_BestReturn : -20.03
TimeSinceStart : 2502.0100181102753
Done logging...



Beginning logging procedure...
Timestep 560001
mean reward (100 episodes) -20.350000
best mean reward -20.030000
running time 2555.288327
Train_EnvstepsSoFar : 570001
Train_AverageReturn : -19.17
Train_BestReturn : -19.17
TimeSinceStart : 2597.657167673111
Done logging...



Beginning logging procedure...
Timestep 580001
mean reward (100 episodes) -19.120000
best mean reward -19.120000
running time 2650.425414
Train_EnvstepsSoFar : 590001
Train_AverageReturn : -19.12
Train_BestReturn : -19.12
TimeSinceStart : 2709.4951927661896
Done logging...



Beginning logging procedure...
Timestep 600001
mean reward (100 episodes) -19.050000
best mean reward -19.050000
running time 2762.775012
Train_EnvstepsSoFar : 560001
Train_AverageReturn : -20.35
Train_BestReturn : -20.03
TimeSinceStart : 2555.2883265018463
Done logging...



Beginning logging procedure...
Timestep 570001
mean reward (100 episodes) -20.330000
best mean reward -20.030000
running time 2608.972065
Train_EnvstepsSoFar : 580001
Train_AverageReturn : -19.12
Train_BestReturn : -19.12
TimeSinceStart : 2650.4254138469696
Done logging...



Beginning logging procedure...
Timestep 590001
mean reward (100 episodes) -19.120000
best mean reward -19.120000
running time 2703.473554
Train_EnvstepsSoFar : 600001
Train_AverageReturn : -19.05
Train_BestReturn : -19.05
TimeSinceStart : 2762.7750124931335
Done logging...



Beginning logging procedure...
Timestep 610001
mean reward (100 episodes) -19.000000
best mean reward -19.000000
running time 2816.079170
Train_EnvstepsSoFar : 570001
Train_AverageReturn : -20.33
Train_BestReturn : -20.03
TimeSinceStart : 2608.972064971924
Done logging...



Beginning logging procedure...
Timestep 580001
mean reward (100 episodes) -20.300000
best mean reward -20.030000
running time 2661.279426
Train_EnvstepsSoFar : 590001
Train_AverageReturn : -19.12
Train_BestReturn : -19.12
TimeSinceStart : 2703.4735536575317
Done logging...



Beginning logging procedure...
Timestep 600001
mean reward (100 episodes) -19.120000
best mean reward -19.120000
running time 2756.676988
Train_EnvstepsSoFar : 610001
Train_AverageReturn : -19.0
Train_BestReturn : -19.0
TimeSinceStart : 2816.0791704654694
Done logging...



Beginning logging procedure...
Timestep 620001
mean reward (100 episodes) -18.880000
best mean reward -18.880000
running time 2870.150933
Train_EnvstepsSoFar : 580001
Train_AverageReturn : -20.3
Train_BestReturn : -20.03
TimeSinceStart : 2661.2794258594513
Done logging...



Beginning logging procedure...
Timestep 590001
mean reward (100 episodes) -20.250000
best mean reward -20.030000
running time 2714.387181
Train_EnvstepsSoFar : 600001
Train_AverageReturn : -19.12
Train_BestReturn : -19.12
TimeSinceStart : 2756.6769876480103
Done logging...



Beginning logging procedure...
Timestep 610001
mean reward (100 episodes) -19.080000
best mean reward -19.080000
running time 2810.290823
Train_EnvstepsSoFar : 620001
Train_AverageReturn : -18.88
Train_BestReturn : -18.88
TimeSinceStart : 2870.1509330272675
Done logging...



Beginning logging procedure...
Timestep 630001
mean reward (100 episodes) -18.810000
best mean reward -18.810000
running time 2923.790066
Train_EnvstepsSoFar : 590001
Train_AverageReturn : -20.25
Train_BestReturn : -20.03
TimeSinceStart : 2714.3871808052063
Done logging...



Beginning logging procedure...
Timestep 600001
mean reward (100 episodes) -20.280000
best mean reward -20.030000
running time 2767.696254
Train_EnvstepsSoFar : 610001
Train_AverageReturn : -19.08
Train_BestReturn : -19.08
TimeSinceStart : 2810.2908227443695
Done logging...



Beginning logging procedure...
Timestep 620001
mean reward (100 episodes) -19.050000
best mean reward -19.050000
running time 2863.653525
Train_EnvstepsSoFar : 630001
Train_AverageReturn : -18.81
Train_BestReturn : -18.81
TimeSinceStart : 2923.790066242218
Done logging...



Beginning logging procedure...
Timestep 640001
mean reward (100 episodes) -18.790000
best mean reward -18.790000
running time 2977.731504
Train_EnvstepsSoFar : 600001
Train_AverageReturn : -20.28
Train_BestReturn : -20.03
TimeSinceStart : 2767.696254014969
Done logging...



Beginning logging procedure...
Timestep 610001
mean reward (100 episodes) -20.320000
best mean reward -20.030000
running time 2821.318795
Train_EnvstepsSoFar : 620001
Train_AverageReturn : -19.05
Train_BestReturn : -19.05
TimeSinceStart : 2863.6535246372223
Done logging...



Beginning logging procedure...
Timestep 630001
mean reward (100 episodes) -19.120000
best mean reward -19.050000
running time 2917.259448
Train_EnvstepsSoFar : 640001
Train_AverageReturn : -18.79
Train_BestReturn : -18.79
TimeSinceStart : 2977.731503725052
Done logging...



Beginning logging procedure...
Timestep 650001
mean reward (100 episodes) -18.660000
best mean reward -18.660000
running time 3031.739787
Train_EnvstepsSoFar : 610001
Train_AverageReturn : -20.32
Train_BestReturn : -20.03
TimeSinceStart : 2821.3187952041626
Done logging...



Beginning logging procedure...
Timestep 620001
mean reward (100 episodes) -20.360000
best mean reward -20.030000
running time 2874.780080
Train_EnvstepsSoFar : 630001
Train_AverageReturn : -19.12
Train_BestReturn : -19.05
TimeSinceStart : 2917.2594475746155
Done logging...



Beginning logging procedure...
Timestep 640001
mean reward (100 episodes) -19.030000
best mean reward -19.030000
running time 2971.498489
Train_EnvstepsSoFar : 650001
Train_AverageReturn : -18.66
Train_BestReturn : -18.66
TimeSinceStart : 3031.7397866249084
Done logging...



Beginning logging procedure...
Timestep 660001
mean reward (100 episodes) -18.510000
best mean reward -18.510000
running time 3086.217712
Train_EnvstepsSoFar : 620001
Train_AverageReturn : -20.36
Train_BestReturn : -20.03
TimeSinceStart : 2874.780080318451
Done logging...



Beginning logging procedure...
Timestep 630001
mean reward (100 episodes) -20.330000
best mean reward -20.030000
running time 2928.486600
Train_EnvstepsSoFar : 640001
Train_AverageReturn : -19.03
Train_BestReturn : -19.03
TimeSinceStart : 2971.498489379883
Done logging...



Beginning logging procedure...
Timestep 650001
mean reward (100 episodes) -18.980000
best mean reward -18.980000
running time 3026.718583
Train_EnvstepsSoFar : 660001
Train_AverageReturn : -18.51
Train_BestReturn : -18.51
TimeSinceStart : 3086.217711687088
Done logging...



Beginning logging procedure...
Timestep 670001
mean reward (100 episodes) -18.360000
best mean reward -18.360000
running time 3140.666210
Train_EnvstepsSoFar : 630001
Train_AverageReturn : -20.33
Train_BestReturn : -20.03
TimeSinceStart : 2928.4866003990173
Done logging...



Beginning logging procedure...
Timestep 640001
mean reward (100 episodes) -20.360000
best mean reward -20.030000
running time 2982.239964
Train_EnvstepsSoFar : 650001
Train_AverageReturn : -18.98
Train_BestReturn : -18.98
TimeSinceStart : 3026.718583345413
Done logging...



Beginning logging procedure...
Timestep 660001
mean reward (100 episodes) -18.920000
best mean reward -18.920000
running time 3081.554920
Train_EnvstepsSoFar : 670001
Train_AverageReturn : -18.36
Train_BestReturn : -18.36
TimeSinceStart : 3140.6662101745605
Done logging...



Beginning logging procedure...
Timestep 680001
mean reward (100 episodes) -18.140000
best mean reward -18.140000
running time 3195.981136
Train_EnvstepsSoFar : 640001
Train_AverageReturn : -20.36
Train_BestReturn : -20.03
TimeSinceStart : 2982.2399644851685
Done logging...



Beginning logging procedure...
Timestep 650001
mean reward (100 episodes) -20.360000
best mean reward -20.030000
running time 3036.155269
Train_EnvstepsSoFar : 660001
Train_AverageReturn : -18.92
Train_BestReturn : -18.92
TimeSinceStart : 3081.554919719696
Done logging...



Beginning logging procedure...
Timestep 670001
mean reward (100 episodes) -18.960000
best mean reward -18.920000
running time 3136.890516
Train_EnvstepsSoFar : 680001
Train_AverageReturn : -18.14
Train_BestReturn : -18.14
TimeSinceStart : 3195.9811358451843
Done logging...



Beginning logging procedure...
Timestep 690001
mean reward (100 episodes) -17.950000
best mean reward -17.950000
running time 3250.631971
Train_EnvstepsSoFar : 650001
Train_AverageReturn : -20.36
Train_BestReturn : -20.03
TimeSinceStart : 3036.155269384384
Done logging...



Beginning logging procedure...
Timestep 660001
mean reward (100 episodes) -20.330000
best mean reward -20.030000
running time 3090.259814
Train_EnvstepsSoFar : 670001
Train_AverageReturn : -18.96
Train_BestReturn : -18.92
TimeSinceStart : 3136.8905160427094
Done logging...



Beginning logging procedure...
Timestep 680001
mean reward (100 episodes) -18.960000
best mean reward -18.920000
running time 3191.738202
Train_EnvstepsSoFar : 690001
Train_AverageReturn : -17.95
Train_BestReturn : -17.95
TimeSinceStart : 3250.631971359253
Done logging...



Beginning logging procedure...
Timestep 700001
mean reward (100 episodes) -17.850000
best mean reward -17.850000
running time 3305.719392
Train_EnvstepsSoFar : 660001
Train_AverageReturn : -20.33
Train_BestReturn : -20.03
TimeSinceStart : 3090.2598140239716
Done logging...



Beginning logging procedure...
Timestep 670001
mean reward (100 episodes) -20.350000
best mean reward -20.030000
running time 3144.585359
Train_EnvstepsSoFar : 680001
Train_AverageReturn : -18.96
Train_BestReturn : -18.92
TimeSinceStart : 3191.7382016181946
Done logging...



Beginning logging procedure...
Timestep 690001
mean reward (100 episodes) -18.790000
best mean reward -18.790000
running time 3246.836144
Train_EnvstepsSoFar : 700001
Train_AverageReturn : -17.85
Train_BestReturn : -17.85
TimeSinceStart : 3305.719391822815
Done logging...



Beginning logging procedure...
Timestep 710001
mean reward (100 episodes) -17.760000
best mean reward -17.760000
running time 3360.911163
Train_EnvstepsSoFar : 670001
Train_AverageReturn : -20.35
Train_BestReturn : -20.03
TimeSinceStart : 3144.58535861969
Done logging...



Beginning logging procedure...
Timestep 680001
mean reward (100 episodes) -20.380000
best mean reward -20.030000
running time 3200.866300
Train_EnvstepsSoFar : 690001
Train_AverageReturn : -18.79
Train_BestReturn : -18.79
TimeSinceStart : 3246.836144208908
Done logging...



Beginning logging procedure...
Timestep 700001
mean reward (100 episodes) -18.670000
best mean reward -18.670000
running time 3302.312859
Train_EnvstepsSoFar : 710001
Train_AverageReturn : -17.76
Train_BestReturn : -17.76
TimeSinceStart : 3360.911163330078
Done logging...



Beginning logging procedure...
Timestep 720001
mean reward (100 episodes) -17.640000
best mean reward -17.640000
running time 3416.118664
Train_EnvstepsSoFar : 680001
Train_AverageReturn : -20.38
Train_BestReturn : -20.03
TimeSinceStart : 3200.8663001060486
Done logging...



Beginning logging procedure...
Timestep 690001
mean reward (100 episodes) -20.410000
best mean reward -20.030000
running time 3255.987247
Train_EnvstepsSoFar : 700001
Train_AverageReturn : -18.67
Train_BestReturn : -18.67
TimeSinceStart : 3302.312858581543
Done logging...



Beginning logging procedure...
Timestep 710001
mean reward (100 episodes) -18.640000
best mean reward -18.640000
running time 3357.658532
Train_EnvstepsSoFar : 720001
Train_AverageReturn : -17.64
Train_BestReturn : -17.64
TimeSinceStart : 3416.118663549423
Done logging...



Beginning logging procedure...
Timestep 730001
mean reward (100 episodes) -17.440000
best mean reward -17.440000
running time 3471.526068
Train_EnvstepsSoFar : 690001
Train_AverageReturn : -20.41
Train_BestReturn : -20.03
TimeSinceStart : 3255.987247467041
Done logging...



Beginning logging procedure...
Timestep 700001
mean reward (100 episodes) -20.440000
best mean reward -20.030000
running time 3311.043912
Train_EnvstepsSoFar : 710001
Train_AverageReturn : -18.64
Train_BestReturn : -18.64
TimeSinceStart : 3357.6585319042206
Done logging...



Beginning logging procedure...
Timestep 720001
mean reward (100 episodes) -18.550000
best mean reward -18.550000
running time 3413.301079
Train_EnvstepsSoFar : 730001
Train_AverageReturn : -17.44
Train_BestReturn : -17.44
TimeSinceStart : 3471.526068210602
Done logging...



Beginning logging procedure...
Timestep 740001
mean reward (100 episodes) -17.320000
best mean reward -17.320000
running time 3527.415153
Train_EnvstepsSoFar : 700001
Train_AverageReturn : -20.44
Train_BestReturn : -20.03
TimeSinceStart : 3311.0439121723175
Done logging...



Beginning logging procedure...
Timestep 710001
mean reward (100 episodes) -20.420000
best mean reward -20.030000
running time 3366.151093
Train_EnvstepsSoFar : 720001
Train_AverageReturn : -18.55
Train_BestReturn : -18.55
TimeSinceStart : 3413.301078557968
Done logging...



Beginning logging procedure...
Timestep 730001
mean reward (100 episodes) -18.580000
best mean reward -18.550000
running time 3468.710794
Train_EnvstepsSoFar : 740001
Train_AverageReturn : -17.32
Train_BestReturn : -17.32
TimeSinceStart : 3527.4151525497437
Done logging...



Beginning logging procedure...
Timestep 750001
mean reward (100 episodes) -17.150000
best mean reward -17.150000
running time 3583.144983
Train_EnvstepsSoFar : 710001
Train_AverageReturn : -20.42
Train_BestReturn : -20.03
TimeSinceStart : 3366.151092529297
Done logging...



Beginning logging procedure...
Timestep 720001
mean reward (100 episodes) -20.420000
best mean reward -20.030000
running time 3421.317469
Train_EnvstepsSoFar : 730001
Train_AverageReturn : -18.58
Train_BestReturn : -18.55
TimeSinceStart : 3468.710793733597
Done logging...



Beginning logging procedure...
Timestep 740001
mean reward (100 episodes) -18.540000
best mean reward -18.540000
running time 3524.516988
Train_EnvstepsSoFar : 750001
Train_AverageReturn : -17.15
Train_BestReturn : -17.15
TimeSinceStart : 3583.144983291626
Done logging...



Beginning logging procedure...
Timestep 760001
mean reward (100 episodes) -17.040000
best mean reward -17.040000
running time 3638.723148
Train_EnvstepsSoFar : 720001
Train_AverageReturn : -20.42
Train_BestReturn : -20.03
TimeSinceStart : 3421.3174691200256
Done logging...



Beginning logging procedure...
Timestep 730001
mean reward (100 episodes) -20.410000
best mean reward -20.030000
running time 3476.675494
Train_EnvstepsSoFar : 740001
Train_AverageReturn : -18.54
Train_BestReturn : -18.54
TimeSinceStart : 3524.516987800598
Done logging...



Beginning logging procedure...
Timestep 750001
mean reward (100 episodes) -18.560000
best mean reward -18.540000
running time 3581.022855
Train_EnvstepsSoFar : 760001
Train_AverageReturn : -17.04
Train_BestReturn : -17.04
TimeSinceStart : 3638.7231476306915
Done logging...



Beginning logging procedure...
Timestep 770001
mean reward (100 episodes) -16.960000
best mean reward -16.960000
running time 3694.973132
Train_EnvstepsSoFar : 730001
Train_AverageReturn : -20.41
Train_BestReturn : -20.03
TimeSinceStart : 3476.6754944324493
Done logging...



Beginning logging procedure...
Timestep 740001
mean reward (100 episodes) -20.450000
best mean reward -20.030000
running time 3532.397057
Train_EnvstepsSoFar : 750001
Train_AverageReturn : -18.56
Train_BestReturn : -18.54
TimeSinceStart : 3581.0228550434113
Done logging...



Beginning logging procedure...
Timestep 760001
mean reward (100 episodes) -18.450000
best mean reward -18.450000
running time 3637.434060
Train_EnvstepsSoFar : 770001
Train_AverageReturn : -16.96
Train_BestReturn : -16.96
TimeSinceStart : 3694.9731316566467
Done logging...



Beginning logging procedure...
Timestep 780001
mean reward (100 episodes) -16.730000
best mean reward -16.730000
running time 3751.350846
Train_EnvstepsSoFar : 740001
Train_AverageReturn : -20.45
Train_BestReturn : -20.03
TimeSinceStart : 3532.39705657959
Done logging...



Beginning logging procedure...
Timestep 750001
mean reward (100 episodes) -20.480000
best mean reward -20.030000
running time 3588.489303
Train_EnvstepsSoFar : 760001
Train_AverageReturn : -18.45
Train_BestReturn : -18.45
TimeSinceStart : 3637.4340596199036
Done logging...



Beginning logging procedure...
Timestep 770001
mean reward (100 episodes) -18.510000
best mean reward -18.450000
running time 3695.637575
Train_EnvstepsSoFar : 780001
Train_AverageReturn : -16.73
Train_BestReturn : -16.73
TimeSinceStart : 3751.35084605217
Done logging...



Beginning logging procedure...
Timestep 790001
mean reward (100 episodes) -16.590000
best mean reward -16.590000
running time 3807.713124
Train_EnvstepsSoFar : 750001
Train_AverageReturn : -20.48
Train_BestReturn : -20.03
TimeSinceStart : 3588.489302635193
Done logging...



Beginning logging procedure...
Timestep 760001
mean reward (100 episodes) -20.470000
best mean reward -20.030000
running time 3644.665573
Train_EnvstepsSoFar : 770001
Train_AverageReturn : -18.51
Train_BestReturn : -18.45
TimeSinceStart : 3695.6375753879547
Done logging...



Beginning logging procedure...
Timestep 780001
mean reward (100 episodes) -18.410000
best mean reward -18.410000
running time 3752.021973
Train_EnvstepsSoFar : 790001
Train_AverageReturn : -16.59
Train_BestReturn : -16.59
TimeSinceStart : 3807.713123559952
Done logging...



Beginning logging procedure...
Timestep 800001
mean reward (100 episodes) -16.420000
best mean reward -16.420000
running time 3864.202953
Train_EnvstepsSoFar : 760001
Train_AverageReturn : -20.47
Train_BestReturn : -20.03
TimeSinceStart : 3644.665573120117
Done logging...



Beginning logging procedure...
Timestep 770001
mean reward (100 episodes) -20.450000
best mean reward -20.030000
running time 3700.845852
Train_EnvstepsSoFar : 780001
Train_AverageReturn : -18.41
Train_BestReturn : -18.41
TimeSinceStart : 3752.02197265625
Done logging...



Beginning logging procedure...
Timestep 790001
mean reward (100 episodes) -18.400000
best mean reward -18.400000
running time 3808.446255
Train_EnvstepsSoFar : 800001
Train_AverageReturn : -16.42
Train_BestReturn : -16.42
TimeSinceStart : 3864.2029526233673
Done logging...



Beginning logging procedure...
Timestep 810001
mean reward (100 episodes) -16.210000
best mean reward -16.210000
running time 3920.716475
Train_EnvstepsSoFar : 770001
Train_AverageReturn : -20.45
Train_BestReturn : -20.03
TimeSinceStart : 3700.845851659775
Done logging...



Beginning logging procedure...
Timestep 780001
mean reward (100 episodes) -20.480000
best mean reward -20.030000
running time 3757.068485
Train_EnvstepsSoFar : 790001
Train_AverageReturn : -18.4
Train_BestReturn : -18.4
TimeSinceStart : 3808.446254968643
Done logging...



Beginning logging procedure...
Timestep 800001
mean reward (100 episodes) -18.340000
best mean reward -18.340000
running time 3864.874190
Train_EnvstepsSoFar : 810001
Train_AverageReturn : -16.21
Train_BestReturn : -16.21
TimeSinceStart : 3920.716475009918
Done logging...



Beginning logging procedure...
Timestep 820001
mean reward (100 episodes) -16.110000
best mean reward -16.110000
running time 3977.400964
Train_EnvstepsSoFar : 780001
Train_AverageReturn : -20.48
Train_BestReturn : -20.03
TimeSinceStart : 3757.0684847831726
Done logging...



Beginning logging procedure...
Timestep 790001
mean reward (100 episodes) -20.480000
best mean reward -20.030000
running time 3813.864975
Train_EnvstepsSoFar : 800001
Train_AverageReturn : -18.34
Train_BestReturn : -18.34
TimeSinceStart : 3864.874190092087
Done logging...



Beginning logging procedure...
Timestep 810001
mean reward (100 episodes) -18.290000
best mean reward -18.290000
running time 3921.564276
Train_EnvstepsSoFar : 820001
Train_AverageReturn : -16.11
Train_BestReturn : -16.11
TimeSinceStart : 3977.4009642601013
Done logging...



Beginning logging procedure...
Timestep 830001
mean reward (100 episodes) -16.000000
best mean reward -16.000000
running time 4034.101794
Train_EnvstepsSoFar : 790001
Train_AverageReturn : -20.48
Train_BestReturn : -20.03
TimeSinceStart : 3813.8649747371674
Done logging...



Beginning logging procedure...
Timestep 800001
mean reward (100 episodes) -20.490000
best mean reward -20.030000
running time 3871.062368
Train_EnvstepsSoFar : 810001
Train_AverageReturn : -18.29
Train_BestReturn : -18.29
TimeSinceStart : 3921.5642755031586
Done logging...



Beginning logging procedure...
Timestep 820001
mean reward (100 episodes) -18.340000
best mean reward -18.290000
running time 3978.846967
Train_EnvstepsSoFar : 830001
Train_AverageReturn : -16.0
Train_BestReturn : -16.0
TimeSinceStart : 4034.1017940044403
Done logging...



Beginning logging procedure...
Timestep 840001
mean reward (100 episodes) -15.870000
best mean reward -15.870000
running time 4091.631040
Train_EnvstepsSoFar : 800001
Train_AverageReturn : -20.49
Train_BestReturn : -20.03
TimeSinceStart : 3871.0623676776886
Done logging...



Beginning logging procedure...
Timestep 810001
mean reward (100 episodes) -20.500000
best mean reward -20.030000
running time 3928.209007
Train_EnvstepsSoFar : 820001
Train_AverageReturn : -18.34
Train_BestReturn : -18.29
TimeSinceStart : 3978.8469667434692
Done logging...



Beginning logging procedure...
Timestep 830001
mean reward (100 episodes) -18.430000
best mean reward -18.290000
running time 4036.003641
Train_EnvstepsSoFar : 840001
Train_AverageReturn : -15.87
Train_BestReturn : -15.87
TimeSinceStart : 4091.6310403347015
Done logging...



Beginning logging procedure...
Timestep 850001
mean reward (100 episodes) -15.750000
best mean reward -15.750000
running time 4149.286785
Train_EnvstepsSoFar : 810001
Train_AverageReturn : -20.5
Train_BestReturn : -20.03
TimeSinceStart : 3928.2090072631836
Done logging...



Beginning logging procedure...
Timestep 820001
mean reward (100 episodes) -20.460000
best mean reward -20.030000
running time 3985.457034
Train_EnvstepsSoFar : 830001
Train_AverageReturn : -18.43
Train_BestReturn : -18.29
TimeSinceStart : 4036.00364112854
Done logging...



Beginning logging procedure...
Timestep 840001
mean reward (100 episodes) -18.490000
best mean reward -18.290000
running time 4093.189409
Train_EnvstepsSoFar : 820001
Train_AverageReturn : -20.46
Train_BestReturn : -20.03
TimeSinceStart : 3985.457034111023
Done logging...



Beginning logging procedure...
Timestep 830001
mean reward (100 episodes) -20.470000
best mean reward -20.030000
running time 4042.108429
Train_EnvstepsSoFar : 850001
Train_AverageReturn : -15.75
Train_BestReturn : -15.75
TimeSinceStart : 4149.286784887314
Done logging...



Beginning logging procedure...
Timestep 860001
mean reward (100 episodes) -15.490000
best mean reward -15.490000
running time 4207.234504
Train_EnvstepsSoFar : 840001
Train_AverageReturn : -18.49
Train_BestReturn : -18.29
TimeSinceStart : 4093.189409017563
Done logging...



Beginning logging procedure...
Timestep 850001
mean reward (100 episodes) -18.590000
best mean reward -18.290000
running time 4150.236540
Train_EnvstepsSoFar : 830001
Train_AverageReturn : -20.47
Train_BestReturn : -20.03
TimeSinceStart : 4042.1084294319153
Done logging...



Beginning logging procedure...
Timestep 840001
mean reward (100 episodes) -20.450000
best mean reward -20.030000
running time 4099.470175
Train_EnvstepsSoFar : 860001
Train_AverageReturn : -15.49
Train_BestReturn : -15.49
TimeSinceStart : 4207.234504461288
Done logging...



Beginning logging procedure...
Timestep 870001
mean reward (100 episodes) -15.300000
best mean reward -15.300000
running time 4264.952429
Train_EnvstepsSoFar : 850001
Train_AverageReturn : -18.59
Train_BestReturn : -18.29
TimeSinceStart : 4150.236540079117
Done logging...



Beginning logging procedure...
Timestep 860001
mean reward (100 episodes) -18.640000
best mean reward -18.290000
running time 4207.959341
Train_EnvstepsSoFar : 840001
Train_AverageReturn : -20.45
Train_BestReturn : -20.03
TimeSinceStart : 4099.470175027847
Done logging...



Beginning logging procedure...
Timestep 850001
mean reward (100 episodes) -20.520000
best mean reward -20.030000
running time 4157.062076
Train_EnvstepsSoFar : 870001
Train_AverageReturn : -15.3
Train_BestReturn : -15.3
TimeSinceStart : 4264.952429294586
Done logging...



Beginning logging procedure...
Timestep 880001
mean reward (100 episodes) -15.290000
best mean reward -15.290000
running time 4323.000090
Train_EnvstepsSoFar : 860001
Train_AverageReturn : -18.64
Train_BestReturn : -18.29
TimeSinceStart : 4207.959341049194
Done logging...



Beginning logging procedure...
Timestep 870001
mean reward (100 episodes) -18.770000
best mean reward -18.290000
running time 4266.100980
Train_EnvstepsSoFar : 850001
Train_AverageReturn : -20.52
Train_BestReturn : -20.03
TimeSinceStart : 4157.062075614929
Done logging...



Beginning logging procedure...
Timestep 860001
mean reward (100 episodes) -20.530000
best mean reward -20.030000
running time 4214.742680
Train_EnvstepsSoFar : 880001
Train_AverageReturn : -15.29
Train_BestReturn : -15.29
TimeSinceStart : 4323.000090122223
Done logging...



Beginning logging procedure...
Timestep 890001
mean reward (100 episodes) -15.220000
best mean reward -15.220000
running time 4383.876012
Train_EnvstepsSoFar : 870001
Train_AverageReturn : -18.77
Train_BestReturn : -18.29
TimeSinceStart : 4266.100980043411
Done logging...



Beginning logging procedure...
Timestep 880001
mean reward (100 episodes) -19.000000
best mean reward -18.290000
running time 4324.828923
Train_EnvstepsSoFar : 860001
Train_AverageReturn : -20.53
Train_BestReturn : -20.03
TimeSinceStart : 4214.742680072784
Done logging...



Beginning logging procedure...
Timestep 870001
mean reward (100 episodes) -20.510000
best mean reward -20.030000
running time 4272.626872
Train_EnvstepsSoFar : 890001
Train_AverageReturn : -15.22
Train_BestReturn : -15.22
TimeSinceStart : 4383.876012325287
Done logging...



Beginning logging procedure...
Timestep 900001
mean reward (100 episodes) -15.040000
best mean reward -15.040000
running time 4442.083463
Train_EnvstepsSoFar : 880001
Train_AverageReturn : -19.0
Train_BestReturn : -18.29
TimeSinceStart : 4324.828922510147
Done logging...



Beginning logging procedure...
Timestep 890001
mean reward (100 episodes) -19.310000
best mean reward -18.290000
running time 4383.326820
Train_EnvstepsSoFar : 870001
Train_AverageReturn : -20.51
Train_BestReturn : -20.03
TimeSinceStart : 4272.6268718242645
Done logging...



Beginning logging procedure...
Timestep 880001
mean reward (100 episodes) -20.510000
best mean reward -20.030000
running time 4331.076754
Train_EnvstepsSoFar : 900001
Train_AverageReturn : -15.04
Train_BestReturn : -15.04
TimeSinceStart : 4442.083463430405
Done logging...



Beginning logging procedure...
Timestep 910001
mean reward (100 episodes) -15.040000
best mean reward -15.040000
running time 4499.906187
Train_EnvstepsSoFar : 890001
Train_AverageReturn : -19.31
Train_BestReturn : -18.29
TimeSinceStart : 4383.326820135117
Done logging...



Beginning logging procedure...
Timestep 900001
mean reward (100 episodes) -19.380000
best mean reward -18.290000
running time 4442.232977
Train_EnvstepsSoFar : 880001
Train_AverageReturn : -20.51
Train_BestReturn : -20.03
TimeSinceStart : 4331.07675409317
Done logging...



Beginning logging procedure...
Timestep 890001
mean reward (100 episodes) -20.560000
best mean reward -20.030000
running time 4388.639093
Train_EnvstepsSoFar : 910001
Train_AverageReturn : -15.04
Train_BestReturn : -15.04
TimeSinceStart : 4499.906187057495
Done logging...



Beginning logging procedure...
Timestep 920001
mean reward (100 episodes) -14.950000
best mean reward -14.950000
running time 4558.748195
Train_EnvstepsSoFar : 900001
Train_AverageReturn : -19.38
Train_BestReturn : -18.29
TimeSinceStart : 4442.232977390289
Done logging...



Beginning logging procedure...
Timestep 910001
mean reward (100 episodes) -19.530000
best mean reward -18.290000
running time 4500.827445
Train_EnvstepsSoFar : 890001
Train_AverageReturn : -20.56
Train_BestReturn : -20.03
TimeSinceStart : 4388.639092683792
Done logging...



Beginning logging procedure...
Timestep 900001
mean reward (100 episodes) -20.590000
best mean reward -20.030000
running time 4447.352034
Train_EnvstepsSoFar : 920001
Train_AverageReturn : -14.95
Train_BestReturn : -14.95
TimeSinceStart : 4558.748195409775
Done logging...



Beginning logging procedure...
Timestep 930001
mean reward (100 episodes) -14.860000
best mean reward -14.860000
running time 4617.278892
Train_EnvstepsSoFar : 910001
Train_AverageReturn : -19.53
Train_BestReturn : -18.29
TimeSinceStart : 4500.827444791794
Done logging...



Beginning logging procedure...
Timestep 920001
mean reward (100 episodes) -19.620000
best mean reward -18.290000
running time 4560.257994
Train_EnvstepsSoFar : 900001
Train_AverageReturn : -20.59
Train_BestReturn : -20.03
TimeSinceStart : 4447.352033853531
Done logging...



Beginning logging procedure...
Timestep 910001
mean reward (100 episodes) -20.590000
best mean reward -20.030000
running time 4506.203910
Train_EnvstepsSoFar : 930001
Train_AverageReturn : -14.86
Train_BestReturn : -14.86
TimeSinceStart : 4617.278892040253
Done logging...



Beginning logging procedure...
Timestep 940001
mean reward (100 episodes) -14.760000
best mean reward -14.760000
running time 4676.107283
Train_EnvstepsSoFar : 920001
Train_AverageReturn : -19.62
Train_BestReturn : -18.29
TimeSinceStart : 4560.257994413376
Done logging...



Beginning logging procedure...
Timestep 930001
mean reward (100 episodes) -19.760000
best mean reward -18.290000
running time 4618.917518
Train_EnvstepsSoFar : 910001
Train_AverageReturn : -20.59
Train_BestReturn : -20.03
TimeSinceStart : 4506.203910112381
Done logging...



Beginning logging procedure...
Timestep 920001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 4564.829187
Train_EnvstepsSoFar : 940001
Train_AverageReturn : -14.76
Train_BestReturn : -14.76
TimeSinceStart : 4676.10728263855
Done logging...



Beginning logging procedure...
Timestep 950001
mean reward (100 episodes) -14.470000
best mean reward -14.470000
running time 4735.061208
Train_EnvstepsSoFar : 930001
Train_AverageReturn : -19.76
Train_BestReturn : -18.29
TimeSinceStart : 4618.9175181388855
Done logging...



Beginning logging procedure...
Timestep 940001
mean reward (100 episodes) -19.880000
best mean reward -18.290000
running time 4678.076913
Train_EnvstepsSoFar : 920001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 4564.829186916351
Done logging...



Beginning logging procedure...
Timestep 930001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 4623.693932
Train_EnvstepsSoFar : 950001
Train_AverageReturn : -14.47
Train_BestReturn : -14.47
TimeSinceStart : 4735.061207771301
Done logging...



Beginning logging procedure...
Timestep 960001
mean reward (100 episodes) -14.270000
best mean reward -14.270000
running time 4793.897008
Train_EnvstepsSoFar : 940001
Train_AverageReturn : -19.88
Train_BestReturn : -18.29
TimeSinceStart : 4678.076913356781
Done logging...



Beginning logging procedure...
Timestep 950001
mean reward (100 episodes) -19.990000
best mean reward -18.290000
running time 4737.574355
Train_EnvstepsSoFar : 930001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 4623.693932294846
Done logging...



Beginning logging procedure...
Timestep 940001
mean reward (100 episodes) -20.550000
best mean reward -20.030000
running time 4682.285084
Train_EnvstepsSoFar : 960001
Train_AverageReturn : -14.27
Train_BestReturn : -14.27
TimeSinceStart : 4793.8970079422
Done logging...



Beginning logging procedure...
Timestep 970001
mean reward (100 episodes) -13.900000
best mean reward -13.900000
running time 4852.831802
Train_EnvstepsSoFar : 950001
Train_AverageReturn : -19.99
Train_BestReturn : -18.29
TimeSinceStart : 4737.574355363846
Done logging...



Beginning logging procedure...
Timestep 960001
mean reward (100 episodes) -20.020000
best mean reward -18.290000
running time 4797.072467
Train_EnvstepsSoFar : 940001
Train_AverageReturn : -20.55
Train_BestReturn : -20.03
TimeSinceStart : 4682.285083532333
Done logging...



Beginning logging procedure...
Timestep 950001
mean reward (100 episodes) -20.620000
best mean reward -20.030000
running time 4741.301825
Train_EnvstepsSoFar : 970001
Train_AverageReturn : -13.9
Train_BestReturn : -13.9
TimeSinceStart : 4852.831801652908
Done logging...



Beginning logging procedure...
Timestep 980001
mean reward (100 episodes) -13.800000
best mean reward -13.800000
running time 4912.194927
Train_EnvstepsSoFar : 960001
Train_AverageReturn : -20.02
Train_BestReturn : -18.29
TimeSinceStart : 4797.072467088699
Done logging...



Beginning logging procedure...
Timestep 970001
mean reward (100 episodes) -19.960000
best mean reward -18.290000
running time 4856.855890
Train_EnvstepsSoFar : 950001
Train_AverageReturn : -20.62
Train_BestReturn : -20.03
TimeSinceStart : 4741.301825284958
Done logging...



Beginning logging procedure...
Timestep 960001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 4801.071654
Train_EnvstepsSoFar : 980001
Train_AverageReturn : -13.8
Train_BestReturn : -13.8
TimeSinceStart : 4912.194926977158
Done logging...



Beginning logging procedure...
Timestep 990001
mean reward (100 episodes) -13.780000
best mean reward -13.780000
running time 4971.692688
Train_EnvstepsSoFar : 970001
Train_AverageReturn : -19.96
Train_BestReturn : -18.29
TimeSinceStart : 4856.855890274048
Done logging...



Beginning logging procedure...
Timestep 980001
mean reward (100 episodes) -19.900000
best mean reward -18.290000
running time 4916.456696
Train_EnvstepsSoFar : 960001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 4801.071653842926
Done logging...



Beginning logging procedure...
Timestep 970001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 4860.336275
Train_EnvstepsSoFar : 990001
Train_AverageReturn : -13.78
Train_BestReturn : -13.78
TimeSinceStart : 4971.692688465118
Done logging...



Beginning logging procedure...
Timestep 1000001
mean reward (100 episodes) -13.650000
best mean reward -13.650000
running time 5031.225420
Train_EnvstepsSoFar : 980001
Train_AverageReturn : -19.9
Train_BestReturn : -18.29
TimeSinceStart : 4916.456695795059
Done logging...



Beginning logging procedure...
Timestep 990001
mean reward (100 episodes) -19.890000
best mean reward -18.290000
running time 4976.212723
Train_EnvstepsSoFar : 970001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 4860.336275100708
Done logging...



Beginning logging procedure...
Timestep 980001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 4919.667272
Train_EnvstepsSoFar : 1000001
Train_AverageReturn : -13.65
Train_BestReturn : -13.65
TimeSinceStart : 5031.225419521332
Done logging...



Beginning logging procedure...
Timestep 1010001
mean reward (100 episodes) -13.360000
best mean reward -13.360000
running time 5090.708965
Train_EnvstepsSoFar : 990001
Train_AverageReturn : -19.89
Train_BestReturn : -18.29
TimeSinceStart : 4976.2127232551575
Done logging...



Beginning logging procedure...
Timestep 1000001
mean reward (100 episodes) -19.970000
best mean reward -18.290000
running time 5036.094303
Train_EnvstepsSoFar : 980001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 4919.667272090912
Done logging...



Beginning logging procedure...
Timestep 990001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 4978.822392
Train_EnvstepsSoFar : 1010001
Train_AverageReturn : -13.36
Train_BestReturn : -13.36
TimeSinceStart : 5090.7089648246765
Done logging...



Beginning logging procedure...
Timestep 1020001
mean reward (100 episodes) -13.200000
best mean reward -13.200000
running time 5150.154864
Train_EnvstepsSoFar : 1000001
Train_AverageReturn : -19.97
Train_BestReturn : -18.29
TimeSinceStart : 5036.094302892685
Done logging...



Beginning logging procedure...
Timestep 1010001
mean reward (100 episodes) -20.010000
best mean reward -18.290000
running time 5096.488348
Train_EnvstepsSoFar : 990001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 4978.8223922252655
Done logging...



Beginning logging procedure...
Timestep 1000001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 5038.432760
Train_EnvstepsSoFar : 1020001
Train_AverageReturn : -13.2
Train_BestReturn : -13.2
TimeSinceStart : 5150.154863834381
Done logging...



Beginning logging procedure...
Timestep 1030001
mean reward (100 episodes) -13.040000
best mean reward -13.040000
running time 5209.709891
Train_EnvstepsSoFar : 1010001
Train_AverageReturn : -20.01
Train_BestReturn : -18.29
TimeSinceStart : 5096.488348484039
Done logging...



Beginning logging procedure...
Timestep 1020001
mean reward (100 episodes) -20.020000
best mean reward -18.290000
running time 5156.582542
Train_EnvstepsSoFar : 1000001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 5038.432760000229
Done logging...



Beginning logging procedure...
Timestep 1010001
mean reward (100 episodes) -20.580000
best mean reward -20.030000
running time 5098.362584
Train_EnvstepsSoFar : 1030001
Train_AverageReturn : -13.04
Train_BestReturn : -13.04
TimeSinceStart : 5209.709891080856
Done logging...



Beginning logging procedure...
Timestep 1040001
mean reward (100 episodes) -12.670000
best mean reward -12.670000
running time 5269.774692
Train_EnvstepsSoFar : 1010001
Train_AverageReturn : -20.58
Train_BestReturn : -20.03
TimeSinceStart : 5098.362584352493
Done logging...



Beginning logging procedure...
Timestep 1020001
mean reward (100 episodes) -20.570000
best mean reward -20.030000
running time 5157.826935
Train_EnvstepsSoFar : 1020001
Train_AverageReturn : -20.02
Train_BestReturn : -18.29
TimeSinceStart : 5156.582542419434
Done logging...



Beginning logging procedure...
Timestep 1030001
mean reward (100 episodes) -20.020000
best mean reward -18.290000
running time 5216.939687
Train_EnvstepsSoFar : 1040001
Train_AverageReturn : -12.67
Train_BestReturn : -12.67
TimeSinceStart : 5269.774691820145
Done logging...



Beginning logging procedure...
Timestep 1050001
mean reward (100 episodes) -12.480000
best mean reward -12.480000
running time 5329.086025
Train_EnvstepsSoFar : 1020001
Train_AverageReturn : -20.57
Train_BestReturn : -20.03
TimeSinceStart : 5157.826934814453
Done logging...



Beginning logging procedure...
Timestep 1030001
mean reward (100 episodes) -20.550000
best mean reward -20.030000
running time 5217.775060
Train_EnvstepsSoFar : 1030001
Train_AverageReturn : -20.02
Train_BestReturn : -18.29
TimeSinceStart : 5216.939687490463
Done logging...



Beginning logging procedure...
Timestep 1040001
mean reward (100 episodes) -19.990000
best mean reward -18.290000
running time 5276.844468
Train_EnvstepsSoFar : 1050001
Train_AverageReturn : -12.48
Train_BestReturn : -12.48
TimeSinceStart : 5329.086025238037
Done logging...



Beginning logging procedure...
Timestep 1060001
mean reward (100 episodes) -12.300000
best mean reward -12.300000
running time 5388.618059
Train_EnvstepsSoFar : 1030001
Train_AverageReturn : -20.55
Train_BestReturn : -20.03
TimeSinceStart : 5217.775059938431
Done logging...



Beginning logging procedure...
Timestep 1040001
mean reward (100 episodes) -20.510000
best mean reward -20.030000
running time 5277.183710
Train_EnvstepsSoFar : 1040001
Train_AverageReturn : -19.99
Train_BestReturn : -18.29
TimeSinceStart : 5276.844468355179
Done logging...



Beginning logging procedure...
Timestep 1050001
mean reward (100 episodes) -20.080000
best mean reward -18.290000
running time 5338.371646
Train_EnvstepsSoFar : 1060001
Train_AverageReturn : -12.3
Train_BestReturn : -12.3
TimeSinceStart : 5388.618059158325
Done logging...



Beginning logging procedure...
Timestep 1070001
mean reward (100 episodes) -12.140000
best mean reward -12.140000
running time 5448.078469
Train_EnvstepsSoFar : 1040001
Train_AverageReturn : -20.51
Train_BestReturn : -20.03
TimeSinceStart : 5277.183710336685
Done logging...



Beginning logging procedure...
Timestep 1050001
mean reward (100 episodes) -20.460000
best mean reward -20.030000
running time 5336.499983
Train_EnvstepsSoFar : 1050001
Train_AverageReturn : -20.08
Train_BestReturn : -18.29
TimeSinceStart : 5338.371646165848
Done logging...



Beginning logging procedure...
Timestep 1060001
mean reward (100 episodes) -20.190000
best mean reward -18.290000
running time 5398.246240
Train_EnvstepsSoFar : 1070001
Train_AverageReturn : -12.14
Train_BestReturn : -12.14
TimeSinceStart : 5448.0784685611725
Done logging...



Beginning logging procedure...
Timestep 1080001
mean reward (100 episodes) -11.980000
best mean reward -11.980000
running time 5508.273360
Train_EnvstepsSoFar : 1050001
Train_AverageReturn : -20.46
Train_BestReturn : -20.03
TimeSinceStart : 5336.499982833862
Done logging...



Beginning logging procedure...
Timestep 1060001
mean reward (100 episodes) -20.510000
best mean reward -20.030000
running time 5396.884148
Train_EnvstepsSoFar : 1060001
Train_AverageReturn : -20.19
Train_BestReturn : -18.29
TimeSinceStart : 5398.246240377426
Done logging...



Beginning logging procedure...
Timestep 1070001
mean reward (100 episodes) -20.370000
best mean reward -18.290000
running time 5458.216737
Train_EnvstepsSoFar : 1080001
Train_AverageReturn : -11.98
Train_BestReturn : -11.98
TimeSinceStart : 5508.273360490799
Done logging...



Beginning logging procedure...
Timestep 1090001
mean reward (100 episodes) -11.540000
best mean reward -11.540000
running time 5568.210085
Train_EnvstepsSoFar : 1060001
Train_AverageReturn : -20.51
Train_BestReturn : -20.03
TimeSinceStart : 5396.884147882462
Done logging...



Beginning logging procedure...
Timestep 1070001
mean reward (100 episodes) -20.570000
best mean reward -20.030000
running time 5457.049792
Train_EnvstepsSoFar : 1070001
Train_AverageReturn : -20.37
Train_BestReturn : -18.29
TimeSinceStart : 5458.2167365550995
Done logging...



Beginning logging procedure...
Timestep 1080001
mean reward (100 episodes) -20.440000
best mean reward -18.290000
running time 5517.983682
Train_EnvstepsSoFar : 1090001
Train_AverageReturn : -11.54
Train_BestReturn : -11.54
TimeSinceStart : 5568.210085391998
Done logging...



Beginning logging procedure...
Timestep 1100001
mean reward (100 episodes) -11.390000
best mean reward -11.390000
running time 5628.794039
Train_EnvstepsSoFar : 1070001
Train_AverageReturn : -20.57
Train_BestReturn : -20.03
TimeSinceStart : 5457.049792051315
Done logging...



Beginning logging procedure...
Timestep 1080001
mean reward (100 episodes) -20.540000
best mean reward -20.030000
running time 5517.280482
Train_EnvstepsSoFar : 1080001
Train_AverageReturn : -20.44
Train_BestReturn : -18.29
TimeSinceStart : 5517.983682394028
Done logging...



Beginning logging procedure...
Timestep 1090001
mean reward (100 episodes) -20.480000
best mean reward -18.290000
running time 5577.454388
Train_EnvstepsSoFar : 1100001
Train_AverageReturn : -11.39
Train_BestReturn : -11.39
TimeSinceStart : 5628.794038772583
Done logging...



Beginning logging procedure...
Timestep 1110001
mean reward (100 episodes) -11.130000
best mean reward -11.130000
running time 5688.989419
Train_EnvstepsSoFar : 1080001
Train_AverageReturn : -20.54
Train_BestReturn : -20.03
TimeSinceStart : 5517.280482053757
Done logging...



Beginning logging procedure...
Timestep 1090001
mean reward (100 episodes) -20.560000
best mean reward -20.030000
running time 5577.467611
Train_EnvstepsSoFar : 1090001
Train_AverageReturn : -20.48
Train_BestReturn : -18.29
TimeSinceStart : 5577.454388141632
Done logging...



Beginning logging procedure...
Timestep 1100001
mean reward (100 episodes) -20.580000
best mean reward -18.290000
running time 5637.486826
Train_EnvstepsSoFar : 1110001
Train_AverageReturn : -11.13
Train_BestReturn : -11.13
TimeSinceStart : 5688.9894189834595
Done logging...



Beginning logging procedure...
Timestep 1120001
mean reward (100 episodes) -10.840000
best mean reward -10.840000
running time 5748.482657
Train_EnvstepsSoFar : 1090001
Train_AverageReturn : -20.56
Train_BestReturn : -20.03
TimeSinceStart : 5577.4676105976105
Done logging...



Beginning logging procedure...
Timestep 1100001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 5637.363936
Train_EnvstepsSoFar : 1100001
Train_AverageReturn : -20.58
Train_BestReturn : -18.29
TimeSinceStart : 5637.48682641983
Done logging...



Beginning logging procedure...
Timestep 1110001
mean reward (100 episodes) -20.670000
best mean reward -18.290000
running time 5697.409879
Train_EnvstepsSoFar : 1120001
Train_AverageReturn : -10.84
Train_BestReturn : -10.84
TimeSinceStart : 5748.482657432556
Done logging...



Beginning logging procedure...
Timestep 1130001
mean reward (100 episodes) -10.690000
best mean reward -10.690000
running time 5808.625190
Train_EnvstepsSoFar : 1100001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 5637.363935947418
Done logging...



Beginning logging procedure...
Timestep 1110001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 5697.020364
Train_EnvstepsSoFar : 1110001
Train_AverageReturn : -20.67
Train_BestReturn : -18.29
TimeSinceStart : 5697.4098789691925
Done logging...



Beginning logging procedure...
Timestep 1120001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 5757.820503
Train_EnvstepsSoFar : 1130001
Train_AverageReturn : -10.69
Train_BestReturn : -10.69
TimeSinceStart : 5808.625190258026
Done logging...



Beginning logging procedure...
Timestep 1140001
mean reward (100 episodes) -10.500000
best mean reward -10.500000
running time 5868.413330
Train_EnvstepsSoFar : 1110001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 5697.020364046097
Done logging...



Beginning logging procedure...
Timestep 1120001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 5756.770294
Train_EnvstepsSoFar : 1120001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 5757.820503473282
Done logging...



Beginning logging procedure...
Timestep 1130001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 5817.221273
Train_EnvstepsSoFar : 1140001
Train_AverageReturn : -10.5
Train_BestReturn : -10.5
TimeSinceStart : 5868.413329839706
Done logging...



Beginning logging procedure...
Timestep 1150001
mean reward (100 episodes) -10.210000
best mean reward -10.210000
running time 5927.940051
Train_EnvstepsSoFar : 1120001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 5756.770294427872
Done logging...



Beginning logging procedure...
Timestep 1130001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 5816.857627
Train_EnvstepsSoFar : 1130001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 5817.221273422241
Done logging...



Beginning logging procedure...
Timestep 1140001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 5877.290581
Train_EnvstepsSoFar : 1150001
Train_AverageReturn : -10.21
Train_BestReturn : -10.21
TimeSinceStart : 5927.940050601959
Done logging...



Beginning logging procedure...
Timestep 1160001
mean reward (100 episodes) -9.810000
best mean reward -9.810000
running time 5987.515188
Train_EnvstepsSoFar : 1130001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 5816.857626914978
Done logging...



Beginning logging procedure...
Timestep 1140001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 5877.325619
Train_EnvstepsSoFar : 1140001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 5877.2905814647675
Done logging...



Beginning logging procedure...
Timestep 1150001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 5937.384327
Train_EnvstepsSoFar : 1160001
Train_AverageReturn : -9.81
Train_BestReturn : -9.81
TimeSinceStart : 5987.515188217163
Done logging...



Beginning logging procedure...
Timestep 1170001
mean reward (100 episodes) -9.580000
best mean reward -9.580000
running time 6047.310171
Train_EnvstepsSoFar : 1140001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 5877.325619220734
Done logging...



Beginning logging procedure...
Timestep 1150001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 5936.890904
Train_EnvstepsSoFar : 1150001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 5937.384326934814
Done logging...



Beginning logging procedure...
Timestep 1160001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 5997.663978
Train_EnvstepsSoFar : 1170001
Train_AverageReturn : -9.58
Train_BestReturn : -9.58
TimeSinceStart : 6047.310171127319
Done logging...



Beginning logging procedure...
Timestep 1180001
mean reward (100 episodes) -9.140000
best mean reward -9.140000
running time 6107.597980
Train_EnvstepsSoFar : 1150001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 5936.890903711319
Done logging...



Beginning logging procedure...
Timestep 1160001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 5996.805016
Train_EnvstepsSoFar : 1160001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 5997.663977861404
Done logging...



Beginning logging procedure...
Timestep 1170001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 6057.681763
Train_EnvstepsSoFar : 1180001
Train_AverageReturn : -9.14
Train_BestReturn : -9.14
TimeSinceStart : 6107.59798002243
Done logging...



Beginning logging procedure...
Timestep 1190001
mean reward (100 episodes) -9.070000
best mean reward -9.070000
running time 6167.549018
Train_EnvstepsSoFar : 1160001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 5996.805015563965
Done logging...



Beginning logging procedure...
Timestep 1170001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 6056.384223
Train_EnvstepsSoFar : 1170001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 6057.6817626953125
Done logging...



Beginning logging procedure...
Timestep 1180001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 6117.768666
Train_EnvstepsSoFar : 1190001
Train_AverageReturn : -9.07
Train_BestReturn : -9.07
TimeSinceStart : 6167.549018383026
Done logging...



Beginning logging procedure...
Timestep 1200001
mean reward (100 episodes) -9.110000
best mean reward -9.070000
running time 6227.000732
Train_EnvstepsSoFar : 1170001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 6056.3842232227325
Done logging...



Beginning logging procedure...
Timestep 1180001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 6116.280133
Train_EnvstepsSoFar : 1180001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 6117.768665552139
Done logging...



Beginning logging procedure...
Timestep 1190001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 6178.211277
Train_EnvstepsSoFar : 1200001
Train_AverageReturn : -9.11
Train_BestReturn : -9.07
TimeSinceStart : 6227.000732421875
Done logging...



Beginning logging procedure...
Timestep 1210001
mean reward (100 episodes) -9.050000
best mean reward -9.050000
running time 6287.101641
Train_EnvstepsSoFar : 1180001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 6116.280132770538
Done logging...



Beginning logging procedure...
Timestep 1190001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 6176.306099
Train_EnvstepsSoFar : 1190001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 6178.211277484894
Done logging...



Beginning logging procedure...
Timestep 1200001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 6238.069552
Train_EnvstepsSoFar : 1210001
Train_AverageReturn : -9.05
Train_BestReturn : -9.05
TimeSinceStart : 6287.1016409397125
Done logging...



Beginning logging procedure...
Timestep 1220001
mean reward (100 episodes) -8.860000
best mean reward -8.860000
running time 6346.388150
Train_EnvstepsSoFar : 1190001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 6176.306098937988
Done logging...



Beginning logging procedure...
Timestep 1200001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 6236.053619
Train_EnvstepsSoFar : 1200001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 6238.069552183151
Done logging...



Beginning logging procedure...
Timestep 1210001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 6298.037961
Train_EnvstepsSoFar : 1220001
Train_AverageReturn : -8.86
Train_BestReturn : -8.86
TimeSinceStart : 6346.3881504535675
Done logging...



Beginning logging procedure...
Timestep 1230001
mean reward (100 episodes) -8.760000
best mean reward -8.760000
running time 6406.142434
Train_EnvstepsSoFar : 1200001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 6236.053619146347
Done logging...



Beginning logging procedure...
Timestep 1210001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 6295.471673
Train_EnvstepsSoFar : 1210001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 6298.037960529327
Done logging...



Beginning logging procedure...
Timestep 1220001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 6358.526762
Train_EnvstepsSoFar : 1230001
Train_AverageReturn : -8.76
Train_BestReturn : -8.76
TimeSinceStart : 6406.14243388176
Done logging...



Beginning logging procedure...
Timestep 1240001
mean reward (100 episodes) -8.500000
best mean reward -8.500000
running time 6465.976740
Train_EnvstepsSoFar : 1210001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 6295.471673488617
Done logging...



Beginning logging procedure...
Timestep 1220001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 6355.057997
Train_EnvstepsSoFar : 1220001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 6358.52676153183
Done logging...



Beginning logging procedure...
Timestep 1230001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 6418.641527
Train_EnvstepsSoFar : 1240001
Train_AverageReturn : -8.5
Train_BestReturn : -8.5
TimeSinceStart : 6465.976739883423
Done logging...



Beginning logging procedure...
Timestep 1250001
mean reward (100 episodes) -8.230000
best mean reward -8.230000
running time 6525.744452
Train_EnvstepsSoFar : 1220001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 6355.057996511459
Done logging...



Beginning logging procedure...
Timestep 1230001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 6414.825195
Train_EnvstepsSoFar : 1230001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 6418.641526937485
Done logging...



Beginning logging procedure...
Timestep 1240001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 6479.231227
Train_EnvstepsSoFar : 1250001
Train_AverageReturn : -8.23
Train_BestReturn : -8.23
TimeSinceStart : 6525.744451761246
Done logging...



Beginning logging procedure...
Timestep 1260001
mean reward (100 episodes) -8.040000
best mean reward -8.040000
running time 6585.665469
Train_EnvstepsSoFar : 1230001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 6414.8251953125
Done logging...



Beginning logging procedure...
Timestep 1240001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 6474.157799
Train_EnvstepsSoFar : 1240001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 6479.231227397919
Done logging...



Beginning logging procedure...
Timestep 1250001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 6539.308571
Train_EnvstepsSoFar : 1260001
Train_AverageReturn : -8.04
Train_BestReturn : -8.04
TimeSinceStart : 6585.665468931198
Done logging...



Beginning logging procedure...
Timestep 1270001
mean reward (100 episodes) -7.870000
best mean reward -7.870000
running time 6645.981475
Train_EnvstepsSoFar : 1240001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 6474.157799243927
Done logging...



Beginning logging procedure...
Timestep 1250001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 6533.858324
Train_EnvstepsSoFar : 1250001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 6539.308570861816
Done logging...



Beginning logging procedure...
Timestep 1260001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 6599.838021
Train_EnvstepsSoFar : 1270001
Train_AverageReturn : -7.87
Train_BestReturn : -7.87
TimeSinceStart : 6645.981475353241
Done logging...



Beginning logging procedure...
Timestep 1280001
mean reward (100 episodes) -7.830000
best mean reward -7.830000
running time 6705.932551
Train_EnvstepsSoFar : 1250001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 6533.858324289322
Done logging...



Beginning logging procedure...
Timestep 1260001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 6594.267138
Train_EnvstepsSoFar : 1260001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 6599.838020563126
Done logging...



Beginning logging procedure...
Timestep 1270001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 6659.612592
Train_EnvstepsSoFar : 1280001
Train_AverageReturn : -7.83
Train_BestReturn : -7.83
TimeSinceStart : 6705.932550907135
Done logging...



Beginning logging procedure...
Timestep 1290001
mean reward (100 episodes) -7.580000
best mean reward -7.580000
running time 6765.873648
Train_EnvstepsSoFar : 1260001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 6594.267138004303
Done logging...



Beginning logging procedure...
Timestep 1270001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 6654.449141
Train_EnvstepsSoFar : 1290001
Train_AverageReturn : -7.58
Train_BestReturn : -7.58
TimeSinceStart : 6765.873647928238
Done logging...



Beginning logging procedure...
Timestep 1300001
mean reward (100 episodes) -7.490000
best mean reward -7.490000
running time 6825.358500
Train_EnvstepsSoFar : 1270001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 6659.612591743469
Done logging...



Beginning logging procedure...
Timestep 1280001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 6719.350972
Train_EnvstepsSoFar : 1270001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 6654.449141025543
Done logging...



Beginning logging procedure...
Timestep 1280001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 6715.017389
Train_EnvstepsSoFar : 1300001
Train_AverageReturn : -7.49
Train_BestReturn : -7.49
TimeSinceStart : 6825.358500242233
Done logging...



Beginning logging procedure...
Timestep 1310001
mean reward (100 episodes) -7.460000
best mean reward -7.460000
running time 6884.657921
Train_EnvstepsSoFar : 1280001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 6719.35097193718
Done logging...



Beginning logging procedure...
Timestep 1290001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 6779.686928
Train_EnvstepsSoFar : 1280001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 6715.017388820648
Done logging...



Beginning logging procedure...
Timestep 1290001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 6775.765771
Train_EnvstepsSoFar : 1310001
Train_AverageReturn : -7.46
Train_BestReturn : -7.46
TimeSinceStart : 6884.657921075821
Done logging...



Beginning logging procedure...
Timestep 1320001
mean reward (100 episodes) -7.380000
best mean reward -7.380000
running time 6943.819430
Train_EnvstepsSoFar : 1290001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 6779.686928272247
Done logging...



Beginning logging procedure...
Timestep 1300001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 6839.520616
Train_EnvstepsSoFar : 1290001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 6775.765770673752
Done logging...



Beginning logging procedure...
Timestep 1300001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 6835.815025
Train_EnvstepsSoFar : 1320001
Train_AverageReturn : -7.38
Train_BestReturn : -7.38
TimeSinceStart : 6943.819430112839
Done logging...



Beginning logging procedure...
Timestep 1330001
mean reward (100 episodes) -7.530000
best mean reward -7.380000
running time 7003.130655
Train_EnvstepsSoFar : 1300001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 6839.5206162929535
Done logging...



Beginning logging procedure...
Timestep 1310001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 6899.681789
Train_EnvstepsSoFar : 1300001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 6835.815024614334
Done logging...



Beginning logging procedure...
Timestep 1310001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 6896.295619
Train_EnvstepsSoFar : 1330001
Train_AverageReturn : -7.53
Train_BestReturn : -7.38
TimeSinceStart : 7003.130654811859
Done logging...



Beginning logging procedure...
Timestep 1340001
mean reward (100 episodes) -7.430000
best mean reward -7.380000
running time 7062.494485
Train_EnvstepsSoFar : 1310001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 6899.681788682938
Done logging...



Beginning logging procedure...
Timestep 1320001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 6959.781412
Train_EnvstepsSoFar : 1310001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 6896.295619487762
Done logging...



Beginning logging procedure...
Timestep 1320001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 6956.499187
Train_EnvstepsSoFar : 1340001
Train_AverageReturn : -7.43
Train_BestReturn : -7.38
TimeSinceStart : 7062.494484901428
Done logging...



Beginning logging procedure...
Timestep 1350001
mean reward (100 episodes) -7.480000
best mean reward -7.380000
running time 7121.614882
Train_EnvstepsSoFar : 1320001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 6959.781412363052
Done logging...



Beginning logging procedure...
Timestep 1330001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 7019.733611
Train_EnvstepsSoFar : 1350001
Train_AverageReturn : -7.48
Train_BestReturn : -7.38
TimeSinceStart : 7121.6148817539215
Done logging...



Beginning logging procedure...
Timestep 1360001
mean reward (100 episodes) -7.350000
best mean reward -7.350000
running time 7180.478625
Train_EnvstepsSoFar : 1320001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 6956.499187231064
Done logging...



Beginning logging procedure...
Timestep 1330001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 7016.929885
Train_EnvstepsSoFar : 1330001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 7019.733610868454
Done logging...



Beginning logging procedure...
Timestep 1340001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 7079.770027
Train_EnvstepsSoFar : 1360001
Train_AverageReturn : -7.35
Train_BestReturn : -7.35
TimeSinceStart : 7180.478625059128
Done logging...



Beginning logging procedure...
Timestep 1370001
mean reward (100 episodes) -7.310000
best mean reward -7.310000
running time 7239.493612
Train_EnvstepsSoFar : 1330001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 7016.929885387421
Done logging...



Beginning logging procedure...
Timestep 1340001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 7077.409389
Train_EnvstepsSoFar : 1340001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 7079.770026922226
Done logging...



Beginning logging procedure...
Timestep 1350001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 7139.746336
Train_EnvstepsSoFar : 1370001
Train_AverageReturn : -7.31
Train_BestReturn : -7.31
TimeSinceStart : 7239.493611574173
Done logging...



Beginning logging procedure...
Timestep 1380001
mean reward (100 episodes) -7.210000
best mean reward -7.210000
running time 7299.116741
Train_EnvstepsSoFar : 1340001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 7077.409388780594
Done logging...



Beginning logging procedure...
Timestep 1350001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 7137.734824
Train_EnvstepsSoFar : 1350001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 7139.746335744858
Done logging...



Beginning logging procedure...
Timestep 1360001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 7200.092644
Train_EnvstepsSoFar : 1380001
Train_AverageReturn : -7.21
Train_BestReturn : -7.21
TimeSinceStart : 7299.11674118042
Done logging...



Beginning logging procedure...
Timestep 1390001
mean reward (100 episodes) -7.010000
best mean reward -7.010000
running time 7358.755478
Train_EnvstepsSoFar : 1350001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 7137.734823703766
Done logging...



Beginning logging procedure...
Timestep 1360001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 7197.954193
Train_EnvstepsSoFar : 1360001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 7200.09264421463
Done logging...



Beginning logging procedure...
Timestep 1370001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 7260.473369
Train_EnvstepsSoFar : 1390001
Train_AverageReturn : -7.01
Train_BestReturn : -7.01
TimeSinceStart : 7358.755477666855
Done logging...



Beginning logging procedure...
Timestep 1400001
mean reward (100 episodes) -7.160000
best mean reward -7.010000
running time 7419.593837
Train_EnvstepsSoFar : 1360001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 7197.954192638397
Done logging...



Beginning logging procedure...
Timestep 1370001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 7258.138256
Train_EnvstepsSoFar : 1370001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 7260.473368883133
Done logging...



Beginning logging procedure...
Timestep 1380001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 7320.618682
Train_EnvstepsSoFar : 1400001
Train_AverageReturn : -7.16
Train_BestReturn : -7.01
TimeSinceStart : 7419.593836545944
Done logging...



Beginning logging procedure...
Timestep 1410001
mean reward (100 episodes) -7.220000
best mean reward -7.010000
running time 7478.767334
Train_EnvstepsSoFar : 1370001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 7258.138255596161
Done logging...



Beginning logging procedure...
Timestep 1380001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 7317.866528
Train_EnvstepsSoFar : 1380001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 7320.618682146072
Done logging...



Beginning logging procedure...
Timestep 1390001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 7381.341934
Train_EnvstepsSoFar : 1410001
Train_AverageReturn : -7.22
Train_BestReturn : -7.01
TimeSinceStart : 7478.767334222794
Done logging...



Beginning logging procedure...
Timestep 1420001
mean reward (100 episodes) -7.090000
best mean reward -7.010000
running time 7537.875195
Train_EnvstepsSoFar : 1380001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 7317.866527795792
Done logging...



Beginning logging procedure...
Timestep 1390001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 7378.021442
Train_EnvstepsSoFar : 1390001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 7381.341933965683
Done logging...



Beginning logging procedure...
Timestep 1400001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 7441.325114
Train_EnvstepsSoFar : 1420001
Train_AverageReturn : -7.09
Train_BestReturn : -7.01
TimeSinceStart : 7537.875194787979
Done logging...



Beginning logging procedure...
Timestep 1430001
mean reward (100 episodes) -6.790000
best mean reward -6.790000
running time 7598.106759
Train_EnvstepsSoFar : 1390001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 7378.021441936493
Done logging...



Beginning logging procedure...
Timestep 1400001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 7437.421375
Train_EnvstepsSoFar : 1400001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 7441.3251140117645
Done logging...



Beginning logging procedure...
Timestep 1410001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 7501.467512
Train_EnvstepsSoFar : 1430001
Train_AverageReturn : -6.79
Train_BestReturn : -6.79
TimeSinceStart : 7598.1067588329315
Done logging...



Beginning logging procedure...
Timestep 1440001
mean reward (100 episodes) -6.690000
best mean reward -6.690000
running time 7657.586927
Train_EnvstepsSoFar : 1400001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 7437.421374797821
Done logging...



Beginning logging procedure...
Timestep 1410001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 7497.236272
Train_EnvstepsSoFar : 1410001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 7501.467512369156
Done logging...



Beginning logging procedure...
Timestep 1420001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 7561.785081
Train_EnvstepsSoFar : 1440001
Train_AverageReturn : -6.69
Train_BestReturn : -6.69
TimeSinceStart : 7657.586926937103
Done logging...



Beginning logging procedure...
Timestep 1450001
mean reward (100 episodes) -6.690000
best mean reward -6.690000
running time 7716.856189
Train_EnvstepsSoFar : 1410001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 7497.236272096634
Done logging...



Beginning logging procedure...
Timestep 1420001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 7557.891343
Train_EnvstepsSoFar : 1420001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 7561.785081148148
Done logging...



Beginning logging procedure...
Timestep 1430001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 7621.506506
Train_EnvstepsSoFar : 1450001
Train_AverageReturn : -6.69
Train_BestReturn : -6.69
TimeSinceStart : 7716.856189250946
Done logging...



Beginning logging procedure...
Timestep 1460001
mean reward (100 episodes) -6.670000
best mean reward -6.670000
running time 7776.329966
Train_EnvstepsSoFar : 1420001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 7557.89134311676
Done logging...



Beginning logging procedure...
Timestep 1430001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 7619.135216
Train_EnvstepsSoFar : 1430001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 7621.506506204605
Done logging...



Beginning logging procedure...
Timestep 1440001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 7681.251710
Train_EnvstepsSoFar : 1460001
Train_AverageReturn : -6.67
Train_BestReturn : -6.67
TimeSinceStart : 7776.329966068268
Done logging...



Beginning logging procedure...
Timestep 1470001
mean reward (100 episodes) -6.580000
best mean reward -6.580000
running time 7835.827082
Train_EnvstepsSoFar : 1430001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 7619.135216474533
Done logging...



Beginning logging procedure...
Timestep 1440001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 7679.329167
Train_EnvstepsSoFar : 1440001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 7681.251709938049
Done logging...



Beginning logging procedure...
Timestep 1450001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 7740.801977
Train_EnvstepsSoFar : 1470001
Train_AverageReturn : -6.58
Train_BestReturn : -6.58
TimeSinceStart : 7835.827082157135
Done logging...



Beginning logging procedure...
Timestep 1480001
mean reward (100 episodes) -6.370000
best mean reward -6.370000
running time 7894.979328
Train_EnvstepsSoFar : 1440001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 7679.329167127609
Done logging...



Beginning logging procedure...
Timestep 1450001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 7739.248311
Train_EnvstepsSoFar : 1450001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 7740.801976680756
Done logging...



Beginning logging procedure...
Timestep 1460001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 7800.912158
Train_EnvstepsSoFar : 1480001
Train_AverageReturn : -6.37
Train_BestReturn : -6.37
TimeSinceStart : 7894.979328155518
Done logging...



Beginning logging procedure...
Timestep 1490001
mean reward (100 episodes) -6.290000
best mean reward -6.290000
running time 7953.776092
Train_EnvstepsSoFar : 1450001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 7739.248310804367
Done logging...



Beginning logging procedure...
Timestep 1460001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 7798.681533
Train_EnvstepsSoFar : 1460001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 7800.91215801239
Done logging...



Beginning logging procedure...
Timestep 1470001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 7860.868881
Train_EnvstepsSoFar : 1490001
Train_AverageReturn : -6.29
Train_BestReturn : -6.29
TimeSinceStart : 7953.776091814041
Done logging...



Beginning logging procedure...
Timestep 1500001
mean reward (100 episodes) -6.230000
best mean reward -6.230000
running time 8012.710665
Train_EnvstepsSoFar : 1460001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 7798.681532859802
Done logging...



Beginning logging procedure...
Timestep 1470001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 7858.208801
Train_EnvstepsSoFar : 1470001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 7860.868881225586
Done logging...



Beginning logging procedure...
Timestep 1480001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 7920.961632
Train_EnvstepsSoFar : 1500001
Train_AverageReturn : -6.23
Train_BestReturn : -6.23
TimeSinceStart : 8012.710665225983
Done logging...



Beginning logging procedure...
Timestep 1510001
mean reward (100 episodes) -5.980000
best mean reward -5.980000
running time 8071.373595
Train_EnvstepsSoFar : 1470001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 7858.208801031113
Done logging...



Beginning logging procedure...
Timestep 1480001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 7917.992501
Train_EnvstepsSoFar : 1480001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 7920.961631774902
Done logging...



Beginning logging procedure...
Timestep 1490001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 7981.414418
Train_EnvstepsSoFar : 1510001
Train_AverageReturn : -5.98
Train_BestReturn : -5.98
TimeSinceStart : 8071.373594760895
Done logging...



Beginning logging procedure...
Timestep 1520001
mean reward (100 episodes) -5.580000
best mean reward -5.580000
running time 8131.489401
Train_EnvstepsSoFar : 1480001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 7917.992501497269
Done logging...



Beginning logging procedure...
Timestep 1490001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 7977.654425
Train_EnvstepsSoFar : 1490001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 7981.414417982101
Done logging...



Beginning logging procedure...
Timestep 1500001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 8041.100158
Train_EnvstepsSoFar : 1520001
Train_AverageReturn : -5.58
Train_BestReturn : -5.58
TimeSinceStart : 8131.489401102066
Done logging...



Beginning logging procedure...
Timestep 1530001
mean reward (100 episodes) -5.240000
best mean reward -5.240000
running time 8191.618376
Train_EnvstepsSoFar : 1490001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 7977.654424667358
Done logging...



Beginning logging procedure...
Timestep 1500001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 8037.731948
Train_EnvstepsSoFar : 1500001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 8041.100158214569
Done logging...



Beginning logging procedure...
Timestep 1510001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 8100.865595
Train_EnvstepsSoFar : 1530001
Train_AverageReturn : -5.24
Train_BestReturn : -5.24
TimeSinceStart : 8191.618376255035
Done logging...



Beginning logging procedure...
Timestep 1540001
mean reward (100 episodes) -4.640000
best mean reward -4.640000
running time 8251.523721
Train_EnvstepsSoFar : 1500001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 8037.731948137283
Done logging...



Beginning logging procedure...
Timestep 1510001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 8098.265680
Train_EnvstepsSoFar : 1510001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 8100.865594863892
Done logging...



Beginning logging procedure...
Timestep 1520001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 8161.138249
Train_EnvstepsSoFar : 1540001
Train_AverageReturn : -4.64
Train_BestReturn : -4.64
TimeSinceStart : 8251.523720741272
Done logging...



Beginning logging procedure...
Timestep 1550001
mean reward (100 episodes) -4.310000
best mean reward -4.310000
running time 8311.282262
Train_EnvstepsSoFar : 1510001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 8098.265680074692
Done logging...



Beginning logging procedure...
Timestep 1520001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 8158.348793
Train_EnvstepsSoFar : 1520001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 8161.138248920441
Done logging...



Beginning logging procedure...
Timestep 1530001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 8219.843708
Train_EnvstepsSoFar : 1550001
Train_AverageReturn : -4.31
Train_BestReturn : -4.31
TimeSinceStart : 8311.282261610031
Done logging...



Beginning logging procedure...
Timestep 1560001
mean reward (100 episodes) -4.100000
best mean reward -4.100000
running time 8370.648607
Train_EnvstepsSoFar : 1520001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 8158.348792791367
Done logging...



Beginning logging procedure...
Timestep 1530001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 8218.134771
Train_EnvstepsSoFar : 1530001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 8219.843707799911
Done logging...



Beginning logging procedure...
Timestep 1540001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 8280.514881
Train_EnvstepsSoFar : 1560001
Train_AverageReturn : -4.1
Train_BestReturn : -4.1
TimeSinceStart : 8370.64860701561
Done logging...



Beginning logging procedure...
Timestep 1570001
mean reward (100 episodes) -3.910000
best mean reward -3.910000
running time 8430.191461
Train_EnvstepsSoFar : 1530001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 8218.134770870209
Done logging...



Beginning logging procedure...
Timestep 1540001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 8277.299062
Train_EnvstepsSoFar : 1540001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 8280.514881372452
Done logging...



Beginning logging procedure...
Timestep 1550001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 8340.910596
Train_EnvstepsSoFar : 1570001
Train_AverageReturn : -3.91
Train_BestReturn : -3.91
TimeSinceStart : 8430.191460847855
Done logging...



Beginning logging procedure...
Timestep 1580001
mean reward (100 episodes) -3.720000
best mean reward -3.720000
running time 8489.206036
Train_EnvstepsSoFar : 1540001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 8277.299061775208
Done logging...



Beginning logging procedure...
Timestep 1550001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 8336.994608
Train_EnvstepsSoFar : 1550001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 8340.910595655441
Done logging...



Beginning logging procedure...
Timestep 1560001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 8401.044186
Train_EnvstepsSoFar : 1580001
Train_AverageReturn : -3.72
Train_BestReturn : -3.72
TimeSinceStart : 8489.206035852432
Done logging...



Beginning logging procedure...
Timestep 1590001
mean reward (100 episodes) -3.850000
best mean reward -3.720000
running time 8547.908071
Train_EnvstepsSoFar : 1550001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 8336.994608402252
Done logging...



Beginning logging procedure...
Timestep 1560001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 8396.529226
Train_EnvstepsSoFar : 1560001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 8401.044186115265
Done logging...



Beginning logging procedure...
Timestep 1570001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 8461.455146
Train_EnvstepsSoFar : 1590001
Train_AverageReturn : -3.85
Train_BestReturn : -3.72
TimeSinceStart : 8547.908070802689
Done logging...



Beginning logging procedure...
Timestep 1600001
mean reward (100 episodes) -3.360000
best mean reward -3.360000
running time 8606.673385
Train_EnvstepsSoFar : 1560001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 8396.529225587845
Done logging...



Beginning logging procedure...
Timestep 1570001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 8457.299147
Train_EnvstepsSoFar : 1570001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 8461.455146312714
Done logging...



Beginning logging procedure...
Timestep 1580001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 8521.422291
Train_EnvstepsSoFar : 1600001
Train_AverageReturn : -3.36
Train_BestReturn : -3.36
TimeSinceStart : 8606.673384904861
Done logging...



Beginning logging procedure...
Timestep 1610001
mean reward (100 episodes) -3.200000
best mean reward -3.200000
running time 8665.788436
Train_EnvstepsSoFar : 1570001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 8457.299147367477
Done logging...



Beginning logging procedure...
Timestep 1580001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 8518.171017
Train_EnvstepsSoFar : 1580001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 8521.422290563583
Done logging...



Beginning logging procedure...
Timestep 1590001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 8581.087156
Train_EnvstepsSoFar : 1610001
Train_AverageReturn : -3.2
Train_BestReturn : -3.2
TimeSinceStart : 8665.788436412811
Done logging...



Beginning logging procedure...
Timestep 1620001
mean reward (100 episodes) -2.890000
best mean reward -2.890000
running time 8724.895032
Train_EnvstepsSoFar : 1580001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 8518.171016693115
Done logging...



Beginning logging procedure...
Timestep 1590001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 8578.002515
Train_EnvstepsSoFar : 1590001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 8581.087156057358
Done logging...



Beginning logging procedure...
Timestep 1600001
mean reward (100 episodes) -20.710000
best mean reward -18.290000
running time 8641.141693
Train_EnvstepsSoFar : 1620001
Train_AverageReturn : -2.89
Train_BestReturn : -2.89
TimeSinceStart : 8724.895032167435
Done logging...



Beginning logging procedure...
Timestep 1630001
mean reward (100 episodes) -2.410000
best mean reward -2.410000
running time 8783.966926
Train_EnvstepsSoFar : 1590001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 8578.002514839172
Done logging...



Beginning logging procedure...
Timestep 1600001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 8637.549592
Train_EnvstepsSoFar : 1600001
Train_AverageReturn : -20.71
Train_BestReturn : -18.29
TimeSinceStart : 8641.141692638397
Done logging...



Beginning logging procedure...
Timestep 1610001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 8701.847836
Train_EnvstepsSoFar : 1630001
Train_AverageReturn : -2.41
Train_BestReturn : -2.41
TimeSinceStart : 8783.966925621033
Done logging...



Beginning logging procedure...
Timestep 1640001
mean reward (100 episodes) -2.420000
best mean reward -2.410000
running time 8843.168703
Train_EnvstepsSoFar : 1600001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 8637.54959154129
Done logging...



Beginning logging procedure...
Timestep 1610001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 8697.274034
Train_EnvstepsSoFar : 1610001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 8701.847836017609
Done logging...



Beginning logging procedure...
Timestep 1620001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 8762.174501
Train_EnvstepsSoFar : 1640001
Train_AverageReturn : -2.42
Train_BestReturn : -2.41
TimeSinceStart : 8843.168702602386
Done logging...



Beginning logging procedure...
Timestep 1650001
mean reward (100 episodes) -1.980000
best mean reward -1.980000
running time 8902.946825
Train_EnvstepsSoFar : 1610001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 8697.274033784866
Done logging...



Beginning logging procedure...
Timestep 1620001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 8757.544889
Train_EnvstepsSoFar : 1620001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 8762.174501419067
Done logging...



Beginning logging procedure...
Timestep 1630001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 8822.093737
Train_EnvstepsSoFar : 1650001
Train_AverageReturn : -1.98
Train_BestReturn : -1.98
TimeSinceStart : 8902.946824550629
Done logging...



Beginning logging procedure...
Timestep 1660001
mean reward (100 episodes) -1.570000
best mean reward -1.570000
running time 8962.443110
Train_EnvstepsSoFar : 1620001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 8757.544889450073
Done logging...



Beginning logging procedure...
Timestep 1630001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 8817.894602
Train_EnvstepsSoFar : 1630001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 8822.093736886978
Done logging...



Beginning logging procedure...
Timestep 1640001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 8882.150377
Train_EnvstepsSoFar : 1660001
Train_AverageReturn : -1.57
Train_BestReturn : -1.57
TimeSinceStart : 8962.44310951233
Done logging...



Beginning logging procedure...
Timestep 1670001
mean reward (100 episodes) -0.910000
best mean reward -0.910000
running time 9021.434002
Train_EnvstepsSoFar : 1630001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 8817.894602060318
Done logging...



Beginning logging procedure...
Timestep 1640001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 8878.155157
Train_EnvstepsSoFar : 1640001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 8882.15037727356
Done logging...



Beginning logging procedure...
Timestep 1650001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 8942.170277
Train_EnvstepsSoFar : 1670001
Train_AverageReturn : -0.91
Train_BestReturn : -0.91
TimeSinceStart : 9021.434002161026
Done logging...



Beginning logging procedure...
Timestep 1680001
mean reward (100 episodes) -0.790000
best mean reward -0.790000
running time 9081.105185
Train_EnvstepsSoFar : 1640001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 8878.155157327652
Done logging...



Beginning logging procedure...
Timestep 1650001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 8938.188632
Train_EnvstepsSoFar : 1650001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 8942.170277118683
Done logging...



Beginning logging procedure...
Timestep 1660001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 9002.695886
Train_EnvstepsSoFar : 1680001
Train_AverageReturn : -0.79
Train_BestReturn : -0.79
TimeSinceStart : 9081.105184793472
Done logging...



Beginning logging procedure...
Timestep 1690001
mean reward (100 episodes) -0.620000
best mean reward -0.620000
running time 9140.469136
Train_EnvstepsSoFar : 1650001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 8938.188632249832
Done logging...



Beginning logging procedure...
Timestep 1660001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 8998.240130
Train_EnvstepsSoFar : 1660001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 9002.695885658264
Done logging...



Beginning logging procedure...
Timestep 1670001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 9062.916297
Train_EnvstepsSoFar : 1690001
Train_AverageReturn : -0.62
Train_BestReturn : -0.62
TimeSinceStart : 9140.469135761261
Done logging...



Beginning logging procedure...
Timestep 1700001
mean reward (100 episodes) -0.550000
best mean reward -0.550000
running time 9199.350331
Train_EnvstepsSoFar : 1660001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 8998.240129709244
Done logging...



Beginning logging procedure...
Timestep 1670001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 9058.938079
Train_EnvstepsSoFar : 1670001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 9062.916296958923
Done logging...



Beginning logging procedure...
Timestep 1680001
mean reward (100 episodes) -20.880000
best mean reward -18.290000
running time 9123.224579
Train_EnvstepsSoFar : 1700001
Train_AverageReturn : -0.55
Train_BestReturn : -0.55
TimeSinceStart : 9199.350331068039
Done logging...



Beginning logging procedure...
Timestep 1710001
mean reward (100 episodes) -0.250000
best mean reward -0.250000
running time 9258.721403
Train_EnvstepsSoFar : 1670001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 9058.93807888031
Done logging...



Beginning logging procedure...
Timestep 1680001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 9119.184496
Train_EnvstepsSoFar : 1680001
Train_AverageReturn : -20.88
Train_BestReturn : -18.29
TimeSinceStart : 9123.224578857422
Done logging...



Beginning logging procedure...
Timestep 1690001
mean reward (100 episodes) -20.880000
best mean reward -18.290000
running time 9183.288180
Train_EnvstepsSoFar : 1710001
Train_AverageReturn : -0.25
Train_BestReturn : -0.25
TimeSinceStart : 9258.72140288353
Done logging...



Beginning logging procedure...
Timestep 1720001
mean reward (100 episodes) -0.190000
best mean reward -0.190000
running time 9318.191710
Train_EnvstepsSoFar : 1680001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 9119.184495925903
Done logging...



Beginning logging procedure...
Timestep 1690001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 9178.560327
Train_EnvstepsSoFar : 1690001
Train_AverageReturn : -20.88
Train_BestReturn : -18.29
TimeSinceStart : 9183.288179636002
Done logging...



Beginning logging procedure...
Timestep 1700001
mean reward (100 episodes) -20.880000
best mean reward -18.290000
running time 9244.389481
Train_EnvstepsSoFar : 1720001
Train_AverageReturn : -0.19
Train_BestReturn : -0.19
TimeSinceStart : 9318.191709756851
Done logging...



Beginning logging procedure...
Timestep 1730001
mean reward (100 episodes) 0.120000
best mean reward 0.120000
running time 9377.637298
Train_EnvstepsSoFar : 1690001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 9178.560326576233
Done logging...



Beginning logging procedure...
Timestep 1700001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 9238.544440
Train_EnvstepsSoFar : 1700001
Train_AverageReturn : -20.88
Train_BestReturn : -18.29
TimeSinceStart : 9244.389480829239
Done logging...



Beginning logging procedure...
Timestep 1710001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 9304.538812
Train_EnvstepsSoFar : 1730001
Train_AverageReturn : 0.12
Train_BestReturn : 0.12
TimeSinceStart : 9377.637297868729
Done logging...



Beginning logging procedure...
Timestep 1740001
mean reward (100 episodes) 0.550000
best mean reward 0.550000
running time 9436.730196
Train_EnvstepsSoFar : 1700001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 9238.544439554214
Done logging...



Beginning logging procedure...
Timestep 1710001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 9298.502291
Train_EnvstepsSoFar : 1710001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 9304.538811683655
Done logging...



Beginning logging procedure...
Timestep 1720001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 9364.822305
Train_EnvstepsSoFar : 1740001
Train_AverageReturn : 0.55
Train_BestReturn : 0.55
TimeSinceStart : 9436.730195522308
Done logging...



Beginning logging procedure...
Timestep 1750001
mean reward (100 episodes) 0.940000
best mean reward 0.940000
running time 9495.729906
Train_EnvstepsSoFar : 1710001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 9298.502290964127
Done logging...



Beginning logging procedure...
Timestep 1720001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 9358.340098
Train_EnvstepsSoFar : 1720001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 9364.822304964066
Done logging...



Beginning logging procedure...
Timestep 1730001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 9424.833833
Train_EnvstepsSoFar : 1750001
Train_AverageReturn : 0.94
Train_BestReturn : 0.94
TimeSinceStart : 9495.729906082153
Done logging...



Beginning logging procedure...
Timestep 1760001
mean reward (100 episodes) 1.430000
best mean reward 1.430000
running time 9558.113634
Train_EnvstepsSoFar : 1720001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 9358.340097904205
Done logging...



Beginning logging procedure...
Timestep 1730001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 9418.552996
Train_EnvstepsSoFar : 1730001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 9424.83383345604
Done logging...



Beginning logging procedure...
Timestep 1740001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 9484.932311
Train_EnvstepsSoFar : 1760001
Train_AverageReturn : 1.43
Train_BestReturn : 1.43
TimeSinceStart : 9558.113634347916
Done logging...



Beginning logging procedure...
Timestep 1770001
mean reward (100 episodes) 1.900000
best mean reward 1.900000
running time 9617.242429
Train_EnvstepsSoFar : 1730001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 9418.5529961586
Done logging...



Beginning logging procedure...
Timestep 1740001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 9478.586016
Train_EnvstepsSoFar : 1740001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 9484.932310819626
Done logging...



Beginning logging procedure...
Timestep 1750001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 9545.343537
Train_EnvstepsSoFar : 1770001
Train_AverageReturn : 1.9
Train_BestReturn : 1.9
TimeSinceStart : 9617.242428541183
Done logging...



Beginning logging procedure...
Timestep 1780001
mean reward (100 episodes) 2.000000
best mean reward 2.000000
running time 9676.965104
Train_EnvstepsSoFar : 1740001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 9478.586015701294
Done logging...



Beginning logging procedure...
Timestep 1750001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 9538.721225
Train_EnvstepsSoFar : 1750001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 9545.343537330627
Done logging...



Beginning logging procedure...
Timestep 1760001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 9605.807615
Train_EnvstepsSoFar : 1780001
Train_AverageReturn : 2.0
Train_BestReturn : 2.0
TimeSinceStart : 9676.965103626251
Done logging...



Beginning logging procedure...
Timestep 1790001
mean reward (100 episodes) 2.350000
best mean reward 2.350000
running time 9736.110002
Train_EnvstepsSoFar : 1750001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 9538.72122502327
Done logging...



Beginning logging procedure...
Timestep 1760001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 9599.077121
Train_EnvstepsSoFar : 1760001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 9605.807615041733
Done logging...



Beginning logging procedure...
Timestep 1770001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 9666.445702
Train_EnvstepsSoFar : 1790001
Train_AverageReturn : 2.35
Train_BestReturn : 2.35
TimeSinceStart : 9736.110001802444
Done logging...



Beginning logging procedure...
Timestep 1800001
mean reward (100 episodes) 2.360000
best mean reward 2.360000
running time 9795.729836
Train_EnvstepsSoFar : 1760001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 9599.077121019363
Done logging...



Beginning logging procedure...
Timestep 1770001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 9659.965492
Train_EnvstepsSoFar : 1770001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 9666.445701599121
Done logging...



Beginning logging procedure...
Timestep 1780001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 9725.732921
Train_EnvstepsSoFar : 1800001
Train_AverageReturn : 2.36
Train_BestReturn : 2.36
TimeSinceStart : 9795.729836463928
Done logging...



Beginning logging procedure...
Timestep 1810001
mean reward (100 episodes) 2.850000
best mean reward 2.850000
running time 9855.076217
Train_EnvstepsSoFar : 1770001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 9659.965492248535
Done logging...



Beginning logging procedure...
Timestep 1780001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 9720.160383
Train_EnvstepsSoFar : 1780001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 9725.732920646667
Done logging...



Beginning logging procedure...
Timestep 1790001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 9785.325952
Train_EnvstepsSoFar : 1810001
Train_AverageReturn : 2.85
Train_BestReturn : 2.85
TimeSinceStart : 9855.07621717453
Done logging...



Beginning logging procedure...
Timestep 1820001
mean reward (100 episodes) 3.300000
best mean reward 3.300000
running time 9914.177567
Train_EnvstepsSoFar : 1780001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 9720.160383462906
Done logging...



Beginning logging procedure...
Timestep 1790001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 9779.944235
Train_EnvstepsSoFar : 1790001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 9785.325952291489
Done logging...



Beginning logging procedure...
Timestep 1800001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 9845.813776
Train_EnvstepsSoFar : 1820001
Train_AverageReturn : 3.3
Train_BestReturn : 3.3
TimeSinceStart : 9914.177567243576
Done logging...



Beginning logging procedure...
Timestep 1830001
mean reward (100 episodes) 3.470000
best mean reward 3.470000
running time 9972.791777
Train_EnvstepsSoFar : 1790001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 9779.944234609604
Done logging...



Beginning logging procedure...
Timestep 1800001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 9840.360970
Train_EnvstepsSoFar : 1800001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 9845.813776016235
Done logging...



Beginning logging procedure...
Timestep 1810001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 9906.322460
Train_EnvstepsSoFar : 1830001
Train_AverageReturn : 3.47
Train_BestReturn : 3.47
TimeSinceStart : 9972.791776895523
Done logging...



Beginning logging procedure...
Timestep 1840001
mean reward (100 episodes) 3.470000
best mean reward 3.470000
running time 10031.619434
Train_EnvstepsSoFar : 1800001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 9840.360969781876
Done logging...



Beginning logging procedure...
Timestep 1810001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 9900.423025
Train_EnvstepsSoFar : 1810001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 9906.32246017456
Done logging...



Beginning logging procedure...
Timestep 1820001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 9966.758197
Train_EnvstepsSoFar : 1840001
Train_AverageReturn : 3.47
Train_BestReturn : 3.47
TimeSinceStart : 10031.61943435669
Done logging...



Beginning logging procedure...
Timestep 1850001
mean reward (100 episodes) 3.720000
best mean reward 3.720000
running time 10091.103363
Train_EnvstepsSoFar : 1810001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 9900.423024654388
Done logging...



Beginning logging procedure...
Timestep 1820001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 9960.202047
Train_EnvstepsSoFar : 1820001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 9966.758196592331
Done logging...



Beginning logging procedure...
Timestep 1830001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 10027.572786
Train_EnvstepsSoFar : 1850001
Train_AverageReturn : 3.72
Train_BestReturn : 3.72
TimeSinceStart : 10091.103363275528
Done logging...



Beginning logging procedure...
Timestep 1860001
mean reward (100 episodes) 4.000000
best mean reward 4.000000
running time 10150.503682
Train_EnvstepsSoFar : 1820001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 9960.202046871185
Done logging...



Beginning logging procedure...
Timestep 1830001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 10020.647189
Train_EnvstepsSoFar : 1830001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 10027.572785615921
Done logging...



Beginning logging procedure...
Timestep 1840001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 10087.154391
Train_EnvstepsSoFar : 1860001
Train_AverageReturn : 4.0
Train_BestReturn : 4.0
TimeSinceStart : 10150.503682374954
Done logging...



Beginning logging procedure...
Timestep 1870001
mean reward (100 episodes) 4.580000
best mean reward 4.580000
running time 10209.587389
Train_EnvstepsSoFar : 1830001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 10020.64718914032
Done logging...



Beginning logging procedure...
Timestep 1840001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 10080.509543
Train_EnvstepsSoFar : 1840001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 10087.154391050339
Done logging...



Beginning logging procedure...
Timestep 1850001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 10147.508327
Train_EnvstepsSoFar : 1870001
Train_AverageReturn : 4.58
Train_BestReturn : 4.58
TimeSinceStart : 10209.587389469147
Done logging...



Beginning logging procedure...
Timestep 1880001
mean reward (100 episodes) 4.720000
best mean reward 4.720000
running time 10268.853885
Train_EnvstepsSoFar : 1840001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 10080.509543180466
Done logging...



Beginning logging procedure...
Timestep 1850001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 10140.892487
Train_EnvstepsSoFar : 1850001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 10147.508327007294
Done logging...



Beginning logging procedure...
Timestep 1860001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 10207.974506
Train_EnvstepsSoFar : 1880001
Train_AverageReturn : 4.72
Train_BestReturn : 4.72
TimeSinceStart : 10268.853884935379
Done logging...



Beginning logging procedure...
Timestep 1890001
mean reward (100 episodes) 4.950000
best mean reward 4.950000
running time 10327.797024
Train_EnvstepsSoFar : 1850001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 10140.892486810684
Done logging...



Beginning logging procedure...
Timestep 1860001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 10201.300068
Train_EnvstepsSoFar : 1860001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 10207.974505901337
Done logging...



Beginning logging procedure...
Timestep 1870001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 10269.075427
Train_EnvstepsSoFar : 1890001
Train_AverageReturn : 4.95
Train_BestReturn : 4.95
TimeSinceStart : 10327.79702448845
Done logging...



Beginning logging procedure...
Timestep 1900001
mean reward (100 episodes) 5.160000
best mean reward 5.160000
running time 10387.006618
Train_EnvstepsSoFar : 1860001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 10201.300067663193
Done logging...



Beginning logging procedure...
Timestep 1870001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 10261.612211
Train_EnvstepsSoFar : 1870001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 10269.075427293777
Done logging...



Beginning logging procedure...
Timestep 1880001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 10329.691292
Train_EnvstepsSoFar : 1900001
Train_AverageReturn : 5.16
Train_BestReturn : 5.16
TimeSinceStart : 10387.006618261337
Done logging...



Beginning logging procedure...
Timestep 1910001
mean reward (100 episodes) 5.160000
best mean reward 5.160000
running time 10446.052490
Train_EnvstepsSoFar : 1870001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 10261.612210988998
Done logging...



Beginning logging procedure...
Timestep 1880001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 10321.706825
Train_EnvstepsSoFar : 1880001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 10329.691291809082
Done logging...



Beginning logging procedure...
Timestep 1890001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 10390.271233
Train_EnvstepsSoFar : 1910001
Train_AverageReturn : 5.16
Train_BestReturn : 5.16
TimeSinceStart : 10446.05248951912
Done logging...



Beginning logging procedure...
Timestep 1920001
mean reward (100 episodes) 5.510000
best mean reward 5.510000
running time 10504.630736
Train_EnvstepsSoFar : 1880001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 10321.706824541092
Done logging...



Beginning logging procedure...
Timestep 1890001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 10381.567008
Train_EnvstepsSoFar : 1890001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 10390.271233081818
Done logging...



Beginning logging procedure...
Timestep 1900001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 10450.721375
Train_EnvstepsSoFar : 1920001
Train_AverageReturn : 5.51
Train_BestReturn : 5.51
TimeSinceStart : 10504.630736351013
Done logging...



Beginning logging procedure...
Timestep 1930001
mean reward (100 episodes) 5.880000
best mean reward 5.880000
running time 10563.655353
Train_EnvstepsSoFar : 1890001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 10381.567007541656
Done logging...



Beginning logging procedure...
Timestep 1900001
mean reward (100 episodes) -20.810000
best mean reward -20.030000
running time 10441.975769
Train_EnvstepsSoFar : 1900001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 10450.721374511719
Done logging...



Beginning logging procedure...
Timestep 1910001
mean reward (100 episodes) -20.720000
best mean reward -18.290000
running time 10510.846364
Train_EnvstepsSoFar : 1930001
Train_AverageReturn : 5.88
Train_BestReturn : 5.88
TimeSinceStart : 10563.655353069305
Done logging...



Beginning logging procedure...
Timestep 1940001
mean reward (100 episodes) 6.060000
best mean reward 6.060000
running time 10622.827766
Train_EnvstepsSoFar : 1900001
Train_AverageReturn : -20.81
Train_BestReturn : -20.03
TimeSinceStart : 10441.975769042969
Done logging...



Beginning logging procedure...
Timestep 1910001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 10501.703497
Train_EnvstepsSoFar : 1910001
Train_AverageReturn : -20.72
Train_BestReturn : -18.29
TimeSinceStart : 10510.846364498138
Done logging...



Beginning logging procedure...
Timestep 1920001
mean reward (100 episodes) -20.720000
best mean reward -18.290000
running time 10571.352169
Train_EnvstepsSoFar : 1940001
Train_AverageReturn : 6.06
Train_BestReturn : 6.06
TimeSinceStart : 10622.827766418457
Done logging...



Beginning logging procedure...
Timestep 1950001
mean reward (100 episodes) 6.570000
best mean reward 6.570000
running time 10682.545566
Train_EnvstepsSoFar : 1910001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 10501.703496694565
Done logging...



Beginning logging procedure...
Timestep 1920001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 10560.823072
Train_EnvstepsSoFar : 1920001
Train_AverageReturn : -20.72
Train_BestReturn : -18.29
TimeSinceStart : 10571.352168560028
Done logging...



Beginning logging procedure...
Timestep 1930001
mean reward (100 episodes) -20.720000
best mean reward -18.290000
running time 10630.842523
Train_EnvstepsSoFar : 1950001
Train_AverageReturn : 6.57
Train_BestReturn : 6.57
TimeSinceStart : 10682.545565843582
Done logging...



Beginning logging procedure...
Timestep 1960001
mean reward (100 episodes) 6.640000
best mean reward 6.640000
running time 10741.476618
Train_EnvstepsSoFar : 1920001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 10560.823071718216
Done logging...



Beginning logging procedure...
Timestep 1930001
mean reward (100 episodes) -20.810000
best mean reward -20.030000
running time 10620.493021
Train_EnvstepsSoFar : 1930001
Train_AverageReturn : -20.72
Train_BestReturn : -18.29
TimeSinceStart : 10630.84252333641
Done logging...



Beginning logging procedure...
Timestep 1940001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 10691.398250
Train_EnvstepsSoFar : 1960001
Train_AverageReturn : 6.64
Train_BestReturn : 6.64
TimeSinceStart : 10741.476617574692
Done logging...



Beginning logging procedure...
Timestep 1970001
mean reward (100 episodes) 6.850000
best mean reward 6.850000
running time 10800.399860
Train_EnvstepsSoFar : 1930001
Train_AverageReturn : -20.81
Train_BestReturn : -20.03
TimeSinceStart : 10620.493020772934
Done logging...



Beginning logging procedure...
Timestep 1940001
mean reward (100 episodes) -20.830000
best mean reward -20.030000
running time 10680.502881
Train_EnvstepsSoFar : 1940001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 10691.398249864578
Done logging...



Beginning logging procedure...
Timestep 1950001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 10751.729293
Train_EnvstepsSoFar : 1970001
Train_AverageReturn : 6.85
Train_BestReturn : 6.85
TimeSinceStart : 10800.399859666824
Done logging...



Beginning logging procedure...
Timestep 1980001
mean reward (100 episodes) 7.110000
best mean reward 7.110000
running time 10859.351939
Train_EnvstepsSoFar : 1940001
Train_AverageReturn : -20.83
Train_BestReturn : -20.03
TimeSinceStart : 10680.50288105011
Done logging...



Beginning logging procedure...
Timestep 1950001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 10741.514666
Train_EnvstepsSoFar : 1950001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 10751.729293107986
Done logging...



Beginning logging procedure...
Timestep 1960001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 10811.801725
Train_EnvstepsSoFar : 1980001
Train_AverageReturn : 7.11
Train_BestReturn : 7.11
TimeSinceStart : 10859.351938962936
Done logging...



Beginning logging procedure...
Timestep 1990001
mean reward (100 episodes) 7.720000
best mean reward 7.720000
running time 10918.186981
Train_EnvstepsSoFar : 1950001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 10741.514666080475
Done logging...



Beginning logging procedure...
Timestep 1960001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 10800.841354
Train_EnvstepsSoFar : 1990001
Train_AverageReturn : 7.72
Train_BestReturn : 7.72
TimeSinceStart : 10918.186981201172
Done logging...



Beginning logging procedure...
Timestep 2000001
mean reward (100 episodes) 8.100000
best mean reward 8.100000
running time 10977.245784
Train_EnvstepsSoFar : 1960001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 10811.801725149155
Done logging...



Beginning logging procedure...
Timestep 1970001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 10871.392337
Train_EnvstepsSoFar : 1960001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 10800.841353654861
Done logging...



Beginning logging procedure...
Timestep 1970001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 10861.463915
Train_EnvstepsSoFar : 2000001
Train_AverageReturn : 8.1
Train_BestReturn : 8.1
TimeSinceStart : 10977.245784282684
Done logging...



Beginning logging procedure...
Timestep 2010001
mean reward (100 episodes) 8.390000
best mean reward 8.390000
running time 11036.393659
Train_EnvstepsSoFar : 1970001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 10871.392337322235
Done logging...



Beginning logging procedure...
Timestep 1980001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 10931.641868
Train_EnvstepsSoFar : 1970001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 10861.463914632797
Done logging...



Beginning logging procedure...
Timestep 1980001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 10922.393134
Train_EnvstepsSoFar : 2010001
Train_AverageReturn : 8.39
Train_BestReturn : 8.39
TimeSinceStart : 11036.393659353256
Done logging...



Beginning logging procedure...
Timestep 2020001
mean reward (100 episodes) 8.410000
best mean reward 8.410000
running time 11096.012131
Train_EnvstepsSoFar : 1980001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 10931.641867876053
Done logging...



Beginning logging procedure...
Timestep 1990001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 10991.269029
Train_EnvstepsSoFar : 1980001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 10922.393133878708
Done logging...



Beginning logging procedure...
Timestep 1990001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 10982.870109
Train_EnvstepsSoFar : 2020001
Train_AverageReturn : 8.41
Train_BestReturn : 8.41
TimeSinceStart : 11096.01213145256
Done logging...



Beginning logging procedure...
Timestep 2030001
mean reward (100 episodes) 8.870000
best mean reward 8.870000
running time 11154.852439
Train_EnvstepsSoFar : 1990001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 10991.269028663635
Done logging...



Beginning logging procedure...
Timestep 2000001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 11051.587846
Train_EnvstepsSoFar : 1990001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 10982.870108604431
Done logging...



Beginning logging procedure...
Timestep 2000001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 11043.158699
Train_EnvstepsSoFar : 2030001
Train_AverageReturn : 8.87
Train_BestReturn : 8.87
TimeSinceStart : 11154.852439403534
Done logging...



Beginning logging procedure...
Timestep 2040001
mean reward (100 episodes) 9.180000
best mean reward 9.180000
running time 11213.882542
Train_EnvstepsSoFar : 2000001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 11051.587845563889
Done logging...



Beginning logging procedure...
Timestep 2010001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 11111.768764
Train_EnvstepsSoFar : 2000001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 11043.158698558807
Done logging...



Beginning logging procedure...
Timestep 2010001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 11103.263189
Train_EnvstepsSoFar : 2040001
Train_AverageReturn : 9.18
Train_BestReturn : 9.18
TimeSinceStart : 11213.882541656494
Done logging...



Beginning logging procedure...
Timestep 2050001
mean reward (100 episodes) 9.740000
best mean reward 9.740000
running time 11272.749008
Train_EnvstepsSoFar : 2010001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 11111.768764019012
Done logging...



Beginning logging procedure...
Timestep 2020001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 11171.716648
Train_EnvstepsSoFar : 2010001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 11103.26318860054
Done logging...



Beginning logging procedure...
Timestep 2020001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 11163.216607
Train_EnvstepsSoFar : 2050001
Train_AverageReturn : 9.74
Train_BestReturn : 9.74
TimeSinceStart : 11272.749008178711
Done logging...



Beginning logging procedure...
Timestep 2060001
mean reward (100 episodes) 9.980000
best mean reward 9.980000
running time 11331.664864
Train_EnvstepsSoFar : 2020001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 11171.71664762497
Done logging...



Beginning logging procedure...
Timestep 2030001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 11231.854441
Train_EnvstepsSoFar : 2020001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 11163.21660733223
Done logging...



Beginning logging procedure...
Timestep 2030001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 11223.588692
Train_EnvstepsSoFar : 2060001
Train_AverageReturn : 9.98
Train_BestReturn : 9.98
TimeSinceStart : 11331.664864301682
Done logging...



Beginning logging procedure...
Timestep 2070001
mean reward (100 episodes) 9.980000
best mean reward 9.980000
running time 11390.654910
Train_EnvstepsSoFar : 2030001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 11231.854440689087
Done logging...



Beginning logging procedure...
Timestep 2040001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 11291.842743
Train_EnvstepsSoFar : 2030001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 11223.588692188263
Done logging...



Beginning logging procedure...
Timestep 2040001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 11283.893181
Train_EnvstepsSoFar : 2070001
Train_AverageReturn : 9.98
Train_BestReturn : 9.98
TimeSinceStart : 11390.654910326004
Done logging...



Beginning logging procedure...
Timestep 2080001
mean reward (100 episodes) 10.130000
best mean reward 10.130000
running time 11450.192914
Train_EnvstepsSoFar : 2040001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 11291.84274315834
Done logging...



Beginning logging procedure...
Timestep 2050001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 11351.823085
Train_EnvstepsSoFar : 2040001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 11283.893180847168
Done logging...



Beginning logging procedure...
Timestep 2050001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 11344.349261
Train_EnvstepsSoFar : 2080001
Train_AverageReturn : 10.13
Train_BestReturn : 10.13
TimeSinceStart : 11450.192914485931
Done logging...



Beginning logging procedure...
Timestep 2090001
mean reward (100 episodes) 10.180000
best mean reward 10.180000
running time 11508.998250
Train_EnvstepsSoFar : 2050001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 11351.82308459282
Done logging...



Beginning logging procedure...
Timestep 2060001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 11412.717073
Train_EnvstepsSoFar : 2090001
Train_AverageReturn : 10.18
Train_BestReturn : 10.18
TimeSinceStart : 11508.998250484467
Done logging...



Beginning logging procedure...
Timestep 2100001
mean reward (100 episodes) 10.320000
best mean reward 10.320000
running time 11568.357683
Train_EnvstepsSoFar : 2050001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 11344.349261283875
Done logging...



Beginning logging procedure...
Timestep 2060001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 11404.587441
Train_EnvstepsSoFar : 2060001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 11412.717073202133
Done logging...



Beginning logging procedure...
Timestep 2070001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 11473.220158
Train_EnvstepsSoFar : 2100001
Train_AverageReturn : 10.32
Train_BestReturn : 10.32
TimeSinceStart : 11568.357683181763
Done logging...



Beginning logging procedure...
Timestep 2110001
mean reward (100 episodes) 10.600000
best mean reward 10.600000
running time 11627.804667
Train_EnvstepsSoFar : 2060001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 11404.587441205978
Done logging...



Beginning logging procedure...
Timestep 2070001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 11465.225805
Train_EnvstepsSoFar : 2070001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 11473.220158100128
Done logging...



Beginning logging procedure...
Timestep 2080001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 11532.603302
Train_EnvstepsSoFar : 2110001
Train_AverageReturn : 10.6
Train_BestReturn : 10.6
TimeSinceStart : 11627.804666757584
Done logging...



Beginning logging procedure...
Timestep 2120001
mean reward (100 episodes) 10.760000
best mean reward 10.760000
running time 11687.012458
Train_EnvstepsSoFar : 2070001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 11465.225805282593
Done logging...



Beginning logging procedure...
Timestep 2080001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 11526.357224
Train_EnvstepsSoFar : 2080001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 11532.603302001953
Done logging...



Beginning logging procedure...
Timestep 2090001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 11593.036490
Train_EnvstepsSoFar : 2120001
Train_AverageReturn : 10.76
Train_BestReturn : 10.76
TimeSinceStart : 11687.012458086014
Done logging...



Beginning logging procedure...
Timestep 2130001
mean reward (100 episodes) 11.080000
best mean reward 11.080000
running time 11746.021188
Train_EnvstepsSoFar : 2080001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 11526.357224225998
Done logging...



Beginning logging procedure...
Timestep 2090001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 11586.261748
Train_EnvstepsSoFar : 2090001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 11593.036490440369
Done logging...



Beginning logging procedure...
Timestep 2100001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 11653.699728
Train_EnvstepsSoFar : 2130001
Train_AverageReturn : 11.08
Train_BestReturn : 11.08
TimeSinceStart : 11746.021188497543
Done logging...



Beginning logging procedure...
Timestep 2140001
mean reward (100 episodes) 11.310000
best mean reward 11.310000
running time 11805.311376
Train_EnvstepsSoFar : 2090001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 11586.261748313904
Done logging...



Beginning logging procedure...
Timestep 2100001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 11646.952730
Train_EnvstepsSoFar : 2100001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 11653.699728250504
Done logging...



Beginning logging procedure...
Timestep 2110001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 11713.669034
Train_EnvstepsSoFar : 2140001
Train_AverageReturn : 11.31
Train_BestReturn : 11.31
TimeSinceStart : 11805.3113758564
Done logging...



Beginning logging procedure...
Timestep 2150001
mean reward (100 episodes) 11.330000
best mean reward 11.330000
running time 11864.275304
Train_EnvstepsSoFar : 2100001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 11646.952730417252
Done logging...



Beginning logging procedure...
Timestep 2110001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 11707.064575
Train_EnvstepsSoFar : 2110001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 11713.669034481049
Done logging...



Beginning logging procedure...
Timestep 2120001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 11774.344777
Train_EnvstepsSoFar : 2150001
Train_AverageReturn : 11.33
Train_BestReturn : 11.33
TimeSinceStart : 11864.275304317474
Done logging...



Beginning logging procedure...
Timestep 2160001
mean reward (100 episodes) 11.640000
best mean reward 11.640000
running time 11923.568824
Train_EnvstepsSoFar : 2110001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 11707.064574718475
Done logging...



Beginning logging procedure...
Timestep 2120001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 11767.163938
Train_EnvstepsSoFar : 2120001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 11774.344777107239
Done logging...



Beginning logging procedure...
Timestep 2130001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 11834.977600
Train_EnvstepsSoFar : 2160001
Train_AverageReturn : 11.64
Train_BestReturn : 11.64
TimeSinceStart : 11923.568823575974
Done logging...



Beginning logging procedure...
Timestep 2170001
mean reward (100 episodes) 11.770000
best mean reward 11.770000
running time 11982.802056
Train_EnvstepsSoFar : 2120001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 11767.16393828392
Done logging...



Beginning logging procedure...
Timestep 2130001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 11827.217051
Train_EnvstepsSoFar : 2130001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 11834.97759962082
Done logging...



Beginning logging procedure...
Timestep 2140001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 11895.556993
Train_EnvstepsSoFar : 2170001
Train_AverageReturn : 11.77
Train_BestReturn : 11.77
TimeSinceStart : 11982.802056312561
Done logging...



Beginning logging procedure...
Timestep 2180001
mean reward (100 episodes) 11.880000
best mean reward 11.880000
running time 12042.508193
Train_EnvstepsSoFar : 2130001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 11827.217050790787
Done logging...



Beginning logging procedure...
Timestep 2140001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 11886.476958
Train_EnvstepsSoFar : 2140001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 11895.556992769241
Done logging...



Beginning logging procedure...
Timestep 2150001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 11955.789085
Train_EnvstepsSoFar : 2180001
Train_AverageReturn : 11.88
Train_BestReturn : 11.88
TimeSinceStart : 12042.508193016052
Done logging...



Beginning logging procedure...
Timestep 2190001
mean reward (100 episodes) 12.100000
best mean reward 12.100000
running time 12101.364211
Train_EnvstepsSoFar : 2140001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 11886.476957559586
Done logging...



Beginning logging procedure...
Timestep 2150001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 11946.766738
Train_EnvstepsSoFar : 2150001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 11955.789085149765
Done logging...



Beginning logging procedure...
Timestep 2160001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 12016.257447
Train_EnvstepsSoFar : 2190001
Train_AverageReturn : 12.1
Train_BestReturn : 12.1
TimeSinceStart : 12101.364210605621
Done logging...



Beginning logging procedure...
Timestep 2200001
mean reward (100 episodes) 12.080000
best mean reward 12.100000
running time 12160.981637
Train_EnvstepsSoFar : 2150001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 11946.766737937927
Done logging...



Beginning logging procedure...
Timestep 2160001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 12007.126920
Train_EnvstepsSoFar : 2160001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 12016.2574467659
Done logging...



Beginning logging procedure...
Timestep 2170001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 12075.481453
Train_EnvstepsSoFar : 2200001
Train_AverageReturn : 12.08
Train_BestReturn : 12.1
TimeSinceStart : 12160.981637001038
Done logging...



Beginning logging procedure...
Timestep 2210001
mean reward (100 episodes) 12.010000
best mean reward 12.100000
running time 12220.217775
Train_EnvstepsSoFar : 2160001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 12007.126919984818
Done logging...



Beginning logging procedure...
Timestep 2170001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 12067.470720
Train_EnvstepsSoFar : 2170001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 12075.481452941895
Done logging...



Beginning logging procedure...
Timestep 2180001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 12135.565777
Train_EnvstepsSoFar : 2210001
Train_AverageReturn : 12.01
Train_BestReturn : 12.1
TimeSinceStart : 12220.217774868011
Done logging...



Beginning logging procedure...
Timestep 2220001
mean reward (100 episodes) 12.150000
best mean reward 12.150000
running time 12278.610027
Train_EnvstepsSoFar : 2170001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 12067.470720291138
Done logging...



Beginning logging procedure...
Timestep 2180001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 12127.702753
Train_EnvstepsSoFar : 2180001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 12135.56577706337
Done logging...



Beginning logging procedure...
Timestep 2190001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 12195.836676
Train_EnvstepsSoFar : 2220001
Train_AverageReturn : 12.15
Train_BestReturn : 12.15
TimeSinceStart : 12278.610027313232
Done logging...



Beginning logging procedure...
Timestep 2230001
mean reward (100 episodes) 12.400000
best mean reward 12.400000
running time 12337.662907
Train_EnvstepsSoFar : 2180001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 12127.702752828598
Done logging...



Beginning logging procedure...
Timestep 2190001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 12188.271353
Train_EnvstepsSoFar : 2190001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 12195.83667588234
Done logging...



Beginning logging procedure...
Timestep 2200001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 12256.133752
Train_EnvstepsSoFar : 2230001
Train_AverageReturn : 12.4
Train_BestReturn : 12.4
TimeSinceStart : 12337.662907123566
Done logging...



Beginning logging procedure...
Timestep 2240001
mean reward (100 episodes) 12.420000
best mean reward 12.420000
running time 12396.697688
Train_EnvstepsSoFar : 2190001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 12188.2713534832
Done logging...



Beginning logging procedure...
Timestep 2200001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 12248.475209
Train_EnvstepsSoFar : 2200001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 12256.133752346039
Done logging...



Beginning logging procedure...
Timestep 2210001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 12316.219339
Train_EnvstepsSoFar : 2240001
Train_AverageReturn : 12.42
Train_BestReturn : 12.42
TimeSinceStart : 12396.697687625885
Done logging...



Beginning logging procedure...
Timestep 2250001
mean reward (100 episodes) 12.640000
best mean reward 12.640000
running time 12456.806209
Train_EnvstepsSoFar : 2200001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 12248.475209236145
Done logging...



Beginning logging procedure...
Timestep 2210001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 12308.375654
Train_EnvstepsSoFar : 2210001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 12316.219339370728
Done logging...



Beginning logging procedure...
Timestep 2220001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 12376.519490
Train_EnvstepsSoFar : 2250001
Train_AverageReturn : 12.64
Train_BestReturn : 12.64
TimeSinceStart : 12456.806208610535
Done logging...



Beginning logging procedure...
Timestep 2260001
mean reward (100 episodes) 12.720000
best mean reward 12.720000
running time 12516.346472
Train_EnvstepsSoFar : 2210001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 12308.375653982162
Done logging...



Beginning logging procedure...
Timestep 2220001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 12368.464276
Train_EnvstepsSoFar : 2220001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 12376.519489526749
Done logging...



Beginning logging procedure...
Timestep 2230001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 12436.856561
Train_EnvstepsSoFar : 2260001
Train_AverageReturn : 12.72
Train_BestReturn : 12.72
TimeSinceStart : 12516.346472263336
Done logging...



Beginning logging procedure...
Timestep 2270001
mean reward (100 episodes) 12.810000
best mean reward 12.810000
running time 12575.335781
Train_EnvstepsSoFar : 2220001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 12368.464276313782
Done logging...



Beginning logging procedure...
Timestep 2230001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 12428.230361
Train_EnvstepsSoFar : 2230001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 12436.856561422348
Done logging...



Beginning logging procedure...
Timestep 2240001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 12497.248187
Train_EnvstepsSoFar : 2270001
Train_AverageReturn : 12.81
Train_BestReturn : 12.81
TimeSinceStart : 12575.335780858994
Done logging...



Beginning logging procedure...
Timestep 2280001
mean reward (100 episodes) 12.850000
best mean reward 12.850000
running time 12634.370879
Train_EnvstepsSoFar : 2230001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 12428.23036146164
Done logging...



Beginning logging procedure...
Timestep 2240001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 12488.654022
Train_EnvstepsSoFar : 2240001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 12497.248187065125
Done logging...



Beginning logging procedure...
Timestep 2250001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 12557.812614
Train_EnvstepsSoFar : 2280001
Train_AverageReturn : 12.85
Train_BestReturn : 12.85
TimeSinceStart : 12634.370879173279
Done logging...



Beginning logging procedure...
Timestep 2290001
mean reward (100 episodes) 13.290000
best mean reward 13.290000
running time 12693.570335
Train_EnvstepsSoFar : 2240001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 12488.654022216797
Done logging...



Beginning logging procedure...
Timestep 2250001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 12549.216560
Train_EnvstepsSoFar : 2250001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 12557.81261396408
Done logging...



Beginning logging procedure...
Timestep 2260001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 12618.044161
Train_EnvstepsSoFar : 2290001
Train_AverageReturn : 13.29
Train_BestReturn : 13.29
TimeSinceStart : 12693.570334672928
Done logging...



Beginning logging procedure...
Timestep 2300001
mean reward (100 episodes) 13.460000
best mean reward 13.460000
running time 12753.046841
Train_EnvstepsSoFar : 2250001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 12549.21656036377
Done logging...



Beginning logging procedure...
Timestep 2260001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 12608.603153
Train_EnvstepsSoFar : 2260001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 12618.044160604477
Done logging...



Beginning logging procedure...
Timestep 2270001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 12678.644732
Train_EnvstepsSoFar : 2300001
Train_AverageReturn : 13.46
Train_BestReturn : 13.46
TimeSinceStart : 12753.046840667725
Done logging...



Beginning logging procedure...
Timestep 2310001
mean reward (100 episodes) 13.370000
best mean reward 13.460000
running time 12811.962286
Train_EnvstepsSoFar : 2260001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 12608.603153467178
Done logging...



Beginning logging procedure...
Timestep 2270001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 12668.625917
Train_EnvstepsSoFar : 2270001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 12678.644732236862
Done logging...



Beginning logging procedure...
Timestep 2280001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 12739.608756
Train_EnvstepsSoFar : 2310001
Train_AverageReturn : 13.37
Train_BestReturn : 13.46
TimeSinceStart : 12811.962285518646
Done logging...



Beginning logging procedure...
Timestep 2320001
mean reward (100 episodes) 13.400000
best mean reward 13.460000
running time 12871.680908
Train_EnvstepsSoFar : 2270001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 12668.625916719437
Done logging...



Beginning logging procedure...
Timestep 2280001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 12727.707671
Train_EnvstepsSoFar : 2280001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 12739.60875582695
Done logging...



Beginning logging procedure...
Timestep 2290001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 12800.165577
Train_EnvstepsSoFar : 2320001
Train_AverageReturn : 13.4
Train_BestReturn : 13.46
TimeSinceStart : 12871.680907726288
Done logging...



Beginning logging procedure...
Timestep 2330001
mean reward (100 episodes) 13.800000
best mean reward 13.800000
running time 12931.425875
Train_EnvstepsSoFar : 2280001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 12727.70767068863
Done logging...



Beginning logging procedure...
Timestep 2290001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 12787.793900
Train_EnvstepsSoFar : 2290001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 12800.165577411652
Done logging...



Beginning logging procedure...
Timestep 2300001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 12860.197708
Train_EnvstepsSoFar : 2330001
Train_AverageReturn : 13.8
Train_BestReturn : 13.8
TimeSinceStart : 12931.425875425339
Done logging...



Beginning logging procedure...
Timestep 2340001
mean reward (100 episodes) 13.900000
best mean reward 13.900000
running time 12990.747373
Train_EnvstepsSoFar : 2290001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 12787.793900489807
Done logging...



Beginning logging procedure...
Timestep 2300001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 12847.750654
Train_EnvstepsSoFar : 2300001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 12860.197708368301
Done logging...



Beginning logging procedure...
Timestep 2310001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 12920.109692
Train_EnvstepsSoFar : 2340001
Train_AverageReturn : 13.9
Train_BestReturn : 13.9
TimeSinceStart : 12990.747373342514
Done logging...



Beginning logging procedure...
Timestep 2350001
mean reward (100 episodes) 14.080000
best mean reward 14.080000
running time 13050.362063
Train_EnvstepsSoFar : 2300001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 12847.750654220581
Done logging...



Beginning logging procedure...
Timestep 2310001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 12907.009196
Train_EnvstepsSoFar : 2310001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 12920.109691619873
Done logging...



Beginning logging procedure...
Timestep 2320001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 12980.677945
Train_EnvstepsSoFar : 2350001
Train_AverageReturn : 14.08
Train_BestReturn : 14.08
TimeSinceStart : 13050.362063407898
Done logging...



Beginning logging procedure...
Timestep 2360001
mean reward (100 episodes) 14.120000
best mean reward 14.120000
running time 13110.287967
Train_EnvstepsSoFar : 2310001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 12907.009196281433
Done logging...



Beginning logging procedure...
Timestep 2320001
mean reward (100 episodes) -20.840000
best mean reward -20.030000
running time 12967.769480
Train_EnvstepsSoFar : 2320001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 12980.677944898605
Done logging...



Beginning logging procedure...
Timestep 2330001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 13040.866035
Train_EnvstepsSoFar : 2360001
Train_AverageReturn : 14.12
Train_BestReturn : 14.12
TimeSinceStart : 13110.287966966629
Done logging...



Beginning logging procedure...
Timestep 2370001
mean reward (100 episodes) 14.310000
best mean reward 14.310000
running time 13169.345459
Train_EnvstepsSoFar : 2320001
Train_AverageReturn : -20.84
Train_BestReturn : -20.03
TimeSinceStart : 12967.769479751587
Done logging...



Beginning logging procedure...
Timestep 2330001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 13028.245851
Train_EnvstepsSoFar : 2330001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 13040.866035223007
Done logging...



Beginning logging procedure...
Timestep 2340001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 13101.752611
Train_EnvstepsSoFar : 2370001
Train_AverageReturn : 14.31
Train_BestReturn : 14.31
TimeSinceStart : 13169.345458984375
Done logging...



Beginning logging procedure...
Timestep 2380001
mean reward (100 episodes) 14.380000
best mean reward 14.380000
running time 13228.553845
Train_EnvstepsSoFar : 2330001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 13028.245850801468
Done logging...



Beginning logging procedure...
Timestep 2340001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 13088.423259
Train_EnvstepsSoFar : 2340001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 13101.752611398697
Done logging...



Beginning logging procedure...
Timestep 2350001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 13161.475662
Train_EnvstepsSoFar : 2380001
Train_AverageReturn : 14.38
Train_BestReturn : 14.38
TimeSinceStart : 13228.55384516716
Done logging...



Beginning logging procedure...
Timestep 2390001
mean reward (100 episodes) 14.420000
best mean reward 14.420000
running time 13287.654960
Train_EnvstepsSoFar : 2340001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 13088.423259496689
Done logging...



Beginning logging procedure...
Timestep 2350001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 13148.676379
Train_EnvstepsSoFar : 2350001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 13161.475662469864
Done logging...



Beginning logging procedure...
Timestep 2360001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 13222.395323
Train_EnvstepsSoFar : 2390001
Train_AverageReturn : 14.42
Train_BestReturn : 14.42
TimeSinceStart : 13287.654959917068
Done logging...



Beginning logging procedure...
Timestep 2400001
mean reward (100 episodes) 14.420000
best mean reward 14.420000
running time 13346.691759
Train_EnvstepsSoFar : 2350001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 13148.676379442215
Done logging...



Beginning logging procedure...
Timestep 2360001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 13209.148848
Train_EnvstepsSoFar : 2360001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 13222.395322561264
Done logging...



Beginning logging procedure...
Timestep 2370001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 13282.740598
Train_EnvstepsSoFar : 2400001
Train_AverageReturn : 14.42
Train_BestReturn : 14.42
TimeSinceStart : 13346.691759109497
Done logging...



Beginning logging procedure...
Timestep 2410001
mean reward (100 episodes) 14.350000
best mean reward 14.420000
running time 13406.198797
Train_EnvstepsSoFar : 2360001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 13209.148848056793
Done logging...



Beginning logging procedure...
Timestep 2370001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 13269.345237
Train_EnvstepsSoFar : 2370001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 13282.740597963333
Done logging...



Beginning logging procedure...
Timestep 2380001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 13343.924165
Train_EnvstepsSoFar : 2410001
Train_AverageReturn : 14.35
Train_BestReturn : 14.42
TimeSinceStart : 13406.198796510696
Done logging...



Beginning logging procedure...
Timestep 2420001
mean reward (100 episodes) 14.280000
best mean reward 14.420000
running time 13465.247401
Train_EnvstepsSoFar : 2370001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 13269.345237016678
Done logging...



Beginning logging procedure...
Timestep 2380001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 13329.784989
Train_EnvstepsSoFar : 2380001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 13343.924165010452
Done logging...



Beginning logging procedure...
Timestep 2390001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 13404.043810
Train_EnvstepsSoFar : 2420001
Train_AverageReturn : 14.28
Train_BestReturn : 14.42
TimeSinceStart : 13465.24740076065
Done logging...



Beginning logging procedure...
Timestep 2430001
mean reward (100 episodes) 14.380000
best mean reward 14.420000
running time 13524.407654
Train_EnvstepsSoFar : 2380001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 13329.784989118576
Done logging...



Beginning logging procedure...
Timestep 2390001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 13390.513608
Train_EnvstepsSoFar : 2390001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 13404.043809890747
Done logging...



Beginning logging procedure...
Timestep 2400001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 13463.968339
Train_EnvstepsSoFar : 2430001
Train_AverageReturn : 14.38
Train_BestReturn : 14.42
TimeSinceStart : 13524.407653570175
Done logging...



Beginning logging procedure...
Timestep 2440001
mean reward (100 episodes) 14.440000
best mean reward 14.440000
running time 13584.186777
Train_EnvstepsSoFar : 2390001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 13390.513608455658
Done logging...



Beginning logging procedure...
Timestep 2400001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 13450.651832
Train_EnvstepsSoFar : 2400001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 13463.968338727951
Done logging...



Beginning logging procedure...
Timestep 2410001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 13523.935267
Train_EnvstepsSoFar : 2440001
Train_AverageReturn : 14.44
Train_BestReturn : 14.44
TimeSinceStart : 13584.186776638031
Done logging...



Beginning logging procedure...
Timestep 2450001
mean reward (100 episodes) 14.630000
best mean reward 14.630000
running time 13643.428849
Train_EnvstepsSoFar : 2400001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 13450.65183210373
Done logging...



Beginning logging procedure...
Timestep 2410001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 13510.596471
Train_EnvstepsSoFar : 2410001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 13523.93526673317
Done logging...



Beginning logging procedure...
Timestep 2420001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 13584.294114
Train_EnvstepsSoFar : 2450001
Train_AverageReturn : 14.63
Train_BestReturn : 14.63
TimeSinceStart : 13643.428848743439
Done logging...



Beginning logging procedure...
Timestep 2460001
mean reward (100 episodes) 14.660000
best mean reward 14.660000
running time 13702.528133
Train_EnvstepsSoFar : 2410001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 13510.596470832825
Done logging...



Beginning logging procedure...
Timestep 2420001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 13570.143868
Train_EnvstepsSoFar : 2420001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 13584.294114351273
Done logging...



Beginning logging procedure...
Timestep 2430001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 13644.350539
Train_EnvstepsSoFar : 2460001
Train_AverageReturn : 14.66
Train_BestReturn : 14.66
TimeSinceStart : 13702.528133153915
Done logging...



Beginning logging procedure...
Timestep 2470001
mean reward (100 episodes) 14.710000
best mean reward 14.710000
running time 13761.976005
Train_EnvstepsSoFar : 2420001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 13570.14386844635
Done logging...



Beginning logging procedure...
Timestep 2430001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 13629.917801
Train_EnvstepsSoFar : 2430001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 13644.350538730621
Done logging...



Beginning logging procedure...
Timestep 2440001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 13704.767455
Train_EnvstepsSoFar : 2470001
Train_AverageReturn : 14.71
Train_BestReturn : 14.71
TimeSinceStart : 13761.976004600525
Done logging...



Beginning logging procedure...
Timestep 2480001
mean reward (100 episodes) 14.720000
best mean reward 14.720000
running time 13821.192267
Train_EnvstepsSoFar : 2430001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 13629.917801141739
Done logging...



Beginning logging procedure...
Timestep 2440001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 13689.980428
Train_EnvstepsSoFar : 2440001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 13704.767454624176
Done logging...



Beginning logging procedure...
Timestep 2450001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 13765.364103
Train_EnvstepsSoFar : 2480001
Train_AverageReturn : 14.72
Train_BestReturn : 14.72
TimeSinceStart : 13821.19226717949
Done logging...



Beginning logging procedure...
Timestep 2490001
mean reward (100 episodes) 14.820000
best mean reward 14.820000
running time 13880.376367
Train_EnvstepsSoFar : 2440001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 13689.980428218842
Done logging...



Beginning logging procedure...
Timestep 2450001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 13749.785750
Train_EnvstepsSoFar : 2450001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 13765.364102840424
Done logging...



Beginning logging procedure...
Timestep 2460001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 13826.273821
Train_EnvstepsSoFar : 2490001
Train_AverageReturn : 14.82
Train_BestReturn : 14.82
TimeSinceStart : 13880.376366853714
Done logging...



Beginning logging procedure...
Timestep 2500001
mean reward (100 episodes) 14.810000
best mean reward 14.820000
running time 13939.273164
Train_EnvstepsSoFar : 2450001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 13749.7857503891
Done logging...



Beginning logging procedure...
Timestep 2460001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 13809.747711
Train_EnvstepsSoFar : 2460001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 13826.273821115494
Done logging...



Beginning logging procedure...
Timestep 2470001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 13887.378758
Train_EnvstepsSoFar : 2500001
Train_AverageReturn : 14.81
Train_BestReturn : 14.82
TimeSinceStart : 13939.27316403389
Done logging...



Beginning logging procedure...
Timestep 2510001
mean reward (100 episodes) 14.650000
best mean reward 14.820000
running time 13998.634340
Train_EnvstepsSoFar : 2460001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 13809.74771118164
Done logging...



Beginning logging procedure...
Timestep 2470001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 13870.324462
Train_EnvstepsSoFar : 2470001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 13887.378757715225
Done logging...



Beginning logging procedure...
Timestep 2480001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 13947.675118
Train_EnvstepsSoFar : 2510001
Train_AverageReturn : 14.65
Train_BestReturn : 14.82
TimeSinceStart : 13998.634340047836
Done logging...



Beginning logging procedure...
Timestep 2520001
mean reward (100 episodes) 14.590000
best mean reward 14.820000
running time 14057.809918
Train_EnvstepsSoFar : 2470001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 13870.32446217537
Done logging...



Beginning logging procedure...
Timestep 2480001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 13929.735027
Train_EnvstepsSoFar : 2480001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 13947.675117731094
Done logging...



Beginning logging procedure...
Timestep 2490001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 14008.934872
Train_EnvstepsSoFar : 2520001
Train_AverageReturn : 14.59
Train_BestReturn : 14.82
TimeSinceStart : 14057.809918403625
Done logging...



Beginning logging procedure...
Timestep 2530001
mean reward (100 episodes) 14.600000
best mean reward 14.820000
running time 14116.786288
Train_EnvstepsSoFar : 2480001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 13929.735026836395
Done logging...



Beginning logging procedure...
Timestep 2490001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 13990.131405
Train_EnvstepsSoFar : 2490001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 14008.934871912003
Done logging...



Beginning logging procedure...
Timestep 2500001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 14069.098119
Train_EnvstepsSoFar : 2530001
Train_AverageReturn : 14.6
Train_BestReturn : 14.82
TimeSinceStart : 14116.786287546158
Done logging...



Beginning logging procedure...
Timestep 2540001
mean reward (100 episodes) 14.300000
best mean reward 14.820000
running time 14175.988755
Train_EnvstepsSoFar : 2490001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 13990.131405115128
Done logging...



Beginning logging procedure...
Timestep 2500001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 14050.808891
Train_EnvstepsSoFar : 2540001
Train_AverageReturn : 14.3
Train_BestReturn : 14.82
TimeSinceStart : 14175.988755464554
Done logging...



Beginning logging procedure...
Timestep 2550001
mean reward (100 episodes) 14.160000
best mean reward 14.820000
running time 14235.184258
Train_EnvstepsSoFar : 2500001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 14069.098119020462
Done logging...



Beginning logging procedure...
Timestep 2510001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 14129.210142
Train_EnvstepsSoFar : 2500001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 14050.808891057968
Done logging...



Beginning logging procedure...
Timestep 2510001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 14111.611523
Train_EnvstepsSoFar : 2550001
Train_AverageReturn : 14.16
Train_BestReturn : 14.82
TimeSinceStart : 14235.18425822258
Done logging...



Beginning logging procedure...
Timestep 2560001
mean reward (100 episodes) 13.940000
best mean reward 14.820000
running time 14294.738256
Train_EnvstepsSoFar : 2510001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 14129.210141897202
Done logging...



Beginning logging procedure...
Timestep 2520001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 14189.625898
Train_EnvstepsSoFar : 2510001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 14111.61152267456
Done logging...



Beginning logging procedure...
Timestep 2520001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 14172.494700
Train_EnvstepsSoFar : 2560001
Train_AverageReturn : 13.94
Train_BestReturn : 14.82
TimeSinceStart : 14294.738256454468
Done logging...



Beginning logging procedure...
Timestep 2570001
mean reward (100 episodes) 13.700000
best mean reward 14.820000
running time 14353.471037
Train_EnvstepsSoFar : 2520001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 14189.625898361206
Done logging...



Beginning logging procedure...
Timestep 2530001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 14250.227282
Train_EnvstepsSoFar : 2520001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 14172.494700431824
Done logging...



Beginning logging procedure...
Timestep 2530001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 14232.527757
Train_EnvstepsSoFar : 2570001
Train_AverageReturn : 13.7
Train_BestReturn : 14.82
TimeSinceStart : 14353.47103714943
Done logging...



Beginning logging procedure...
Timestep 2580001
mean reward (100 episodes) 13.700000
best mean reward 14.820000
running time 14413.185096
Train_EnvstepsSoFar : 2530001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 14250.227282047272
Done logging...



Beginning logging procedure...
Timestep 2540001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 14310.498660
Train_EnvstepsSoFar : 2530001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 14232.527756929398
Done logging...



Beginning logging procedure...
Timestep 2540001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 14292.565598
Train_EnvstepsSoFar : 2580001
Train_AverageReturn : 13.7
Train_BestReturn : 14.82
TimeSinceStart : 14413.185095787048
Done logging...



Beginning logging procedure...
Timestep 2590001
mean reward (100 episodes) 13.480000
best mean reward 14.820000
running time 14471.994037
Train_EnvstepsSoFar : 2540001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 14310.498660087585
Done logging...



Beginning logging procedure...
Timestep 2550001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 14370.641277
Train_EnvstepsSoFar : 2540001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 14292.565597772598
Done logging...



Beginning logging procedure...
Timestep 2550001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 14352.532310
Train_EnvstepsSoFar : 2590001
Train_AverageReturn : 13.48
Train_BestReturn : 14.82
TimeSinceStart : 14471.994037389755
Done logging...



Beginning logging procedure...
Timestep 2600001
mean reward (100 episodes) 13.310000
best mean reward 14.820000
running time 14531.378904
Train_EnvstepsSoFar : 2550001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 14370.641277074814
Done logging...



Beginning logging procedure...
Timestep 2560001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 14431.277632
Train_EnvstepsSoFar : 2550001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 14352.532309532166
Done logging...



Beginning logging procedure...
Timestep 2560001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 14412.378280
Train_EnvstepsSoFar : 2600001
Train_AverageReturn : 13.31
Train_BestReturn : 14.82
TimeSinceStart : 14531.378904342651
Done logging...



Beginning logging procedure...
Timestep 2610001
mean reward (100 episodes) 13.050000
best mean reward 14.820000
running time 14590.498108
Train_EnvstepsSoFar : 2560001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 14431.27763223648
Done logging...



Beginning logging procedure...
Timestep 2570001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 14492.160567
Train_EnvstepsSoFar : 2560001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 14412.378280162811
Done logging...



Beginning logging procedure...
Timestep 2570001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 14471.346501
Train_EnvstepsSoFar : 2610001
Train_AverageReturn : 13.05
Train_BestReturn : 14.82
TimeSinceStart : 14590.498108148575
Done logging...



Beginning logging procedure...
Timestep 2620001
mean reward (100 episodes) 12.940000
best mean reward 14.820000
running time 14649.602780
Train_EnvstepsSoFar : 2570001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 14492.16056728363
Done logging...



Beginning logging procedure...
Timestep 2580001
mean reward (100 episodes) -20.880000
best mean reward -18.290000
running time 14552.952817
Train_EnvstepsSoFar : 2570001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 14471.346500873566
Done logging...



Beginning logging procedure...
Timestep 2580001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 14531.089244
Train_EnvstepsSoFar : 2620001
Train_AverageReturn : 12.94
Train_BestReturn : 14.82
TimeSinceStart : 14649.602779865265
Done logging...



Beginning logging procedure...
Timestep 2630001
mean reward (100 episodes) 12.900000
best mean reward 14.820000
running time 14708.430999
Train_EnvstepsSoFar : 2580001
Train_AverageReturn : -20.88
Train_BestReturn : -18.29
TimeSinceStart : 14552.952816724777
Done logging...



Beginning logging procedure...
Timestep 2590001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 14612.494575
Train_EnvstepsSoFar : 2580001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 14531.089244365692
Done logging...



Beginning logging procedure...
Timestep 2590001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 14591.031612
Train_EnvstepsSoFar : 2630001
Train_AverageReturn : 12.9
Train_BestReturn : 14.82
TimeSinceStart : 14708.430998802185
Done logging...



Beginning logging procedure...
Timestep 2640001
mean reward (100 episodes) 12.930000
best mean reward 14.820000
running time 14767.563439
Train_EnvstepsSoFar : 2590001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 14612.494574546814
Done logging...



Beginning logging procedure...
Timestep 2600001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 14673.162255
Train_EnvstepsSoFar : 2590001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 14591.031611680984
Done logging...



Beginning logging procedure...
Timestep 2600001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 14650.874280
Train_EnvstepsSoFar : 2640001
Train_AverageReturn : 12.93
Train_BestReturn : 14.82
TimeSinceStart : 14767.563439369202
Done logging...



Beginning logging procedure...
Timestep 2650001
mean reward (100 episodes) 12.820000
best mean reward 14.820000
running time 14827.395945
Train_EnvstepsSoFar : 2600001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 14673.162254810333
Done logging...



Beginning logging procedure...
Timestep 2610001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 14733.465211
Train_EnvstepsSoFar : 2600001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 14650.874279975891
Done logging...



Beginning logging procedure...
Timestep 2610001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 14711.529486
Train_EnvstepsSoFar : 2650001
Train_AverageReturn : 12.82
Train_BestReturn : 14.82
TimeSinceStart : 14827.395945072174
Done logging...



Beginning logging procedure...
Timestep 2660001
mean reward (100 episodes) 12.850000
best mean reward 14.820000
running time 14887.169569
Train_EnvstepsSoFar : 2610001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 14733.465211391449
Done logging...



Beginning logging procedure...
Timestep 2620001
mean reward (100 episodes) -20.880000
best mean reward -18.290000
running time 14793.832633
Train_EnvstepsSoFar : 2610001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 14711.529485702515
Done logging...



Beginning logging procedure...
Timestep 2620001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 14771.031458
Train_EnvstepsSoFar : 2660001
Train_AverageReturn : 12.85
Train_BestReturn : 14.82
TimeSinceStart : 14887.169568538666
Done logging...



Beginning logging procedure...
Timestep 2670001
mean reward (100 episodes) 13.030000
best mean reward 14.820000
running time 14946.722574
Train_EnvstepsSoFar : 2620001
Train_AverageReturn : -20.88
Train_BestReturn : -18.29
TimeSinceStart : 14793.832633256912
Done logging...



Beginning logging procedure...
Timestep 2630001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 14853.978449
Train_EnvstepsSoFar : 2620001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 14771.03145813942
Done logging...



Beginning logging procedure...
Timestep 2630001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 14830.294126
Train_EnvstepsSoFar : 2670001
Train_AverageReturn : 13.03
Train_BestReturn : 14.82
TimeSinceStart : 14946.722573518753
Done logging...



Beginning logging procedure...
Timestep 2680001
mean reward (100 episodes) 13.220000
best mean reward 14.820000
running time 15006.648830
Train_EnvstepsSoFar : 2630001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 14853.978448867798
Done logging...



Beginning logging procedure...
Timestep 2640001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 14914.722707
Train_EnvstepsSoFar : 2630001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 14830.294126033783
Done logging...



Beginning logging procedure...
Timestep 2640001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 14890.638585
Train_EnvstepsSoFar : 2680001
Train_AverageReturn : 13.22
Train_BestReturn : 14.82
TimeSinceStart : 15006.648829936981
Done logging...



Beginning logging procedure...
Timestep 2690001
mean reward (100 episodes) 13.370000
best mean reward 14.820000
running time 15066.000751
Train_EnvstepsSoFar : 2640001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 14914.722706794739
Done logging...



Beginning logging procedure...
Timestep 2650001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 14975.435022
Train_EnvstepsSoFar : 2640001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 14890.638585329056
Done logging...



Beginning logging procedure...
Timestep 2650001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 14951.230853
Train_EnvstepsSoFar : 2690001
Train_AverageReturn : 13.37
Train_BestReturn : 14.82
TimeSinceStart : 15066.000750780106
Done logging...



Beginning logging procedure...
Timestep 2700001
mean reward (100 episodes) 13.360000
best mean reward 14.820000
running time 15124.818146
Train_EnvstepsSoFar : 2650001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 14975.435021877289
Done logging...



Beginning logging procedure...
Timestep 2660001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 15034.812767
Train_EnvstepsSoFar : 2650001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 14951.23085308075
Done logging...



Beginning logging procedure...
Timestep 2660001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 15011.836098
Train_EnvstepsSoFar : 2700001
Train_AverageReturn : 13.36
Train_BestReturn : 14.82
TimeSinceStart : 15124.818145751953
Done logging...



Beginning logging procedure...
Timestep 2710001
mean reward (100 episodes) 13.380000
best mean reward 14.820000
running time 15183.703055
Train_EnvstepsSoFar : 2660001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 15034.812766551971
Done logging...



Beginning logging procedure...
Timestep 2670001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 15095.031492
Train_EnvstepsSoFar : 2660001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 15011.836097717285
Done logging...



Beginning logging procedure...
Timestep 2670001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 15072.058308
Train_EnvstepsSoFar : 2710001
Train_AverageReturn : 13.38
Train_BestReturn : 14.82
TimeSinceStart : 15183.70305466652
Done logging...



Beginning logging procedure...
Timestep 2720001
mean reward (100 episodes) 13.460000
best mean reward 14.820000
running time 15243.127374
Train_EnvstepsSoFar : 2670001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 15095.03149151802
Done logging...



Beginning logging procedure...
Timestep 2680001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 15155.919579
Train_EnvstepsSoFar : 2670001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 15072.058308124542
Done logging...



Beginning logging procedure...
Timestep 2680001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 15132.284822
Train_EnvstepsSoFar : 2720001
Train_AverageReturn : 13.46
Train_BestReturn : 14.82
TimeSinceStart : 15243.12737441063
Done logging...



Beginning logging procedure...
Timestep 2730001
mean reward (100 episodes) 13.550000
best mean reward 14.820000
running time 15302.329697
Train_EnvstepsSoFar : 2680001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 15155.919578552246
Done logging...



Beginning logging procedure...
Timestep 2690001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 15216.605009
Train_EnvstepsSoFar : 2680001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 15132.28482246399
Done logging...



Beginning logging procedure...
Timestep 2690001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 15192.743063
Train_EnvstepsSoFar : 2730001
Train_AverageReturn : 13.55
Train_BestReturn : 14.82
TimeSinceStart : 15302.32969737053
Done logging...



Beginning logging procedure...
Timestep 2740001
mean reward (100 episodes) 13.480000
best mean reward 14.820000
running time 15361.273189
Train_EnvstepsSoFar : 2690001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 15216.60500907898
Done logging...



Beginning logging procedure...
Timestep 2700001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 15278.451803
Train_EnvstepsSoFar : 2690001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 15192.743062734604
Done logging...



Beginning logging procedure...
Timestep 2700001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 15252.847547
Train_EnvstepsSoFar : 2740001
Train_AverageReturn : 13.48
Train_BestReturn : 14.82
TimeSinceStart : 15361.273188591003
Done logging...



Beginning logging procedure...
Timestep 2750001
mean reward (100 episodes) 13.580000
best mean reward 14.820000
running time 15420.306765
Train_EnvstepsSoFar : 2700001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 15278.45180273056
Done logging...



Beginning logging procedure...
Timestep 2710001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 15338.849851
Train_EnvstepsSoFar : 2700001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 15252.847546815872
Done logging...



Beginning logging procedure...
Timestep 2710001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 15313.130489
Train_EnvstepsSoFar : 2750001
Train_AverageReturn : 13.58
Train_BestReturn : 14.82
TimeSinceStart : 15420.306765317917
Done logging...



Beginning logging procedure...
Timestep 2760001
mean reward (100 episodes) 13.620000
best mean reward 14.820000
running time 15479.928154
Train_EnvstepsSoFar : 2710001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 15338.849850654602
Done logging...



Beginning logging procedure...
Timestep 2720001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 15399.154380
Train_EnvstepsSoFar : 2710001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 15313.130488872528
Done logging...



Beginning logging procedure...
Timestep 2720001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 15373.289387
Train_EnvstepsSoFar : 2760001
Train_AverageReturn : 13.62
Train_BestReturn : 14.82
TimeSinceStart : 15479.928153514862
Done logging...



Beginning logging procedure...
Timestep 2770001
mean reward (100 episodes) 13.720000
best mean reward 14.820000
running time 15539.658589
Train_EnvstepsSoFar : 2720001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 15399.154380083084
Done logging...



Beginning logging procedure...
Timestep 2730001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 15459.370771
Train_EnvstepsSoFar : 2720001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 15373.289386510849
Done logging...



Beginning logging procedure...
Timestep 2730001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 15433.744324
Train_EnvstepsSoFar : 2770001
Train_AverageReturn : 13.72
Train_BestReturn : 14.82
TimeSinceStart : 15539.658589363098
Done logging...



Beginning logging procedure...
Timestep 2780001
mean reward (100 episodes) 13.720000
best mean reward 14.820000
running time 15599.303352
Train_EnvstepsSoFar : 2730001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 15459.370771169662
Done logging...



Beginning logging procedure...
Timestep 2740001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 15519.490314
Train_EnvstepsSoFar : 2730001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 15433.744323968887
Done logging...



Beginning logging procedure...
Timestep 2740001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 15493.076497
Train_EnvstepsSoFar : 2780001
Train_AverageReturn : 13.72
Train_BestReturn : 14.82
TimeSinceStart : 15599.303352117538
Done logging...



Beginning logging procedure...
Timestep 2790001
mean reward (100 episodes) 13.750000
best mean reward 14.820000
running time 15658.855652
Train_EnvstepsSoFar : 2740001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 15519.490314245224
Done logging...



Beginning logging procedure...
Timestep 2750001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 15579.385998
Train_EnvstepsSoFar : 2740001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 15493.07649731636
Done logging...



Beginning logging procedure...
Timestep 2750001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 15553.024970
Train_EnvstepsSoFar : 2790001
Train_AverageReturn : 13.75
Train_BestReturn : 14.82
TimeSinceStart : 15658.855651855469
Done logging...



Beginning logging procedure...
Timestep 2800001
mean reward (100 episodes) 13.880000
best mean reward 14.820000
running time 15718.429411
Train_EnvstepsSoFar : 2750001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 15579.385997533798
Done logging...



Beginning logging procedure...
Timestep 2760001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 15639.237093
Train_EnvstepsSoFar : 2750001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 15553.024969816208
Done logging...



Beginning logging procedure...
Timestep 2760001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 15612.787403
Train_EnvstepsSoFar : 2800001
Train_AverageReturn : 13.88
Train_BestReturn : 14.82
TimeSinceStart : 15718.429411172867
Done logging...



Beginning logging procedure...
Timestep 2810001
mean reward (100 episodes) 14.140000
best mean reward 14.820000
running time 15777.916681
Train_EnvstepsSoFar : 2760001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 15639.237093448639
Done logging...



Beginning logging procedure...
Timestep 2770001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 15699.917342
Train_EnvstepsSoFar : 2760001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 15612.787403345108
Done logging...



Beginning logging procedure...
Timestep 2770001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 15672.755954
Train_EnvstepsSoFar : 2810001
Train_AverageReturn : 14.14
Train_BestReturn : 14.82
TimeSinceStart : 15777.916680812836
Done logging...



Beginning logging procedure...
Timestep 2820001
mean reward (100 episodes) 14.360000
best mean reward 14.820000
running time 15837.361301
Train_EnvstepsSoFar : 2770001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 15699.917341947556
Done logging...



Beginning logging procedure...
Timestep 2780001
mean reward (100 episodes) -20.700000
best mean reward -18.290000
running time 15760.342664
Train_EnvstepsSoFar : 2820001
Train_AverageReturn : 14.36
Train_BestReturn : 14.82
TimeSinceStart : 15837.36130142212
Done logging...



Beginning logging procedure...
Timestep 2830001
mean reward (100 episodes) 14.780000
best mean reward 14.820000
running time 15896.588846
Train_EnvstepsSoFar : 2770001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 15672.755954265594
Done logging...



Beginning logging procedure...
Timestep 2780001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 15732.937122
Train_EnvstepsSoFar : 2780001
Train_AverageReturn : -20.7
Train_BestReturn : -18.29
TimeSinceStart : 15760.34266424179
Done logging...



Beginning logging procedure...
Timestep 2790001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 15821.650298
Train_EnvstepsSoFar : 2830001
Train_AverageReturn : 14.78
Train_BestReturn : 14.82
TimeSinceStart : 15896.588845729828
Done logging...



Beginning logging procedure...
Timestep 2840001
mean reward (100 episodes) 15.130000
best mean reward 15.130000
running time 15955.729460
Train_EnvstepsSoFar : 2780001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 15732.937121868134
Done logging...



Beginning logging procedure...
Timestep 2790001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 15793.259130
Train_EnvstepsSoFar : 2790001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 15821.65029835701
Done logging...



Beginning logging procedure...
Timestep 2800001
mean reward (100 episodes) -20.720000
best mean reward -18.290000
running time 15882.855903
Train_EnvstepsSoFar : 2840001
Train_AverageReturn : 15.13
Train_BestReturn : 15.13
TimeSinceStart : 15955.729460000992
Done logging...



Beginning logging procedure...
Timestep 2850001
mean reward (100 episodes) 15.590000
best mean reward 15.590000
running time 16015.768647
Train_EnvstepsSoFar : 2790001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 15793.259130239487
Done logging...



Beginning logging procedure...
Timestep 2800001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 15853.244559
Train_EnvstepsSoFar : 2800001
Train_AverageReturn : -20.72
Train_BestReturn : -18.29
TimeSinceStart : 15882.85590338707
Done logging...



Beginning logging procedure...
Timestep 2810001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 15943.503326
Train_EnvstepsSoFar : 2850001
Train_AverageReturn : 15.59
Train_BestReturn : 15.59
TimeSinceStart : 16015.768646717072
Done logging...



Beginning logging procedure...
Timestep 2860001
mean reward (100 episodes) 15.700000
best mean reward 15.700000
running time 16075.339642
Train_EnvstepsSoFar : 2800001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 15853.244559288025
Done logging...



Beginning logging procedure...
Timestep 2810001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 15912.803241
Train_EnvstepsSoFar : 2810001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 15943.503325939178
Done logging...



Beginning logging procedure...
Timestep 2820001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 16004.445092
Train_EnvstepsSoFar : 2860001
Train_AverageReturn : 15.7
Train_BestReturn : 15.7
TimeSinceStart : 16075.339642047882
Done logging...



Beginning logging procedure...
Timestep 2870001
mean reward (100 episodes) 15.870000
best mean reward 15.870000
running time 16134.457350
Train_EnvstepsSoFar : 2810001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 15912.8032412529
Done logging...



Beginning logging procedure...
Timestep 2820001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 15973.480536
Train_EnvstepsSoFar : 2820001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 16004.445092201233
Done logging...



Beginning logging procedure...
Timestep 2830001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 16065.135021
Train_EnvstepsSoFar : 2870001
Train_AverageReturn : 15.87
Train_BestReturn : 15.87
TimeSinceStart : 16134.45735001564
Done logging...



Beginning logging procedure...
Timestep 2880001
mean reward (100 episodes) 16.030000
best mean reward 16.030000
running time 16193.586225
Train_EnvstepsSoFar : 2820001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 15973.480535507202
Done logging...



Beginning logging procedure...
Timestep 2830001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 16033.686817
Train_EnvstepsSoFar : 2830001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 16065.135020971298
Done logging...



Beginning logging procedure...
Timestep 2840001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 16125.881948
Train_EnvstepsSoFar : 2880001
Train_AverageReturn : 16.03
Train_BestReturn : 16.03
TimeSinceStart : 16193.586224794388
Done logging...



Beginning logging procedure...
Timestep 2890001
mean reward (100 episodes) 16.050000
best mean reward 16.050000
running time 16252.656370
Train_EnvstepsSoFar : 2830001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 16033.68681716919
Done logging...



Beginning logging procedure...
Timestep 2840001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 16094.235587
Train_EnvstepsSoFar : 2840001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 16125.88194847107
Done logging...



Beginning logging procedure...
Timestep 2850001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 16186.787981
Train_EnvstepsSoFar : 2890001
Train_AverageReturn : 16.05
Train_BestReturn : 16.05
TimeSinceStart : 16252.656369686127
Done logging...



Beginning logging procedure...
Timestep 2900001
mean reward (100 episodes) 16.110000
best mean reward 16.110000
running time 16311.708059
Train_EnvstepsSoFar : 2840001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 16094.235587358475
Done logging...



Beginning logging procedure...
Timestep 2850001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 16154.616755
Train_EnvstepsSoFar : 2850001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 16186.787980794907
Done logging...



Beginning logging procedure...
Timestep 2860001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 16247.516504
Train_EnvstepsSoFar : 2900001
Train_AverageReturn : 16.11
Train_BestReturn : 16.11
TimeSinceStart : 16311.708059072495
Done logging...



Beginning logging procedure...
Timestep 2910001
mean reward (100 episodes) 16.130000
best mean reward 16.130000
running time 16370.547677
Train_EnvstepsSoFar : 2850001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 16154.616755247116
Done logging...



Beginning logging procedure...
Timestep 2860001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 16215.295379
Train_EnvstepsSoFar : 2860001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 16247.516504049301
Done logging...



Beginning logging procedure...
Timestep 2870001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 16307.648337
Train_EnvstepsSoFar : 2910001
Train_AverageReturn : 16.13
Train_BestReturn : 16.13
TimeSinceStart : 16370.547676801682
Done logging...



Beginning logging procedure...
Timestep 2920001
mean reward (100 episodes) 16.100000
best mean reward 16.130000
running time 16430.141300
Train_EnvstepsSoFar : 2860001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 16215.295379161835
Done logging...



Beginning logging procedure...
Timestep 2870001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 16275.753115
Train_EnvstepsSoFar : 2870001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 16307.648337125778
Done logging...



Beginning logging procedure...
Timestep 2880001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 16367.959495
Train_EnvstepsSoFar : 2920001
Train_AverageReturn : 16.1
Train_BestReturn : 16.13
TimeSinceStart : 16430.141299962997
Done logging...



Beginning logging procedure...
Timestep 2930001
mean reward (100 episodes) 16.240000
best mean reward 16.240000
running time 16489.215515
Train_EnvstepsSoFar : 2870001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 16275.753114700317
Done logging...



Beginning logging procedure...
Timestep 2880001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 16336.065434
Train_EnvstepsSoFar : 2880001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 16367.959495306015
Done logging...



Beginning logging procedure...
Timestep 2890001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 16428.761558
Train_EnvstepsSoFar : 2930001
Train_AverageReturn : 16.24
Train_BestReturn : 16.24
TimeSinceStart : 16489.215515375137
Done logging...



Beginning logging procedure...
Timestep 2940001
mean reward (100 episodes) 16.300000
best mean reward 16.300000
running time 16548.290066
Train_EnvstepsSoFar : 2880001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 16336.065433979034
Done logging...



Beginning logging procedure...
Timestep 2890001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 16395.831179
Train_EnvstepsSoFar : 2890001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 16428.76155781746
Done logging...



Beginning logging procedure...
Timestep 2900001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 16489.568898
Train_EnvstepsSoFar : 2940001
Train_AverageReturn : 16.3
Train_BestReturn : 16.3
TimeSinceStart : 16548.290066480637
Done logging...



Beginning logging procedure...
Timestep 2950001
mean reward (100 episodes) 16.310000
best mean reward 16.310000
running time 16607.227180
Train_EnvstepsSoFar : 2890001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 16395.831179380417
Done logging...



Beginning logging procedure...
Timestep 2900001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 16455.924090
Train_EnvstepsSoFar : 2900001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 16489.56889820099
Done logging...



Beginning logging procedure...
Timestep 2910001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 16550.424531
Train_EnvstepsSoFar : 2950001
Train_AverageReturn : 16.31
Train_BestReturn : 16.31
TimeSinceStart : 16607.2271797657
Done logging...



Beginning logging procedure...
Timestep 2960001
mean reward (100 episodes) 16.370000
best mean reward 16.370000
running time 16666.378580
Train_EnvstepsSoFar : 2900001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 16455.92409014702
Done logging...



Beginning logging procedure...
Timestep 2910001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 16516.451665
Train_EnvstepsSoFar : 2910001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 16550.42453145981
Done logging...



Beginning logging procedure...
Timestep 2920001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 16610.846746
Train_EnvstepsSoFar : 2960001
Train_AverageReturn : 16.37
Train_BestReturn : 16.37
TimeSinceStart : 16666.378579854965
Done logging...



Beginning logging procedure...
Timestep 2970001
mean reward (100 episodes) 16.510000
best mean reward 16.510000
running time 16726.003134
Train_EnvstepsSoFar : 2910001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 16516.45166492462
Done logging...



Beginning logging procedure...
Timestep 2920001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 16576.382109
Train_EnvstepsSoFar : 2920001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 16610.846746206284
Done logging...



Beginning logging procedure...
Timestep 2930001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 16671.135824
Train_EnvstepsSoFar : 2970001
Train_AverageReturn : 16.51
Train_BestReturn : 16.51
TimeSinceStart : 16726.003133535385
Done logging...



Beginning logging procedure...
Timestep 2980001
mean reward (100 episodes) 16.550000
best mean reward 16.550000
running time 16785.313989
Train_EnvstepsSoFar : 2920001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 16576.38210916519
Done logging...



Beginning logging procedure...
Timestep 2930001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 16636.027785
Train_EnvstepsSoFar : 2930001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 16671.135823965073
Done logging...



Beginning logging procedure...
Timestep 2940001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 16731.029970
Train_EnvstepsSoFar : 2980001
Train_AverageReturn : 16.55
Train_BestReturn : 16.55
TimeSinceStart : 16785.313989162445
Done logging...



Beginning logging procedure...
Timestep 2990001
mean reward (100 episodes) 16.630000
best mean reward 16.630000
running time 16845.300819
Train_EnvstepsSoFar : 2930001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 16636.02778530121
Done logging...



Beginning logging procedure...
Timestep 2940001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 16695.451466
Train_EnvstepsSoFar : 2940001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 16731.029970169067
Done logging...



Beginning logging procedure...
Timestep 2950001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 16790.394743
Train_EnvstepsSoFar : 2990001
Train_AverageReturn : 16.63
Train_BestReturn : 16.63
TimeSinceStart : 16845.300819158554
Done logging...



Beginning logging procedure...
Timestep 3000001
mean reward (100 episodes) 16.740000
best mean reward 16.740000
running time 16905.506697
Train_EnvstepsSoFar : 2940001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 16695.45146560669
Done logging...



Beginning logging procedure...
Timestep 2950001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 16755.560537
Train_EnvstepsSoFar : 2950001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 16790.394743442535
Done logging...



Beginning logging procedure...
Timestep 2960001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 16850.298673
Train_EnvstepsSoFar : 3000001
Train_AverageReturn : 16.74
Train_BestReturn : 16.74
TimeSinceStart : 16905.50669693947
Done logging...



Beginning logging procedure...
Timestep 3010001
mean reward (100 episodes) 16.760000
best mean reward 16.760000
running time 16965.649575
Train_EnvstepsSoFar : 2950001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 16755.56053686142
Done logging...



Beginning logging procedure...
Timestep 2960001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 16816.099011
Train_EnvstepsSoFar : 2960001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 16850.298672676086
Done logging...



Beginning logging procedure...
Timestep 2970001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 16910.688846
Train_EnvstepsSoFar : 3010001
Train_AverageReturn : 16.76
Train_BestReturn : 16.76
TimeSinceStart : 16965.649574756622
Done logging...



Beginning logging procedure...
Timestep 3020001
mean reward (100 episodes) 16.840000
best mean reward 16.840000
running time 17025.119442
Train_EnvstepsSoFar : 2960001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 16816.099011421204
Done logging...



Beginning logging procedure...
Timestep 2970001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 16876.568442
Train_EnvstepsSoFar : 2970001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 16910.688846111298
Done logging...



Beginning logging procedure...
Timestep 2980001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 16969.870178
Train_EnvstepsSoFar : 3020001
Train_AverageReturn : 16.84
Train_BestReturn : 16.84
TimeSinceStart : 17025.119442224503
Done logging...



Beginning logging procedure...
Timestep 3030001
mean reward (100 episodes) 16.790000
best mean reward 16.840000
running time 17084.558065
Train_EnvstepsSoFar : 2970001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 16876.568442106247
Done logging...



Beginning logging procedure...
Timestep 2980001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 16936.814851
Train_EnvstepsSoFar : 2980001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 16969.8701775074
Done logging...



Beginning logging procedure...
Timestep 2990001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 17029.756894
Train_EnvstepsSoFar : 3030001
Train_AverageReturn : 16.79
Train_BestReturn : 16.84
TimeSinceStart : 17084.55806517601
Done logging...



Beginning logging procedure...
Timestep 3040001
mean reward (100 episodes) 16.650000
best mean reward 16.840000
running time 17143.628772
Train_EnvstepsSoFar : 2980001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 16936.81485104561
Done logging...



Beginning logging procedure...
Timestep 2990001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 16996.997910
Train_EnvstepsSoFar : 2990001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 17029.756894111633
Done logging...



Beginning logging procedure...
Timestep 3000001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 17090.349166
Train_EnvstepsSoFar : 3040001
Train_AverageReturn : 16.65
Train_BestReturn : 16.84
TimeSinceStart : 17143.62877178192
Done logging...



Beginning logging procedure...
Timestep 3050001
mean reward (100 episodes) 16.630000
best mean reward 16.840000
running time 17202.993304
Train_EnvstepsSoFar : 2990001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 16996.997910261154
Done logging...



Beginning logging procedure...
Timestep 3000001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 17057.512183
Train_EnvstepsSoFar : 3000001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 17090.349165916443
Done logging...



Beginning logging procedure...
Timestep 3010001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 17150.842284
Train_EnvstepsSoFar : 3050001
Train_AverageReturn : 16.63
Train_BestReturn : 16.84
TimeSinceStart : 17202.993304014206
Done logging...



Beginning logging procedure...
Timestep 3060001
mean reward (100 episodes) 16.590000
best mean reward 16.840000
running time 17262.097294
Train_EnvstepsSoFar : 3000001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 17057.512183189392
Done logging...



Beginning logging procedure...
Timestep 3010001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 17118.223609
Train_EnvstepsSoFar : 3010001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 17150.842284202576
Done logging...



Beginning logging procedure...
Timestep 3020001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 17211.166182
Train_EnvstepsSoFar : 3060001
Train_AverageReturn : 16.59
Train_BestReturn : 16.84
TimeSinceStart : 17262.09729409218
Done logging...



Beginning logging procedure...
Timestep 3070001
mean reward (100 episodes) 16.490000
best mean reward 16.840000
running time 17321.932498
Train_EnvstepsSoFar : 3010001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 17118.22360944748
Done logging...



Beginning logging procedure...
Timestep 3020001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 17178.363670
Train_EnvstepsSoFar : 3020001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 17211.16618156433
Done logging...



Beginning logging procedure...
Timestep 3030001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 17271.134269
Train_EnvstepsSoFar : 3070001
Train_AverageReturn : 16.49
Train_BestReturn : 16.84
TimeSinceStart : 17321.932497739792
Done logging...



Beginning logging procedure...
Timestep 3080001
mean reward (100 episodes) 16.460000
best mean reward 16.840000
running time 17381.119226
Train_EnvstepsSoFar : 3020001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 17178.363669872284
Done logging...



Beginning logging procedure...
Timestep 3030001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 17238.809985
Train_EnvstepsSoFar : 3030001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 17271.1342689991
Done logging...



Beginning logging procedure...
Timestep 3040001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 17331.692526
Train_EnvstepsSoFar : 3080001
Train_AverageReturn : 16.46
Train_BestReturn : 16.84
TimeSinceStart : 17381.119225740433
Done logging...



Beginning logging procedure...
Timestep 3090001
mean reward (100 episodes) 16.500000
best mean reward 16.840000
running time 17440.513572
Train_EnvstepsSoFar : 3030001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 17238.80998468399
Done logging...



Beginning logging procedure...
Timestep 3040001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 17298.551957
Train_EnvstepsSoFar : 3040001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 17331.692526340485
Done logging...



Beginning logging procedure...
Timestep 3050001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 17392.337876
Train_EnvstepsSoFar : 3090001
Train_AverageReturn : 16.5
Train_BestReturn : 16.84
TimeSinceStart : 17440.513572216034
Done logging...



Beginning logging procedure...
Timestep 3100001
mean reward (100 episodes) 16.440000
best mean reward 16.840000
running time 17500.056651
Train_EnvstepsSoFar : 3040001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 17298.551957130432
Done logging...



Beginning logging procedure...
Timestep 3050001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 17359.286071
Train_EnvstepsSoFar : 3100001
Train_AverageReturn : 16.44
Train_BestReturn : 16.84
TimeSinceStart : 17500.056651353836
Done logging...



Beginning logging procedure...
Timestep 3110001
mean reward (100 episodes) 16.480000
best mean reward 16.840000
running time 17559.295884
Train_EnvstepsSoFar : 3050001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 17392.33787560463
Done logging...



Beginning logging procedure...
Timestep 3060001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 17453.446463
Train_EnvstepsSoFar : 3050001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 17359.28607058525
Done logging...



Beginning logging procedure...
Timestep 3060001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 17419.592475
Train_EnvstepsSoFar : 3110001
Train_AverageReturn : 16.48
Train_BestReturn : 16.84
TimeSinceStart : 17559.295883893967
Done logging...



Beginning logging procedure...
Timestep 3120001
mean reward (100 episodes) 16.440000
best mean reward 16.840000
running time 17619.081976
Train_EnvstepsSoFar : 3060001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 17453.446462631226
Done logging...



Beginning logging procedure...
Timestep 3070001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 17513.756851
Train_EnvstepsSoFar : 3060001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 17419.592475175858
Done logging...



Beginning logging procedure...
Timestep 3070001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 17479.178389
Train_EnvstepsSoFar : 3120001
Train_AverageReturn : 16.44
Train_BestReturn : 16.84
TimeSinceStart : 17619.08197593689
Done logging...



Beginning logging procedure...
Timestep 3130001
mean reward (100 episodes) 16.460000
best mean reward 16.840000
running time 17678.675904
Train_EnvstepsSoFar : 3070001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 17513.756851434708
Done logging...



Beginning logging procedure...
Timestep 3080001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 17574.897330
Train_EnvstepsSoFar : 3070001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 17479.178389310837
Done logging...



Beginning logging procedure...
Timestep 3080001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 17538.693244
Train_EnvstepsSoFar : 3130001
Train_AverageReturn : 16.46
Train_BestReturn : 16.84
TimeSinceStart : 17678.67590403557
Done logging...



Beginning logging procedure...
Timestep 3140001
mean reward (100 episodes) 16.300000
best mean reward 16.840000
running time 17738.158099
Train_EnvstepsSoFar : 3080001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 17574.89732980728
Done logging...



Beginning logging procedure...
Timestep 3090001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 17635.519730
Train_EnvstepsSoFar : 3080001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 17538.693244457245
Done logging...



Beginning logging procedure...
Timestep 3090001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 17598.393524
Train_EnvstepsSoFar : 3140001
Train_AverageReturn : 16.3
Train_BestReturn : 16.84
TimeSinceStart : 17738.158099412918
Done logging...



Beginning logging procedure...
Timestep 3150001
mean reward (100 episodes) 16.150000
best mean reward 16.840000
running time 17797.431027
Train_EnvstepsSoFar : 3090001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 17635.519730091095
Done logging...



Beginning logging procedure...
Timestep 3100001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 17696.260012
Train_EnvstepsSoFar : 3090001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 17598.393523931503
Done logging...



Beginning logging procedure...
Timestep 3100001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 17658.747874
Train_EnvstepsSoFar : 3150001
Train_AverageReturn : 16.15
Train_BestReturn : 16.84
TimeSinceStart : 17797.431027412415
Done logging...



Beginning logging procedure...
Timestep 3160001
mean reward (100 episodes) 16.010000
best mean reward 16.840000
running time 17856.882797
Train_EnvstepsSoFar : 3100001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 17696.260011911392
Done logging...



Beginning logging procedure...
Timestep 3110001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 17756.514237
Train_EnvstepsSoFar : 3100001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 17658.74787425995
Done logging...



Beginning logging procedure...
Timestep 3110001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 17719.328667
Train_EnvstepsSoFar : 3160001
Train_AverageReturn : 16.01
Train_BestReturn : 16.84
TimeSinceStart : 17856.882797002792
Done logging...



Beginning logging procedure...
Timestep 3170001
mean reward (100 episodes) 15.830000
best mean reward 16.840000
running time 17915.818731
Train_EnvstepsSoFar : 3110001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 17756.51423740387
Done logging...



Beginning logging procedure...
Timestep 3120001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 17817.678259
Train_EnvstepsSoFar : 3110001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 17719.32866716385
Done logging...



Beginning logging procedure...
Timestep 3120001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 17779.511938
Train_EnvstepsSoFar : 3170001
Train_AverageReturn : 15.83
Train_BestReturn : 16.84
TimeSinceStart : 17915.818730831146
Done logging...



Beginning logging procedure...
Timestep 3180001
mean reward (100 episodes) 15.610000
best mean reward 16.840000
running time 17975.020615
Train_EnvstepsSoFar : 3120001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 17817.678258657455
Done logging...



Beginning logging procedure...
Timestep 3130001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 17878.086188
Train_EnvstepsSoFar : 3120001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 17779.511938095093
Done logging...



Beginning logging procedure...
Timestep 3130001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 17839.916532
Train_EnvstepsSoFar : 3180001
Train_AverageReturn : 15.61
Train_BestReturn : 16.84
TimeSinceStart : 17975.020614624023
Done logging...



Beginning logging procedure...
Timestep 3190001
mean reward (100 episodes) 15.420000
best mean reward 16.840000
running time 18034.108162
Train_EnvstepsSoFar : 3130001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 17878.086188077927
Done logging...



Beginning logging procedure...
Timestep 3140001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 17938.656124
Train_EnvstepsSoFar : 3130001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 17839.916531562805
Done logging...



Beginning logging procedure...
Timestep 3140001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 17900.037738
Train_EnvstepsSoFar : 3190001
Train_AverageReturn : 15.42
Train_BestReturn : 16.84
TimeSinceStart : 18034.10816168785
Done logging...



Beginning logging procedure...
Timestep 3200001
mean reward (100 episodes) 15.330000
best mean reward 16.840000
running time 18093.423789
Train_EnvstepsSoFar : 3140001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 17938.65612411499
Done logging...



Beginning logging procedure...
Timestep 3150001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 17999.807678
Train_EnvstepsSoFar : 3140001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 17900.037738084793
Done logging...



Beginning logging procedure...
Timestep 3150001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 17960.616824
Train_EnvstepsSoFar : 3200001
Train_AverageReturn : 15.33
Train_BestReturn : 16.84
TimeSinceStart : 18093.423788547516
Done logging...



Beginning logging procedure...
Timestep 3210001
mean reward (100 episodes) 15.050000
best mean reward 16.840000
running time 18153.154095
Train_EnvstepsSoFar : 3150001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 17999.807677984238
Done logging...



Beginning logging procedure...
Timestep 3160001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 18060.440193
Train_EnvstepsSoFar : 3150001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 17960.616824150085
Done logging...



Beginning logging procedure...
Timestep 3160001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 18020.821792
Train_EnvstepsSoFar : 3210001
Train_AverageReturn : 15.05
Train_BestReturn : 16.84
TimeSinceStart : 18153.154094696045
Done logging...



Beginning logging procedure...
Timestep 3220001
mean reward (100 episodes) 14.870000
best mean reward 16.840000
running time 18212.555514
Train_EnvstepsSoFar : 3160001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 18060.44019293785
Done logging...



Beginning logging procedure...
Timestep 3170001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 18121.266438
Train_EnvstepsSoFar : 3160001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 18020.821791648865
Done logging...



Beginning logging procedure...
Timestep 3170001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 18080.861864
Train_EnvstepsSoFar : 3220001
Train_AverageReturn : 14.87
Train_BestReturn : 16.84
TimeSinceStart : 18212.555513858795
Done logging...



Beginning logging procedure...
Timestep 3230001
mean reward (100 episodes) 14.700000
best mean reward 16.840000
running time 18272.512041
Train_EnvstepsSoFar : 3170001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 18121.266438484192
Done logging...



Beginning logging procedure...
Timestep 3180001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 18181.629182
Train_EnvstepsSoFar : 3170001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 18080.86186361313
Done logging...



Beginning logging procedure...
Timestep 3180001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 18141.510565
Train_EnvstepsSoFar : 3230001
Train_AverageReturn : 14.7
Train_BestReturn : 16.84
TimeSinceStart : 18272.51204061508
Done logging...



Beginning logging procedure...
Timestep 3240001
mean reward (100 episodes) 14.660000
best mean reward 16.840000
running time 18332.204087
Train_EnvstepsSoFar : 3180001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 18181.629181861877
Done logging...



Beginning logging procedure...
Timestep 3190001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 18242.275322
Train_EnvstepsSoFar : 3180001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 18141.51056456566
Done logging...



Beginning logging procedure...
Timestep 3190001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 18201.635438
Train_EnvstepsSoFar : 3240001
Train_AverageReturn : 14.66
Train_BestReturn : 16.84
TimeSinceStart : 18332.204087495804
Done logging...



Beginning logging procedure...
Timestep 3250001
mean reward (100 episodes) 14.600000
best mean reward 16.840000
running time 18392.186470
Train_EnvstepsSoFar : 3190001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 18242.275322198868
Done logging...



Beginning logging procedure...
Timestep 3200001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 18302.069770
Train_EnvstepsSoFar : 3190001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 18201.63543820381
Done logging...



Beginning logging procedure...
Timestep 3200001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 18262.351906
Train_EnvstepsSoFar : 3250001
Train_AverageReturn : 14.6
Train_BestReturn : 16.84
TimeSinceStart : 18392.18646979332
Done logging...



Beginning logging procedure...
Timestep 3260001
mean reward (100 episodes) 14.570000
best mean reward 16.840000
running time 18451.358950
Train_EnvstepsSoFar : 3200001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 18302.069769859314
Done logging...



Beginning logging procedure...
Timestep 3210001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 18362.278406
Train_EnvstepsSoFar : 3200001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 18262.351906061172
Done logging...



Beginning logging procedure...
Timestep 3210001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 18323.235975
Train_EnvstepsSoFar : 3260001
Train_AverageReturn : 14.57
Train_BestReturn : 16.84
TimeSinceStart : 18451.358949661255
Done logging...



Beginning logging procedure...
Timestep 3270001
mean reward (100 episodes) 14.590000
best mean reward 16.840000
running time 18510.917902
Train_EnvstepsSoFar : 3210001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 18362.27840614319
Done logging...



Beginning logging procedure...
Timestep 3220001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 18422.651524
Train_EnvstepsSoFar : 3210001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 18323.235975027084
Done logging...



Beginning logging procedure...
Timestep 3220001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 18383.840726
Train_EnvstepsSoFar : 3270001
Train_AverageReturn : 14.59
Train_BestReturn : 16.84
TimeSinceStart : 18510.917902231216
Done logging...



Beginning logging procedure...
Timestep 3280001
mean reward (100 episodes) 14.660000
best mean reward 16.840000
running time 18570.129219
Train_EnvstepsSoFar : 3220001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 18422.651524305344
Done logging...



Beginning logging procedure...
Timestep 3230001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 18482.767661
Train_EnvstepsSoFar : 3220001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 18383.84072613716
Done logging...



Beginning logging procedure...
Timestep 3230001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 18444.054892
Train_EnvstepsSoFar : 3280001
Train_AverageReturn : 14.66
Train_BestReturn : 16.84
TimeSinceStart : 18570.129219293594
Done logging...



Beginning logging procedure...
Timestep 3290001
mean reward (100 episodes) 14.630000
best mean reward 16.840000
running time 18628.960112
Train_EnvstepsSoFar : 3230001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 18482.767661094666
Done logging...



Beginning logging procedure...
Timestep 3240001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 18543.361479
Train_EnvstepsSoFar : 3230001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 18444.05489230156
Done logging...



Beginning logging procedure...
Timestep 3240001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 18504.711927
Train_EnvstepsSoFar : 3290001
Train_AverageReturn : 14.63
Train_BestReturn : 16.84
TimeSinceStart : 18628.96011185646
Done logging...



Beginning logging procedure...
Timestep 3300001
mean reward (100 episodes) 14.410000
best mean reward 16.840000
running time 18688.126907
Train_EnvstepsSoFar : 3240001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 18543.361478805542
Done logging...



Beginning logging procedure...
Timestep 3250001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 18603.919727
Train_EnvstepsSoFar : 3240001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 18504.71192741394
Done logging...



Beginning logging procedure...
Timestep 3250001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 18564.508842
Train_EnvstepsSoFar : 3300001
Train_AverageReturn : 14.41
Train_BestReturn : 16.84
TimeSinceStart : 18688.126907110214
Done logging...



Beginning logging procedure...
Timestep 3310001
mean reward (100 episodes) 14.340000
best mean reward 16.840000
running time 18747.479640
Train_EnvstepsSoFar : 3250001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 18603.91972708702
Done logging...



Beginning logging procedure...
Timestep 3260001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 18664.258282
Train_EnvstepsSoFar : 3250001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 18564.508841753006
Done logging...



Beginning logging procedure...
Timestep 3260001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 18624.967036
Train_EnvstepsSoFar : 3310001
Train_AverageReturn : 14.34
Train_BestReturn : 16.84
TimeSinceStart : 18747.479640483856
Done logging...



Beginning logging procedure...
Timestep 3320001
mean reward (100 episodes) 14.190000
best mean reward 16.840000
running time 18806.677809
Train_EnvstepsSoFar : 3260001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 18664.25828242302
Done logging...



Beginning logging procedure...
Timestep 3270001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 18724.534395
Train_EnvstepsSoFar : 3260001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 18624.967036247253
Done logging...



Beginning logging procedure...
Timestep 3270001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 18685.558563
Train_EnvstepsSoFar : 3320001
Train_AverageReturn : 14.19
Train_BestReturn : 16.84
TimeSinceStart : 18806.677809000015
Done logging...



Beginning logging procedure...
Timestep 3330001
mean reward (100 episodes) 14.060000
best mean reward 16.840000
running time 18866.063770
Train_EnvstepsSoFar : 3270001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 18724.53439474106
Done logging...



Beginning logging procedure...
Timestep 3280001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 18784.699281
Train_EnvstepsSoFar : 3270001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 18685.55856347084
Done logging...



Beginning logging procedure...
Timestep 3280001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 18745.789947
Train_EnvstepsSoFar : 3330001
Train_AverageReturn : 14.06
Train_BestReturn : 16.84
TimeSinceStart : 18866.063769578934
Done logging...



Beginning logging procedure...
Timestep 3340001
mean reward (100 episodes) 13.860000
best mean reward 16.840000
running time 18925.067472
Train_EnvstepsSoFar : 3280001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 18784.699280500412
Done logging...



Beginning logging procedure...
Timestep 3290001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 18845.231412
Train_EnvstepsSoFar : 3280001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 18745.78994703293
Done logging...



Beginning logging procedure...
Timestep 3290001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 18806.295948
Train_EnvstepsSoFar : 3340001
Train_AverageReturn : 13.86
Train_BestReturn : 16.84
TimeSinceStart : 18925.06747174263
Done logging...



Beginning logging procedure...
Timestep 3350001
mean reward (100 episodes) 13.730000
best mean reward 16.840000
running time 18984.972476
Train_EnvstepsSoFar : 3290001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 18845.231412172318
Done logging...



Beginning logging procedure...
Timestep 3300001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 18905.655803
Train_EnvstepsSoFar : 3290001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 18806.295947790146
Done logging...



Beginning logging procedure...
Timestep 3300001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 18866.590631
Train_EnvstepsSoFar : 3350001
Train_AverageReturn : 13.73
Train_BestReturn : 16.84
TimeSinceStart : 18984.972476243973
Done logging...



Beginning logging procedure...
Timestep 3360001
mean reward (100 episodes) 13.640000
best mean reward 16.840000
running time 19044.186178
Train_EnvstepsSoFar : 3300001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 18905.655803442
Done logging...



Beginning logging procedure...
Timestep 3310001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 18966.324438
Train_EnvstepsSoFar : 3300001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 18866.590631246567
Done logging...



Beginning logging procedure...
Timestep 3310001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 18927.494954
Train_EnvstepsSoFar : 3360001
Train_AverageReturn : 13.64
Train_BestReturn : 16.84
TimeSinceStart : 19044.18617773056
Done logging...



Beginning logging procedure...
Timestep 3370001
mean reward (100 episodes) 13.580000
best mean reward 16.840000
running time 19103.445502
Train_EnvstepsSoFar : 3310001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 18966.324438095093
Done logging...



Beginning logging procedure...
Timestep 3320001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 19027.158362
Train_EnvstepsSoFar : 3310001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 18927.494953632355
Done logging...



Beginning logging procedure...
Timestep 3320001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 18987.545823
Train_EnvstepsSoFar : 3370001
Train_AverageReturn : 13.58
Train_BestReturn : 16.84
TimeSinceStart : 19103.44550228119
Done logging...



Beginning logging procedure...
Timestep 3380001
mean reward (100 episodes) 13.480000
best mean reward 16.840000
running time 19162.816328
Train_EnvstepsSoFar : 3320001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 19027.158361673355
Done logging...



Beginning logging procedure...
Timestep 3330001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 19087.230224
Train_EnvstepsSoFar : 3320001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 18987.54582309723
Done logging...



Beginning logging procedure...
Timestep 3330001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 19048.269314
Train_EnvstepsSoFar : 3380001
Train_AverageReturn : 13.48
Train_BestReturn : 16.84
TimeSinceStart : 19162.816328048706
Done logging...



Beginning logging procedure...
Timestep 3390001
mean reward (100 episodes) 13.490000
best mean reward 16.840000
running time 19222.300707
Train_EnvstepsSoFar : 3330001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 19087.230224370956
Done logging...



Beginning logging procedure...
Timestep 3340001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 19147.519898
Train_EnvstepsSoFar : 3330001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 19048.269314050674
Done logging...



Beginning logging procedure...
Timestep 3340001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 19108.465268
Train_EnvstepsSoFar : 3390001
Train_AverageReturn : 13.49
Train_BestReturn : 16.84
TimeSinceStart : 19222.30070734024
Done logging...



Beginning logging procedure...
Timestep 3400001
mean reward (100 episodes) 13.630000
best mean reward 16.840000
running time 19282.012614
Train_EnvstepsSoFar : 3340001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 19147.519897699356
Done logging...



Beginning logging procedure...
Timestep 3350001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 19207.776508
Train_EnvstepsSoFar : 3340001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 19108.46526813507
Done logging...



Beginning logging procedure...
Timestep 3350001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 19168.578214
Train_EnvstepsSoFar : 3400001
Train_AverageReturn : 13.63
Train_BestReturn : 16.84
TimeSinceStart : 19282.012613534927
Done logging...



Beginning logging procedure...
Timestep 3410001
mean reward (100 episodes) 13.600000
best mean reward 16.840000
running time 19341.548973
Train_EnvstepsSoFar : 3350001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 19207.776507616043
Done logging...



Beginning logging procedure...
Timestep 3360001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 19267.781177
Train_EnvstepsSoFar : 3350001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 19168.57821416855
Done logging...



Beginning logging procedure...
Timestep 3360001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 19228.707147
Train_EnvstepsSoFar : 3410001
Train_AverageReturn : 13.6
Train_BestReturn : 16.84
TimeSinceStart : 19341.54897260666
Done logging...



Beginning logging procedure...
Timestep 3420001
mean reward (100 episodes) 13.680000
best mean reward 16.840000
running time 19400.584793
Train_EnvstepsSoFar : 3360001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 19267.781176805496
Done logging...



Beginning logging procedure...
Timestep 3370001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 19327.992937
Train_EnvstepsSoFar : 3360001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 19228.707147359848
Done logging...



Beginning logging procedure...
Timestep 3370001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 19289.012820
Train_EnvstepsSoFar : 3420001
Train_AverageReturn : 13.68
Train_BestReturn : 16.84
TimeSinceStart : 19400.584792613983
Done logging...



Beginning logging procedure...
Timestep 3430001
mean reward (100 episodes) 13.920000
best mean reward 16.840000
running time 19459.607709
Train_EnvstepsSoFar : 3370001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 19327.992936611176
Done logging...



Beginning logging procedure...
Timestep 3380001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 19388.553330
Train_EnvstepsSoFar : 3370001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 19289.012820005417
Done logging...



Beginning logging procedure...
Timestep 3380001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 19349.434323
Train_EnvstepsSoFar : 3430001
Train_AverageReturn : 13.92
Train_BestReturn : 16.84
TimeSinceStart : 19459.60770893097
Done logging...



Beginning logging procedure...
Timestep 3440001
mean reward (100 episodes) 13.980000
best mean reward 16.840000
running time 19518.875700
Train_EnvstepsSoFar : 3380001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 19388.553330421448
Done logging...



Beginning logging procedure...
Timestep 3390001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 19448.678260
Train_EnvstepsSoFar : 3380001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 19349.434323310852
Done logging...



Beginning logging procedure...
Timestep 3390001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 19409.984019
Train_EnvstepsSoFar : 3440001
Train_AverageReturn : 13.98
Train_BestReturn : 16.84
TimeSinceStart : 19518.875700235367
Done logging...



Beginning logging procedure...
Timestep 3450001
mean reward (100 episodes) 14.030000
best mean reward 16.840000
running time 19578.483840
Train_EnvstepsSoFar : 3390001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 19448.67825961113
Done logging...



Beginning logging procedure...
Timestep 3400001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 19509.032687
Train_EnvstepsSoFar : 3390001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 19409.984018802643
Done logging...



Beginning logging procedure...
Timestep 3400001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 19470.254076
Train_EnvstepsSoFar : 3450001
Train_AverageReturn : 14.03
Train_BestReturn : 16.84
TimeSinceStart : 19578.48383998871
Done logging...



Beginning logging procedure...
Timestep 3460001
mean reward (100 episodes) 14.280000
best mean reward 16.840000
running time 19638.524387
Train_EnvstepsSoFar : 3400001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 19509.032687187195
Done logging...



Beginning logging procedure...
Timestep 3410001
mean reward (100 episodes) -20.710000
best mean reward -18.290000
running time 19569.059305
Train_EnvstepsSoFar : 3400001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 19470.25407576561
Done logging...



Beginning logging procedure...
Timestep 3410001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 19530.714368
Train_EnvstepsSoFar : 3460001
Train_AverageReturn : 14.28
Train_BestReturn : 16.84
TimeSinceStart : 19638.52438735962
Done logging...



Beginning logging procedure...
Timestep 3470001
mean reward (100 episodes) 14.390000
best mean reward 16.840000
running time 19698.131587
Train_EnvstepsSoFar : 3410001
Train_AverageReturn : -20.71
Train_BestReturn : -18.29
TimeSinceStart : 19569.05930495262
Done logging...



Beginning logging procedure...
Timestep 3420001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 19629.458217
Train_EnvstepsSoFar : 3410001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 19530.714367628098
Done logging...



Beginning logging procedure...
Timestep 3420001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 19590.928876
Train_EnvstepsSoFar : 3470001
Train_AverageReturn : 14.39
Train_BestReturn : 16.84
TimeSinceStart : 19698.131586790085
Done logging...



Beginning logging procedure...
Timestep 3480001
mean reward (100 episodes) 14.560000
best mean reward 16.840000
running time 19757.906884
Train_EnvstepsSoFar : 3420001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 19629.458216905594
Done logging...



Beginning logging procedure...
Timestep 3430001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 19690.354339
Train_EnvstepsSoFar : 3420001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 19590.928875684738
Done logging...



Beginning logging procedure...
Timestep 3430001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 19651.352835
Train_EnvstepsSoFar : 3480001
Train_AverageReturn : 14.56
Train_BestReturn : 16.84
TimeSinceStart : 19757.906883716583
Done logging...



Beginning logging procedure...
Timestep 3490001
mean reward (100 episodes) 14.710000
best mean reward 16.840000
running time 19817.274522
Train_EnvstepsSoFar : 3430001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 19690.354338645935
Done logging...



Beginning logging procedure...
Timestep 3440001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 19750.811469
Train_EnvstepsSoFar : 3430001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 19651.352835178375
Done logging...



Beginning logging procedure...
Timestep 3440001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 19711.896324
Train_EnvstepsSoFar : 3490001
Train_AverageReturn : 14.71
Train_BestReturn : 16.84
TimeSinceStart : 19817.274522066116
Done logging...



Beginning logging procedure...
Timestep 3500001
mean reward (100 episodes) 14.920000
best mean reward 16.840000
running time 19876.472127
Train_EnvstepsSoFar : 3440001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 19750.811468839645
Done logging...



Beginning logging procedure...
Timestep 3450001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 19811.472680
Train_EnvstepsSoFar : 3500001
Train_AverageReturn : 14.92
Train_BestReturn : 16.84
TimeSinceStart : 19876.472126722336
Done logging...



Beginning logging procedure...
Timestep 3510001
mean reward (100 episodes) 14.960000
best mean reward 16.840000
running time 19935.834514
Train_EnvstepsSoFar : 3440001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 19711.896323680878
Done logging...



Beginning logging procedure...
Timestep 3450001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 19772.433958
Train_EnvstepsSoFar : 3450001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 19811.47267961502
Done logging...



Beginning logging procedure...
Timestep 3460001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 19871.836735
Train_EnvstepsSoFar : 3510001
Train_AverageReturn : 14.96
Train_BestReturn : 16.84
TimeSinceStart : 19935.834513902664
Done logging...



Beginning logging procedure...
Timestep 3520001
mean reward (100 episodes) 14.890000
best mean reward 16.840000
running time 19995.949372
Train_EnvstepsSoFar : 3450001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 19772.43395781517
Done logging...



Beginning logging procedure...
Timestep 3460001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 19833.753870
Train_EnvstepsSoFar : 3460001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 19871.836735248566
Done logging...



Beginning logging procedure...
Timestep 3470001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 19931.990752
Train_EnvstepsSoFar : 3520001
Train_AverageReturn : 14.89
Train_BestReturn : 16.84
TimeSinceStart : 19995.949372053146
Done logging...



Beginning logging procedure...
Timestep 3530001
mean reward (100 episodes) 14.980000
best mean reward 16.840000
running time 20055.796986
Train_EnvstepsSoFar : 3460001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 19833.753870487213
Done logging...



Beginning logging procedure...
Timestep 3470001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 19894.316141
Train_EnvstepsSoFar : 3470001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 19931.990751504898
Done logging...



Beginning logging procedure...
Timestep 3480001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 19992.203695
Train_EnvstepsSoFar : 3530001
Train_AverageReturn : 14.98
Train_BestReturn : 16.84
TimeSinceStart : 20055.796986103058
Done logging...



Beginning logging procedure...
Timestep 3540001
mean reward (100 episodes) 15.220000
best mean reward 16.840000
running time 20115.228876
Train_EnvstepsSoFar : 3470001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 19894.31614136696
Done logging...



Beginning logging procedure...
Timestep 3480001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 19954.298513
Train_EnvstepsSoFar : 3480001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 19992.20369529724
Done logging...



Beginning logging procedure...
Timestep 3490001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 20053.037887
Train_EnvstepsSoFar : 3540001
Train_AverageReturn : 15.22
Train_BestReturn : 16.84
TimeSinceStart : 20115.228875875473
Done logging...



Beginning logging procedure...
Timestep 3550001
mean reward (100 episodes) 15.390000
best mean reward 16.840000
running time 20174.692088
Train_EnvstepsSoFar : 3480001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 19954.298513412476
Done logging...



Beginning logging procedure...
Timestep 3490001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 20013.674960
Train_EnvstepsSoFar : 3490001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 20053.037887334824
Done logging...



Beginning logging procedure...
Timestep 3500001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 20113.577400
Train_EnvstepsSoFar : 3550001
Train_AverageReturn : 15.39
Train_BestReturn : 16.84
TimeSinceStart : 20174.692087888718
Done logging...



Beginning logging procedure...
Timestep 3560001
mean reward (100 episodes) 15.640000
best mean reward 16.840000
running time 20234.956767
Train_EnvstepsSoFar : 3490001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 20013.674959659576
Done logging...



Beginning logging procedure...
Timestep 3500001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 20073.498857
Train_EnvstepsSoFar : 3500001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 20113.577400445938
Done logging...



Beginning logging procedure...
Timestep 3510001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 20174.215654
Train_EnvstepsSoFar : 3560001
Train_AverageReturn : 15.64
Train_BestReturn : 16.84
TimeSinceStart : 20234.956767082214
Done logging...



Beginning logging procedure...
Timestep 3570001
mean reward (100 episodes) 15.940000
best mean reward 16.840000
running time 20295.040560
Train_EnvstepsSoFar : 3500001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 20073.498856782913
Done logging...



Beginning logging procedure...
Timestep 3510001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 20133.184496
Train_EnvstepsSoFar : 3510001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 20174.215653657913
Done logging...



Beginning logging procedure...
Timestep 3520001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 20234.645605
Train_EnvstepsSoFar : 3570001
Train_AverageReturn : 15.94
Train_BestReturn : 16.84
TimeSinceStart : 20295.040560483932
Done logging...



Beginning logging procedure...
Timestep 3580001
mean reward (100 episodes) 16.150000
best mean reward 16.840000
running time 20354.570824
Train_EnvstepsSoFar : 3510001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 20133.184495925903
Done logging...



Beginning logging procedure...
Timestep 3520001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 20193.411005
Train_EnvstepsSoFar : 3520001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 20234.64560484886
Done logging...



Beginning logging procedure...
Timestep 3530001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 20296.305538
Train_EnvstepsSoFar : 3580001
Train_AverageReturn : 16.15
Train_BestReturn : 16.84
TimeSinceStart : 20354.57082438469
Done logging...



Beginning logging procedure...
Timestep 3590001
mean reward (100 episodes) 16.280000
best mean reward 16.840000
running time 20414.194022
Train_EnvstepsSoFar : 3520001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 20193.41100549698
Done logging...



Beginning logging procedure...
Timestep 3530001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 20253.434283
Train_EnvstepsSoFar : 3530001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 20296.30553793907
Done logging...



Beginning logging procedure...
Timestep 3540001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 20356.602889
Train_EnvstepsSoFar : 3590001
Train_AverageReturn : 16.28
Train_BestReturn : 16.84
TimeSinceStart : 20414.19402217865
Done logging...



Beginning logging procedure...
Timestep 3600001
mean reward (100 episodes) 16.340000
best mean reward 16.840000
running time 20473.872484
Train_EnvstepsSoFar : 3530001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 20253.434283018112
Done logging...



Beginning logging procedure...
Timestep 3540001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 20313.389391
Train_EnvstepsSoFar : 3540001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 20356.602889299393
Done logging...



Beginning logging procedure...
Timestep 3550001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 20417.098124
Train_EnvstepsSoFar : 3600001
Train_AverageReturn : 16.34
Train_BestReturn : 16.84
TimeSinceStart : 20473.872484445572
Done logging...



Beginning logging procedure...
Timestep 3610001
mean reward (100 episodes) 16.510000
best mean reward 16.840000
running time 20533.837990
Train_EnvstepsSoFar : 3540001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 20313.389390707016
Done logging...



Beginning logging procedure...
Timestep 3550001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 20372.755232
Train_EnvstepsSoFar : 3550001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 20417.098123550415
Done logging...



Beginning logging procedure...
Timestep 3560001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 20477.666738
Train_EnvstepsSoFar : 3610001
Train_AverageReturn : 16.51
Train_BestReturn : 16.84
TimeSinceStart : 20533.837990045547
Done logging...



Beginning logging procedure...
Timestep 3620001
mean reward (100 episodes) 16.610000
best mean reward 16.840000
running time 20593.410639
Train_EnvstepsSoFar : 3550001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 20372.7552318573
Done logging...



Beginning logging procedure...
Timestep 3560001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 20432.520090
Train_EnvstepsSoFar : 3560001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 20477.666737794876
Done logging...



Beginning logging procedure...
Timestep 3570001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 20538.330018
Train_EnvstepsSoFar : 3620001
Train_AverageReturn : 16.61
Train_BestReturn : 16.84
TimeSinceStart : 20593.410638809204
Done logging...



Beginning logging procedure...
Timestep 3630001
mean reward (100 episodes) 16.760000
best mean reward 16.840000
running time 20652.966103
Train_EnvstepsSoFar : 3560001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 20432.520090341568
Done logging...



Beginning logging procedure...
Timestep 3570001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 20492.365546
Train_EnvstepsSoFar : 3570001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 20538.33001756668
Done logging...



Beginning logging procedure...
Timestep 3580001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 20599.100821
Train_EnvstepsSoFar : 3630001
Train_AverageReturn : 16.76
Train_BestReturn : 16.84
TimeSinceStart : 20652.966102600098
Done logging...



Beginning logging procedure...
Timestep 3640001
mean reward (100 episodes) 16.830000
best mean reward 16.840000
running time 20712.264228
Train_EnvstepsSoFar : 3570001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 20492.3655462265
Done logging...



Beginning logging procedure...
Timestep 3580001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 20552.619223
Train_EnvstepsSoFar : 3580001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 20599.10082101822
Done logging...



Beginning logging procedure...
Timestep 3590001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 20660.062838
Train_EnvstepsSoFar : 3640001
Train_AverageReturn : 16.83
Train_BestReturn : 16.84
TimeSinceStart : 20712.264228343964
Done logging...



Beginning logging procedure...
Timestep 3650001
mean reward (100 episodes) 17.010000
best mean reward 17.010000
running time 20771.841179
Train_EnvstepsSoFar : 3580001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 20552.61922264099
Done logging...



Beginning logging procedure...
Timestep 3590001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 20612.951309
Train_EnvstepsSoFar : 3590001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 20660.062838077545
Done logging...



Beginning logging procedure...
Timestep 3600001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 20720.470250
Train_EnvstepsSoFar : 3650001
Train_AverageReturn : 17.01
Train_BestReturn : 17.01
TimeSinceStart : 20771.84117937088
Done logging...



Beginning logging procedure...
Timestep 3660001
mean reward (100 episodes) 17.010000
best mean reward 17.010000
running time 20831.775533
Train_EnvstepsSoFar : 3590001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 20612.951308727264
Done logging...



Beginning logging procedure...
Timestep 3600001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 20672.744728
Train_EnvstepsSoFar : 3600001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 20720.470249652863
Done logging...



Beginning logging procedure...
Timestep 3610001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 20780.903537
Train_EnvstepsSoFar : 3660001
Train_AverageReturn : 17.01
Train_BestReturn : 17.01
TimeSinceStart : 20831.77553343773
Done logging...



Beginning logging procedure...
Timestep 3670001
mean reward (100 episodes) 17.180000
best mean reward 17.180000
running time 20891.618438
Train_EnvstepsSoFar : 3600001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 20672.74472784996
Done logging...



Beginning logging procedure...
Timestep 3610001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 20733.340849
Train_EnvstepsSoFar : 3610001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 20780.90353679657
Done logging...



Beginning logging procedure...
Timestep 3620001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 20840.927989
Train_EnvstepsSoFar : 3670001
Train_AverageReturn : 17.18
Train_BestReturn : 17.18
TimeSinceStart : 20891.618438482285
Done logging...



Beginning logging procedure...
Timestep 3680001
mean reward (100 episodes) 17.210000
best mean reward 17.210000
running time 20951.844516
Train_EnvstepsSoFar : 3610001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 20733.340849399567
Done logging...



Beginning logging procedure...
Timestep 3620001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 20794.229392
Train_EnvstepsSoFar : 3620001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 20840.92798948288
Done logging...



Beginning logging procedure...
Timestep 3630001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 20901.403396
Train_EnvstepsSoFar : 3680001
Train_AverageReturn : 17.21
Train_BestReturn : 17.21
TimeSinceStart : 20951.844515800476
Done logging...



Beginning logging procedure...
Timestep 3690001
mean reward (100 episodes) 17.210000
best mean reward 17.210000
running time 21011.600267
Train_EnvstepsSoFar : 3620001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 20794.229392290115
Done logging...



Beginning logging procedure...
Timestep 3630001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 20854.449195
Train_EnvstepsSoFar : 3630001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 20901.40339589119
Done logging...



Beginning logging procedure...
Timestep 3640001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 20961.848496
Train_EnvstepsSoFar : 3690001
Train_AverageReturn : 17.21
Train_BestReturn : 17.21
TimeSinceStart : 21011.60026741028
Done logging...



Beginning logging procedure...
Timestep 3700001
mean reward (100 episodes) 17.250000
best mean reward 17.250000
running time 21070.911037
Train_EnvstepsSoFar : 3630001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 20854.44919538498
Done logging...



Beginning logging procedure...
Timestep 3640001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 20914.808875
Train_EnvstepsSoFar : 3640001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 20961.848496198654
Done logging...



Beginning logging procedure...
Timestep 3650001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 21021.831069
Train_EnvstepsSoFar : 3700001
Train_AverageReturn : 17.25
Train_BestReturn : 17.25
TimeSinceStart : 21070.911036729813
Done logging...



Beginning logging procedure...
Timestep 3710001
mean reward (100 episodes) 17.160000
best mean reward 17.250000
running time 21130.405560
Train_EnvstepsSoFar : 3640001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 20914.808875083923
Done logging...



Beginning logging procedure...
Timestep 3650001
mean reward (100 episodes) -20.620000
best mean reward -20.030000
running time 20975.378531
Train_EnvstepsSoFar : 3650001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 21021.831068992615
Done logging...



Beginning logging procedure...
Timestep 3660001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 21082.402853
Train_EnvstepsSoFar : 3710001
Train_AverageReturn : 17.16
Train_BestReturn : 17.25
TimeSinceStart : 21130.405559539795
Done logging...



Beginning logging procedure...
Timestep 3720001
mean reward (100 episodes) 17.020000
best mean reward 17.250000
running time 21189.737497
Train_EnvstepsSoFar : 3650001
Train_AverageReturn : -20.62
Train_BestReturn : -20.03
TimeSinceStart : 20975.37853050232
Done logging...



Beginning logging procedure...
Timestep 3660001
mean reward (100 episodes) -20.620000
best mean reward -20.030000
running time 21036.187156
Train_EnvstepsSoFar : 3720001
Train_AverageReturn : 17.02
Train_BestReturn : 17.25
TimeSinceStart : 21189.737497329712
Done logging...



Beginning logging procedure...
Timestep 3730001
mean reward (100 episodes) 17.120000
best mean reward 17.250000
running time 21249.007787
Train_EnvstepsSoFar : 3660001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 21082.402853012085
Done logging...



Beginning logging procedure...
Timestep 3670001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 21143.243541
Train_EnvstepsSoFar : 3660001
Train_AverageReturn : -20.62
Train_BestReturn : -20.03
TimeSinceStart : 21036.187156438828
Done logging...



Beginning logging procedure...
Timestep 3670001
mean reward (100 episodes) -20.580000
best mean reward -20.030000
running time 21096.035350
Train_EnvstepsSoFar : 3730001
Train_AverageReturn : 17.12
Train_BestReturn : 17.25
TimeSinceStart : 21249.007786512375
Done logging...



Beginning logging procedure...
Timestep 3740001
mean reward (100 episodes) 17.130000
best mean reward 17.250000
running time 21308.470982
Train_EnvstepsSoFar : 3670001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 21143.243541002274
Done logging...



Beginning logging procedure...
Timestep 3680001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 21203.583573
Train_EnvstepsSoFar : 3670001
Train_AverageReturn : -20.58
Train_BestReturn : -20.03
TimeSinceStart : 21096.035350084305
Done logging...



Beginning logging procedure...
Timestep 3680001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 21155.705072
Train_EnvstepsSoFar : 3740001
Train_AverageReturn : 17.13
Train_BestReturn : 17.25
TimeSinceStart : 21308.4709815979
Done logging...



Beginning logging procedure...
Timestep 3750001
mean reward (100 episodes) 17.100000
best mean reward 17.250000
running time 21367.296940
Train_EnvstepsSoFar : 3680001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 21203.58357334137
Done logging...



Beginning logging procedure...
Timestep 3690001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 21264.248145
Train_EnvstepsSoFar : 3680001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 21155.705071926117
Done logging...



Beginning logging procedure...
Timestep 3690001
mean reward (100 episodes) -20.580000
best mean reward -20.030000
running time 21215.578622
Train_EnvstepsSoFar : 3750001
Train_AverageReturn : 17.1
Train_BestReturn : 17.25
TimeSinceStart : 21367.296940088272
Done logging...



Beginning logging procedure...
Timestep 3760001
mean reward (100 episodes) 17.180000
best mean reward 17.250000
running time 21427.077471
Train_EnvstepsSoFar : 3690001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 21264.248144865036
Done logging...



Beginning logging procedure...
Timestep 3700001
mean reward (100 episodes) -20.730000
best mean reward -18.290000
running time 21324.578285
Train_EnvstepsSoFar : 3690001
Train_AverageReturn : -20.58
Train_BestReturn : -20.03
TimeSinceStart : 21215.578622102737
Done logging...



Beginning logging procedure...
Timestep 3700001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 21276.002720
Train_EnvstepsSoFar : 3760001
Train_AverageReturn : 17.18
Train_BestReturn : 17.25
TimeSinceStart : 21427.077471256256
Done logging...



Beginning logging procedure...
Timestep 3770001
mean reward (100 episodes) 17.160000
best mean reward 17.250000
running time 21486.821875
Train_EnvstepsSoFar : 3700001
Train_AverageReturn : -20.73
Train_BestReturn : -18.29
TimeSinceStart : 21324.578285455704
Done logging...



Beginning logging procedure...
Timestep 3710001
mean reward (100 episodes) -20.710000
best mean reward -18.290000
running time 21384.555936
Train_EnvstepsSoFar : 3700001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 21276.00271987915
Done logging...



Beginning logging procedure...
Timestep 3710001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 21336.563276
Train_EnvstepsSoFar : 3770001
Train_AverageReturn : 17.16
Train_BestReturn : 17.25
TimeSinceStart : 21486.821875333786
Done logging...



Beginning logging procedure...
Timestep 3780001
mean reward (100 episodes) 17.170000
best mean reward 17.250000
running time 21546.122449
Train_EnvstepsSoFar : 3710001
Train_AverageReturn : -20.71
Train_BestReturn : -18.29
TimeSinceStart : 21384.55593585968
Done logging...



Beginning logging procedure...
Timestep 3720001
mean reward (100 episodes) -20.710000
best mean reward -18.290000
running time 21445.405477
Train_EnvstepsSoFar : 3710001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 21336.563276052475
Done logging...



Beginning logging procedure...
Timestep 3720001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 21396.269574
Train_EnvstepsSoFar : 3780001
Train_AverageReturn : 17.17
Train_BestReturn : 17.25
TimeSinceStart : 21546.12244939804
Done logging...



Beginning logging procedure...
Timestep 3790001
mean reward (100 episodes) 17.130000
best mean reward 17.250000
running time 21605.259292
Train_EnvstepsSoFar : 3720001
Train_AverageReturn : -20.71
Train_BestReturn : -18.29
TimeSinceStart : 21445.405477046967
Done logging...



Beginning logging procedure...
Timestep 3730001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 21506.264397
Train_EnvstepsSoFar : 3720001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 21396.269574403763
Done logging...



Beginning logging procedure...
Timestep 3730001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 21456.208369
Train_EnvstepsSoFar : 3790001
Train_AverageReturn : 17.13
Train_BestReturn : 17.25
TimeSinceStart : 21605.259291648865
Done logging...



Beginning logging procedure...
Timestep 3800001
mean reward (100 episodes) 17.190000
best mean reward 17.250000
running time 21664.185637
Train_EnvstepsSoFar : 3730001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 21506.2643969059
Done logging...



Beginning logging procedure...
Timestep 3740001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 21567.116517
Train_EnvstepsSoFar : 3730001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 21456.208369016647
Done logging...



Beginning logging procedure...
Timestep 3740001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 21516.080436
Train_EnvstepsSoFar : 3800001
Train_AverageReturn : 17.19
Train_BestReturn : 17.25
TimeSinceStart : 21664.185636520386
Done logging...



Beginning logging procedure...
Timestep 3810001
mean reward (100 episodes) 17.230000
best mean reward 17.250000
running time 21723.220254
Train_EnvstepsSoFar : 3740001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 21567.116516828537
Done logging...



Beginning logging procedure...
Timestep 3750001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 21627.617625
Train_EnvstepsSoFar : 3740001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 21516.08043551445
Done logging...



Beginning logging procedure...
Timestep 3750001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 21575.764911
Train_EnvstepsSoFar : 3810001
Train_AverageReturn : 17.23
Train_BestReturn : 17.25
TimeSinceStart : 21723.220254421234
Done logging...



Beginning logging procedure...
Timestep 3820001
mean reward (100 episodes) 17.250000
best mean reward 17.250000
running time 21783.267089
Train_EnvstepsSoFar : 3750001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 21627.617624521255
Done logging...



Beginning logging procedure...
Timestep 3760001
mean reward (100 episodes) -20.740000
best mean reward -18.290000
running time 21688.072193
Train_EnvstepsSoFar : 3750001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 21575.764911174774
Done logging...



Beginning logging procedure...
Timestep 3760001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 21635.671150
Train_EnvstepsSoFar : 3820001
Train_AverageReturn : 17.25
Train_BestReturn : 17.25
TimeSinceStart : 21783.267088651657
Done logging...



Beginning logging procedure...
Timestep 3830001
mean reward (100 episodes) 17.220000
best mean reward 17.250000
running time 21842.991415
Train_EnvstepsSoFar : 3760001
Train_AverageReturn : -20.74
Train_BestReturn : -18.29
TimeSinceStart : 21688.072192668915
Done logging...



Beginning logging procedure...
Timestep 3770001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 21748.766066
Train_EnvstepsSoFar : 3760001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 21635.6711499691
Done logging...



Beginning logging procedure...
Timestep 3770001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 21696.225992
Train_EnvstepsSoFar : 3830001
Train_AverageReturn : 17.22
Train_BestReturn : 17.25
TimeSinceStart : 21842.991415262222
Done logging...



Beginning logging procedure...
Timestep 3840001
mean reward (100 episodes) 17.230000
best mean reward 17.250000
running time 21902.691876
Train_EnvstepsSoFar : 3770001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 21748.766065835953
Done logging...



Beginning logging procedure...
Timestep 3780001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 21809.158645
Train_EnvstepsSoFar : 3770001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 21696.22599172592
Done logging...



Beginning logging procedure...
Timestep 3780001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 21756.325926
Train_EnvstepsSoFar : 3840001
Train_AverageReturn : 17.23
Train_BestReturn : 17.25
TimeSinceStart : 21902.691875696182
Done logging...



Beginning logging procedure...
Timestep 3850001
mean reward (100 episodes) 17.330000
best mean reward 17.330000
running time 21962.218950
Train_EnvstepsSoFar : 3780001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 21809.158644914627
Done logging...



Beginning logging procedure...
Timestep 3790001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 21869.523168
Train_EnvstepsSoFar : 3780001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 21756.325926303864
Done logging...



Beginning logging procedure...
Timestep 3790001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 21817.113996
Train_EnvstepsSoFar : 3850001
Train_AverageReturn : 17.33
Train_BestReturn : 17.33
TimeSinceStart : 21962.218950033188
Done logging...



Beginning logging procedure...
Timestep 3860001
mean reward (100 episodes) 17.270000
best mean reward 17.330000
running time 22022.264944
Train_EnvstepsSoFar : 3790001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 21869.523167848587
Done logging...



Beginning logging procedure...
Timestep 3800001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 21930.105283
Train_EnvstepsSoFar : 3790001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 21817.1139960289
Done logging...



Beginning logging procedure...
Timestep 3800001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 21877.668169
Train_EnvstepsSoFar : 3860001
Train_AverageReturn : 17.27
Train_BestReturn : 17.33
TimeSinceStart : 22022.264944076538
Done logging...



Beginning logging procedure...
Timestep 3870001
mean reward (100 episodes) 17.350000
best mean reward 17.350000
running time 22081.174339
Train_EnvstepsSoFar : 3800001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 21930.105283498764
Done logging...



Beginning logging procedure...
Timestep 3810001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 21990.527625
Train_EnvstepsSoFar : 3800001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 21877.668169260025
Done logging...



Beginning logging procedure...
Timestep 3810001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 21937.995287
Train_EnvstepsSoFar : 3870001
Train_AverageReturn : 17.35
Train_BestReturn : 17.35
TimeSinceStart : 22081.174339294434
Done logging...



Beginning logging procedure...
Timestep 3880001
mean reward (100 episodes) 17.350000
best mean reward 17.350000
running time 22141.328533
Train_EnvstepsSoFar : 3810001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 21990.527624607086
Done logging...



Beginning logging procedure...
Timestep 3820001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 22051.074498
Train_EnvstepsSoFar : 3810001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 21937.99528694153
Done logging...



Beginning logging procedure...
Timestep 3820001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 21997.997859
Train_EnvstepsSoFar : 3880001
Train_AverageReturn : 17.35
Train_BestReturn : 17.35
TimeSinceStart : 22141.32853269577
Done logging...



Beginning logging procedure...
Timestep 3890001
mean reward (100 episodes) 17.300000
best mean reward 17.350000
running time 22200.725009
Train_EnvstepsSoFar : 3820001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 22051.074498176575
Done logging...



Beginning logging procedure...
Timestep 3830001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 22111.443110
Train_EnvstepsSoFar : 3820001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 21997.997858524323
Done logging...



Beginning logging procedure...
Timestep 3830001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 22057.640555
Train_EnvstepsSoFar : 3890001
Train_AverageReturn : 17.3
Train_BestReturn : 17.35
TimeSinceStart : 22200.72500896454
Done logging...



Beginning logging procedure...
Timestep 3900001
mean reward (100 episodes) 17.310000
best mean reward 17.350000
running time 22260.861427
Train_EnvstepsSoFar : 3830001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 22111.443109750748
Done logging...



Beginning logging procedure...
Timestep 3840001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 22171.751154
Train_EnvstepsSoFar : 3830001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 22057.640555381775
Done logging...



Beginning logging procedure...
Timestep 3840001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 22117.729167
Train_EnvstepsSoFar : 3900001
Train_AverageReturn : 17.31
Train_BestReturn : 17.35
TimeSinceStart : 22260.86142683029
Done logging...



Beginning logging procedure...
Timestep 3910001
mean reward (100 episodes) 17.230000
best mean reward 17.350000
running time 22320.247329
Train_EnvstepsSoFar : 3840001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 22171.75115442276
Done logging...



Beginning logging procedure...
Timestep 3850001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 22231.952914
Train_EnvstepsSoFar : 3840001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 22117.729167222977
Done logging...



Beginning logging procedure...
Timestep 3850001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 22177.851904
Train_EnvstepsSoFar : 3910001
Train_AverageReturn : 17.23
Train_BestReturn : 17.35
TimeSinceStart : 22320.247329235077
Done logging...



Beginning logging procedure...
Timestep 3920001
mean reward (100 episodes) 17.220000
best mean reward 17.350000
running time 22379.804885
Train_EnvstepsSoFar : 3850001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 22231.952914237976
Done logging...



Beginning logging procedure...
Timestep 3860001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 22292.222337
Train_EnvstepsSoFar : 3850001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 22177.851903676987
Done logging...



Beginning logging procedure...
Timestep 3860001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 22238.284271
Train_EnvstepsSoFar : 3920001
Train_AverageReturn : 17.22
Train_BestReturn : 17.35
TimeSinceStart : 22379.804884672165
Done logging...



Beginning logging procedure...
Timestep 3930001
mean reward (100 episodes) 17.290000
best mean reward 17.350000
running time 22439.310647
Train_EnvstepsSoFar : 3860001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 22292.222336530685
Done logging...



Beginning logging procedure...
Timestep 3870001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 22352.876899
Train_EnvstepsSoFar : 3860001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 22238.28427052498
Done logging...



Beginning logging procedure...
Timestep 3870001
mean reward (100 episodes) -20.620000
best mean reward -20.030000
running time 22298.814512
Train_EnvstepsSoFar : 3930001
Train_AverageReturn : 17.29
Train_BestReturn : 17.35
TimeSinceStart : 22439.310647010803
Done logging...



Beginning logging procedure...
Timestep 3940001
mean reward (100 episodes) 17.270000
best mean reward 17.350000
running time 22498.684846
Train_EnvstepsSoFar : 3870001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 22352.876899003983
Done logging...



Beginning logging procedure...
Timestep 3880001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 22413.557524
Train_EnvstepsSoFar : 3870001
Train_AverageReturn : -20.62
Train_BestReturn : -20.03
TimeSinceStart : 22298.814511537552
Done logging...



Beginning logging procedure...
Timestep 3880001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 22358.986318
Train_EnvstepsSoFar : 3940001
Train_AverageReturn : 17.27
Train_BestReturn : 17.35
TimeSinceStart : 22498.68484568596
Done logging...



Beginning logging procedure...
Timestep 3950001
mean reward (100 episodes) 17.290000
best mean reward 17.350000
running time 22557.918349
Train_EnvstepsSoFar : 3880001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 22413.557523965836
Done logging...



Beginning logging procedure...
Timestep 3890001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 22474.385296
Train_EnvstepsSoFar : 3880001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 22358.986317873
Done logging...



Beginning logging procedure...
Timestep 3890001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 22419.064449
Train_EnvstepsSoFar : 3950001
Train_AverageReturn : 17.29
Train_BestReturn : 17.35
TimeSinceStart : 22557.918348550797
Done logging...



Beginning logging procedure...
Timestep 3960001
mean reward (100 episodes) 17.400000
best mean reward 17.400000
running time 22617.575326
Train_EnvstepsSoFar : 3890001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 22474.38529586792
Done logging...



Beginning logging procedure...
Timestep 3900001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 22535.049527
Train_EnvstepsSoFar : 3890001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 22419.064449071884
Done logging...



Beginning logging procedure...
Timestep 3900001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 22479.477438
Train_EnvstepsSoFar : 3960001
Train_AverageReturn : 17.4
Train_BestReturn : 17.4
TimeSinceStart : 22617.5753262043
Done logging...



Beginning logging procedure...
Timestep 3970001
mean reward (100 episodes) 17.350000
best mean reward 17.400000
running time 22677.173277
Train_EnvstepsSoFar : 3900001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 22535.049527168274
Done logging...



Beginning logging procedure...
Timestep 3910001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 22595.403531
Train_EnvstepsSoFar : 3900001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 22479.47743844986
Done logging...



Beginning logging procedure...
Timestep 3910001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 22539.486047
Train_EnvstepsSoFar : 3970001
Train_AverageReturn : 17.35
Train_BestReturn : 17.4
TimeSinceStart : 22677.173276662827
Done logging...



Beginning logging procedure...
Timestep 3980001
mean reward (100 episodes) 17.400000
best mean reward 17.400000
running time 22736.692130
Train_EnvstepsSoFar : 3910001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 22595.403531312943
Done logging...



Beginning logging procedure...
Timestep 3920001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 22656.647438
Train_EnvstepsSoFar : 3910001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 22539.486046791077
Done logging...



Beginning logging procedure...
Timestep 3920001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 22598.905767
Train_EnvstepsSoFar : 3980001
Train_AverageReturn : 17.4
Train_BestReturn : 17.4
TimeSinceStart : 22736.69212961197
Done logging...



Beginning logging procedure...
Timestep 3990001
mean reward (100 episodes) 17.450000
best mean reward 17.450000
running time 22795.513127
Train_EnvstepsSoFar : 3920001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 22656.647438049316
Done logging...



Beginning logging procedure...
Timestep 3930001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 22716.701498
Train_EnvstepsSoFar : 3920001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 22598.90576696396
Done logging...



Beginning logging procedure...
Timestep 3930001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 22659.125087
Train_EnvstepsSoFar : 3990001
Train_AverageReturn : 17.45
Train_BestReturn : 17.45
TimeSinceStart : 22795.513126850128
Done logging...



Beginning logging procedure...
Timestep 4000001
mean reward (100 episodes) 17.560000
best mean reward 17.560000
running time 22854.911095
Train_EnvstepsSoFar : 3930001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 22659.12508702278
Done logging...



Beginning logging procedure...
Timestep 3940001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 22719.122649
Train_EnvstepsSoFar : 3930001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 22716.701498270035
Done logging...



Beginning logging procedure...
Timestep 3940001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 22777.910169
Train_EnvstepsSoFar : 4000001
Train_AverageReturn : 17.56
Train_BestReturn : 17.56
TimeSinceStart : 22854.911095380783
Done logging...



Beginning logging procedure...
Timestep 4010001
mean reward (100 episodes) 17.590000
best mean reward 17.590000
running time 22914.333912
Train_EnvstepsSoFar : 3940001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 22719.12264919281
Done logging...



Beginning logging procedure...
Timestep 3950001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 22779.643789
Train_EnvstepsSoFar : 3940001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 22777.910168886185
Done logging...



Beginning logging procedure...
Timestep 3950001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 22838.566381
Train_EnvstepsSoFar : 4010001
Train_AverageReturn : 17.59
Train_BestReturn : 17.59
TimeSinceStart : 22914.33391237259
Done logging...



Beginning logging procedure...
Timestep 4020001
mean reward (100 episodes) 17.570000
best mean reward 17.590000
running time 22973.493311
Train_EnvstepsSoFar : 3950001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 22779.643789291382
Done logging...



Beginning logging procedure...
Timestep 3960001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 22839.896030
Train_EnvstepsSoFar : 3950001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 22838.56638097763
Done logging...



Beginning logging procedure...
Timestep 3960001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 22899.121133
Train_EnvstepsSoFar : 4020001
Train_AverageReturn : 17.57
Train_BestReturn : 17.59
TimeSinceStart : 22973.493311166763
Done logging...



Beginning logging procedure...
Timestep 4030001
mean reward (100 episodes) 17.640000
best mean reward 17.640000
running time 23033.007809
Train_EnvstepsSoFar : 3960001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 22839.896030426025
Done logging...



Beginning logging procedure...
Timestep 3970001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 22899.440411
Train_EnvstepsSoFar : 3960001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 22899.121133327484
Done logging...



Beginning logging procedure...
Timestep 3970001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 22959.986700
Train_EnvstepsSoFar : 4030001
Train_AverageReturn : 17.64
Train_BestReturn : 17.64
TimeSinceStart : 23033.007808685303
Done logging...



Beginning logging procedure...
Timestep 4040001
mean reward (100 episodes) 17.670000
best mean reward 17.670000
running time 23092.147938
Train_EnvstepsSoFar : 3970001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 22899.44041109085
Done logging...



Beginning logging procedure...
Timestep 3980001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 22959.842389
Train_EnvstepsSoFar : 3970001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 22959.986700057983
Done logging...



Beginning logging procedure...
Timestep 3980001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 23020.726464
Train_EnvstepsSoFar : 4040001
Train_AverageReturn : 17.67
Train_BestReturn : 17.67
TimeSinceStart : 23092.14793777466
Done logging...



Beginning logging procedure...
Timestep 4050001
mean reward (100 episodes) 17.570000
best mean reward 17.670000
running time 23151.690864
Train_EnvstepsSoFar : 3980001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 22959.842388868332
Done logging...



Beginning logging procedure...
Timestep 3990001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 23019.970751
Train_EnvstepsSoFar : 3980001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 23020.726464271545
Done logging...



Beginning logging procedure...
Timestep 3990001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 23080.964077
Train_EnvstepsSoFar : 4050001
Train_AverageReturn : 17.57
Train_BestReturn : 17.67
TimeSinceStart : 23151.69086432457
Done logging...



Beginning logging procedure...
Timestep 4060001
mean reward (100 episodes) 17.600000
best mean reward 17.670000
running time 23211.729224
Train_EnvstepsSoFar : 3990001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 23019.970750808716
Done logging...



Beginning logging procedure...
Timestep 4000001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 23079.880027
Train_EnvstepsSoFar : 3990001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 23080.964077472687
Done logging...



Beginning logging procedure...
Timestep 4000001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 23140.828048
Train_EnvstepsSoFar : 4060001
Train_AverageReturn : 17.6
Train_BestReturn : 17.67
TimeSinceStart : 23211.72922372818
Done logging...



Beginning logging procedure...
Timestep 4070001
mean reward (100 episodes) 17.610000
best mean reward 17.670000
running time 23271.696828
Train_EnvstepsSoFar : 4000001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 23079.880026578903
Done logging...



Beginning logging procedure...
Timestep 4010001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 23140.068337
Train_EnvstepsSoFar : 4000001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 23140.8280479908
Done logging...



Beginning logging procedure...
Timestep 4010001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 23201.196650
Train_EnvstepsSoFar : 4070001
Train_AverageReturn : 17.61
Train_BestReturn : 17.67
TimeSinceStart : 23271.69682788849
Done logging...



Beginning logging procedure...
Timestep 4080001
mean reward (100 episodes) 17.510000
best mean reward 17.670000
running time 23331.304831
Train_EnvstepsSoFar : 4010001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 23140.068336725235
Done logging...



Beginning logging procedure...
Timestep 4020001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 23200.895210
Train_EnvstepsSoFar : 4010001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 23201.196650266647
Done logging...



Beginning logging procedure...
Timestep 4020001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 23261.566017
Train_EnvstepsSoFar : 4080001
Train_AverageReturn : 17.51
Train_BestReturn : 17.67
TimeSinceStart : 23331.304831027985
Done logging...



Beginning logging procedure...
Timestep 4090001
mean reward (100 episodes) 17.580000
best mean reward 17.670000
running time 23391.170753
Train_EnvstepsSoFar : 4020001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 23200.895210027695
Done logging...



Beginning logging procedure...
Timestep 4030001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 23261.380682
Train_EnvstepsSoFar : 4020001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 23261.56601691246
Done logging...



Beginning logging procedure...
Timestep 4030001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 23322.127459
Train_EnvstepsSoFar : 4090001
Train_AverageReturn : 17.58
Train_BestReturn : 17.67
TimeSinceStart : 23391.170753240585
Done logging...



Beginning logging procedure...
Timestep 4100001
mean reward (100 episodes) 17.520000
best mean reward 17.670000
running time 23450.604038
Train_EnvstepsSoFar : 4030001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 23261.38068151474
Done logging...



Beginning logging procedure...
Timestep 4040001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 23321.880578
Train_EnvstepsSoFar : 4030001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 23322.127459049225
Done logging...



Beginning logging procedure...
Timestep 4040001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 23382.804410
Train_EnvstepsSoFar : 4100001
Train_AverageReturn : 17.52
Train_BestReturn : 17.67
TimeSinceStart : 23450.604038000107
Done logging...



Beginning logging procedure...
Timestep 4110001
mean reward (100 episodes) 17.500000
best mean reward 17.670000
running time 23510.026945
Train_EnvstepsSoFar : 4040001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 23321.88057756424
Done logging...



Beginning logging procedure...
Timestep 4050001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 23382.135402
Train_EnvstepsSoFar : 4040001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 23382.804410219193
Done logging...



Beginning logging procedure...
Timestep 4050001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 23443.186232
Train_EnvstepsSoFar : 4110001
Train_AverageReturn : 17.5
Train_BestReturn : 17.67
TimeSinceStart : 23510.0269446373
Done logging...



Beginning logging procedure...
Timestep 4120001
mean reward (100 episodes) 17.500000
best mean reward 17.670000
running time 23569.338205
Train_EnvstepsSoFar : 4050001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 23382.13540172577
Done logging...



Beginning logging procedure...
Timestep 4060001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 23441.943408
Train_EnvstepsSoFar : 4050001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 23443.186231851578
Done logging...



Beginning logging procedure...
Timestep 4060001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 23503.731601
Train_EnvstepsSoFar : 4120001
Train_AverageReturn : 17.5
Train_BestReturn : 17.67
TimeSinceStart : 23569.33820462227
Done logging...



Beginning logging procedure...
Timestep 4130001
mean reward (100 episodes) 17.530000
best mean reward 17.670000
running time 23628.823223
Train_EnvstepsSoFar : 4060001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 23441.943408489227
Done logging...



Beginning logging procedure...
Timestep 4070001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 23501.899082
Train_EnvstepsSoFar : 4060001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 23503.73160147667
Done logging...



Beginning logging procedure...
Timestep 4070001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 23564.081234
Train_EnvstepsSoFar : 4130001
Train_AverageReturn : 17.53
Train_BestReturn : 17.67
TimeSinceStart : 23628.823222637177
Done logging...



Beginning logging procedure...
Timestep 4140001
mean reward (100 episodes) 17.520000
best mean reward 17.670000
running time 23688.650638
Train_EnvstepsSoFar : 4070001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 23501.899082422256
Done logging...



Beginning logging procedure...
Timestep 4080001
mean reward (100 episodes) -20.620000
best mean reward -20.030000
running time 23562.331208
Train_EnvstepsSoFar : 4070001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 23564.081233501434
Done logging...



Beginning logging procedure...
Timestep 4080001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 23623.694479
Train_EnvstepsSoFar : 4140001
Train_AverageReturn : 17.52
Train_BestReturn : 17.67
TimeSinceStart : 23688.650638103485
Done logging...



Beginning logging procedure...
Timestep 4150001
mean reward (100 episodes) 17.600000
best mean reward 17.670000
running time 23748.112054
Train_EnvstepsSoFar : 4080001
Train_AverageReturn : -20.62
Train_BestReturn : -20.03
TimeSinceStart : 23562.331208229065
Done logging...



Beginning logging procedure...
Timestep 4090001
mean reward (100 episodes) -20.590000
best mean reward -20.030000
running time 23622.949868
Train_EnvstepsSoFar : 4080001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 23623.694479227066
Done logging...



Beginning logging procedure...
Timestep 4090001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 23684.351219
Train_EnvstepsSoFar : 4150001
Train_AverageReturn : 17.6
Train_BestReturn : 17.67
TimeSinceStart : 23748.112053632736
Done logging...



Beginning logging procedure...
Timestep 4160001
mean reward (100 episodes) 17.480000
best mean reward 17.670000
running time 23807.334379
Train_EnvstepsSoFar : 4090001
Train_AverageReturn : -20.59
Train_BestReturn : -20.03
TimeSinceStart : 23622.949867725372
Done logging...



Beginning logging procedure...
Timestep 4100001
mean reward (100 episodes) -20.600000
best mean reward -20.030000
running time 23683.442896
Train_EnvstepsSoFar : 4090001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 23684.35121870041
Done logging...



Beginning logging procedure...
Timestep 4100001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 23745.117471
Train_EnvstepsSoFar : 4160001
Train_AverageReturn : 17.48
Train_BestReturn : 17.67
TimeSinceStart : 23807.33437871933
Done logging...



Beginning logging procedure...
Timestep 4170001
mean reward (100 episodes) 17.490000
best mean reward 17.670000
running time 23866.333055
Train_EnvstepsSoFar : 4100001
Train_AverageReturn : -20.6
Train_BestReturn : -20.03
TimeSinceStart : 23683.442895650864
Done logging...



Beginning logging procedure...
Timestep 4110001
mean reward (100 episodes) -20.590000
best mean reward -20.030000
running time 23743.706965
Train_EnvstepsSoFar : 4100001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 23745.117471456528
Done logging...



Beginning logging procedure...
Timestep 4110001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 23805.786645
Train_EnvstepsSoFar : 4170001
Train_AverageReturn : 17.49
Train_BestReturn : 17.67
TimeSinceStart : 23866.333055257797
Done logging...



Beginning logging procedure...
Timestep 4180001
mean reward (100 episodes) 17.490000
best mean reward 17.670000
running time 23927.397745
Train_EnvstepsSoFar : 4110001
Train_AverageReturn : -20.59
Train_BestReturn : -20.03
TimeSinceStart : 23743.706964731216
Done logging...



Beginning logging procedure...
Timestep 4120001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 23803.950416
Train_EnvstepsSoFar : 4110001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 23805.786645174026
Done logging...



Beginning logging procedure...
Timestep 4120001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 23866.319496
Train_EnvstepsSoFar : 4180001
Train_AverageReturn : 17.49
Train_BestReturn : 17.67
TimeSinceStart : 23927.397745132446
Done logging...



Beginning logging procedure...
Timestep 4190001
mean reward (100 episodes) 17.460000
best mean reward 17.670000
running time 23986.699176
Train_EnvstepsSoFar : 4120001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 23803.950415611267
Done logging...



Beginning logging procedure...
Timestep 4130001
mean reward (100 episodes) -20.640000
best mean reward -20.030000
running time 23864.240540
Train_EnvstepsSoFar : 4120001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 23866.319496393204
Done logging...



Beginning logging procedure...
Timestep 4130001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 23926.920521
Train_EnvstepsSoFar : 4190001
Train_AverageReturn : 17.46
Train_BestReturn : 17.67
TimeSinceStart : 23986.699176311493
Done logging...



Beginning logging procedure...
Timestep 4200001
mean reward (100 episodes) 17.370000
best mean reward 17.670000
running time 24046.379984
Train_EnvstepsSoFar : 4130001
Train_AverageReturn : -20.64
Train_BestReturn : -20.03
TimeSinceStart : 23864.24054002762
Done logging...



Beginning logging procedure...
Timestep 4140001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 23924.766114
Train_EnvstepsSoFar : 4130001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 23926.92052078247
Done logging...



Beginning logging procedure...
Timestep 4140001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 23987.827472
Train_EnvstepsSoFar : 4200001
Train_AverageReturn : 17.37
Train_BestReturn : 17.67
TimeSinceStart : 24046.379984140396
Done logging...



Beginning logging procedure...
Timestep 4210001
mean reward (100 episodes) 17.310000
best mean reward 17.670000
running time 24106.219321
Train_EnvstepsSoFar : 4140001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 23924.76611351967
Done logging...



Beginning logging procedure...
Timestep 4150001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 23984.620813
Train_EnvstepsSoFar : 4140001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 23987.827471971512
Done logging...



Beginning logging procedure...
Timestep 4150001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 24048.700487
Train_EnvstepsSoFar : 4210001
Train_AverageReturn : 17.31
Train_BestReturn : 17.67
TimeSinceStart : 24106.219321250916
Done logging...



Beginning logging procedure...
Timestep 4220001
mean reward (100 episodes) 17.270000
best mean reward 17.670000
running time 24166.025122
Train_EnvstepsSoFar : 4150001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 23984.620812892914
Done logging...



Beginning logging procedure...
Timestep 4160001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 24044.343284
Train_EnvstepsSoFar : 4150001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 24048.70048713684
Done logging...



Beginning logging procedure...
Timestep 4160001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 24108.847508
Train_EnvstepsSoFar : 4220001
Train_AverageReturn : 17.27
Train_BestReturn : 17.67
TimeSinceStart : 24166.02512192726
Done logging...



Beginning logging procedure...
Timestep 4230001
mean reward (100 episodes) 17.270000
best mean reward 17.670000
running time 24225.897811
Train_EnvstepsSoFar : 4160001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 24044.343283891678
Done logging...



Beginning logging procedure...
Timestep 4170001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 24104.664986
Train_EnvstepsSoFar : 4160001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 24108.84750843048
Done logging...



Beginning logging procedure...
Timestep 4170001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 24168.862205
Train_EnvstepsSoFar : 4230001
Train_AverageReturn : 17.27
Train_BestReturn : 17.67
TimeSinceStart : 24225.897810935974
Done logging...



Beginning logging procedure...
Timestep 4240001
mean reward (100 episodes) 17.240000
best mean reward 17.670000
running time 24285.519889
Train_EnvstepsSoFar : 4170001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 24104.66498565674
Done logging...



Beginning logging procedure...
Timestep 4180001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 24164.772964
Train_EnvstepsSoFar : 4170001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 24168.862205266953
Done logging...



Beginning logging procedure...
Timestep 4180001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 24229.123831
Train_EnvstepsSoFar : 4240001
Train_AverageReturn : 17.24
Train_BestReturn : 17.67
TimeSinceStart : 24285.51988863945
Done logging...



Beginning logging procedure...
Timestep 4250001
mean reward (100 episodes) 17.260000
best mean reward 17.670000
running time 24345.251789
Train_EnvstepsSoFar : 4180001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 24164.77296447754
Done logging...



Beginning logging procedure...
Timestep 4190001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 24225.047335
Train_EnvstepsSoFar : 4180001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 24229.123830795288
Done logging...



Beginning logging procedure...
Timestep 4190001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 24289.611823
Train_EnvstepsSoFar : 4250001
Train_AverageReturn : 17.26
Train_BestReturn : 17.67
TimeSinceStart : 24345.25178861618
Done logging...



Beginning logging procedure...
Timestep 4260001
mean reward (100 episodes) 17.290000
best mean reward 17.670000
running time 24405.460053
Train_EnvstepsSoFar : 4190001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 24225.047335147858
Done logging...



Beginning logging procedure...
Timestep 4200001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 24284.992904
Train_EnvstepsSoFar : 4190001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 24289.61182332039
Done logging...



Beginning logging procedure...
Timestep 4200001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 24349.948945
Train_EnvstepsSoFar : 4260001
Train_AverageReturn : 17.29
Train_BestReturn : 17.67
TimeSinceStart : 24405.46005320549
Done logging...



Beginning logging procedure...
Timestep 4270001
mean reward (100 episodes) 17.360000
best mean reward 17.670000
running time 24465.198184
Train_EnvstepsSoFar : 4200001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 24284.992904424667
Done logging...



Beginning logging procedure...
Timestep 4210001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 24345.705351
Train_EnvstepsSoFar : 4200001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 24349.94894504547
Done logging...



Beginning logging procedure...
Timestep 4210001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 24409.815838
Train_EnvstepsSoFar : 4270001
Train_AverageReturn : 17.36
Train_BestReturn : 17.67
TimeSinceStart : 24465.198183774948
Done logging...



Beginning logging procedure...
Timestep 4280001
mean reward (100 episodes) 17.370000
best mean reward 17.670000
running time 24525.220026
Train_EnvstepsSoFar : 4210001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 24345.705350875854
Done logging...



Beginning logging procedure...
Timestep 4220001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 24406.002672
Train_EnvstepsSoFar : 4210001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 24409.815838336945
Done logging...



Beginning logging procedure...
Timestep 4220001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 24470.406329
Train_EnvstepsSoFar : 4280001
Train_AverageReturn : 17.37
Train_BestReturn : 17.67
TimeSinceStart : 24525.220026254654
Done logging...



Beginning logging procedure...
Timestep 4290001
mean reward (100 episodes) 17.400000
best mean reward 17.670000
running time 24584.944347
Train_EnvstepsSoFar : 4220001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 24406.002671957016
Done logging...



Beginning logging procedure...
Timestep 4230001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 24466.591714
Train_EnvstepsSoFar : 4220001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 24470.40632891655
Done logging...



Beginning logging procedure...
Timestep 4230001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 24530.815617
Train_EnvstepsSoFar : 4290001
Train_AverageReturn : 17.4
Train_BestReturn : 17.67
TimeSinceStart : 24584.94434738159
Done logging...



Beginning logging procedure...
Timestep 4300001
mean reward (100 episodes) 17.480000
best mean reward 17.670000
running time 24644.742151
Train_EnvstepsSoFar : 4230001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 24466.591713905334
Done logging...



Beginning logging procedure...
Timestep 4240001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 24527.454134
Train_EnvstepsSoFar : 4230001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 24530.815616846085
Done logging...



Beginning logging procedure...
Timestep 4240001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 24591.457554
Train_EnvstepsSoFar : 4300001
Train_AverageReturn : 17.48
Train_BestReturn : 17.67
TimeSinceStart : 24644.74215078354
Done logging...



Beginning logging procedure...
Timestep 4310001
mean reward (100 episodes) 17.560000
best mean reward 17.670000
running time 24704.558734
Train_EnvstepsSoFar : 4240001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 24527.45413351059
Done logging...



Beginning logging procedure...
Timestep 4250001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 24587.543559
Train_EnvstepsSoFar : 4240001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 24591.457553863525
Done logging...



Beginning logging procedure...
Timestep 4250001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 24652.150189
Train_EnvstepsSoFar : 4310001
Train_AverageReturn : 17.56
Train_BestReturn : 17.67
TimeSinceStart : 24704.558733940125
Done logging...



Beginning logging procedure...
Timestep 4320001
mean reward (100 episodes) 17.660000
best mean reward 17.670000
running time 24764.515175
Train_EnvstepsSoFar : 4250001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 24587.54355931282
Done logging...



Beginning logging procedure...
Timestep 4260001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 24648.116732
Train_EnvstepsSoFar : 4250001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 24652.150188684464
Done logging...



Beginning logging procedure...
Timestep 4260001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 24712.360119
Train_EnvstepsSoFar : 4320001
Train_AverageReturn : 17.66
Train_BestReturn : 17.67
TimeSinceStart : 24764.515174865723
Done logging...



Beginning logging procedure...
Timestep 4330001
mean reward (100 episodes) 17.720000
best mean reward 17.720000
running time 24824.333738
Train_EnvstepsSoFar : 4260001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 24648.116731882095
Done logging...



Beginning logging procedure...
Timestep 4270001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 24709.286442
Train_EnvstepsSoFar : 4260001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 24712.360118865967
Done logging...



Beginning logging procedure...
Timestep 4270001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 24772.826725
Train_EnvstepsSoFar : 4330001
Train_AverageReturn : 17.72
Train_BestReturn : 17.72
TimeSinceStart : 24824.333738088608
Done logging...



Beginning logging procedure...
Timestep 4340001
mean reward (100 episodes) 17.710000
best mean reward 17.720000
running time 24883.777156
Train_EnvstepsSoFar : 4270001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 24709.286442041397
Done logging...



Beginning logging procedure...
Timestep 4280001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 24770.267725
Train_EnvstepsSoFar : 4270001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 24772.826724529266
Done logging...



Beginning logging procedure...
Timestep 4280001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 24832.543626
Train_EnvstepsSoFar : 4340001
Train_AverageReturn : 17.71
Train_BestReturn : 17.72
TimeSinceStart : 24883.777156352997
Done logging...



Beginning logging procedure...
Timestep 4350001
mean reward (100 episodes) 17.560000
best mean reward 17.720000
running time 24943.333351
Train_EnvstepsSoFar : 4280001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 24770.267724990845
Done logging...



Beginning logging procedure...
Timestep 4290001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 24830.624382
Train_EnvstepsSoFar : 4280001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 24832.543625593185
Done logging...



Beginning logging procedure...
Timestep 4290001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 24893.159602
Train_EnvstepsSoFar : 4350001
Train_AverageReturn : 17.56
Train_BestReturn : 17.72
TimeSinceStart : 24943.333350896835
Done logging...



Beginning logging procedure...
Timestep 4360001
mean reward (100 episodes) 17.590000
best mean reward 17.720000
running time 25002.791621
Train_EnvstepsSoFar : 4290001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 24830.624382019043
Done logging...



Beginning logging procedure...
Timestep 4300001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 24891.735875
Train_EnvstepsSoFar : 4290001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 24893.159601688385
Done logging...



Beginning logging procedure...
Timestep 4300001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 24953.778285
Train_EnvstepsSoFar : 4360001
Train_AverageReturn : 17.59
Train_BestReturn : 17.72
TimeSinceStart : 25002.79162144661
Done logging...



Beginning logging procedure...
Timestep 4370001
mean reward (100 episodes) 17.600000
best mean reward 17.720000
running time 25062.446194
Train_EnvstepsSoFar : 4300001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 24891.7358751297
Done logging...



Beginning logging procedure...
Timestep 4310001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 24951.952803
Train_EnvstepsSoFar : 4300001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 24953.778284549713
Done logging...



Beginning logging procedure...
Timestep 4310001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 25014.306849
Train_EnvstepsSoFar : 4370001
Train_AverageReturn : 17.6
Train_BestReturn : 17.72
TimeSinceStart : 25062.446194410324
Done logging...



Beginning logging procedure...
Timestep 4380001
mean reward (100 episodes) 17.630000
best mean reward 17.720000
running time 25121.769866
Train_EnvstepsSoFar : 4310001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 24951.952803373337
Done logging...



Beginning logging procedure...
Timestep 4320001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 25012.014608
Train_EnvstepsSoFar : 4310001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 25014.30684876442
Done logging...



Beginning logging procedure...
Timestep 4320001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 25075.180076
Train_EnvstepsSoFar : 4380001
Train_AverageReturn : 17.63
Train_BestReturn : 17.72
TimeSinceStart : 25121.769865989685
Done logging...



Beginning logging procedure...
Timestep 4390001
mean reward (100 episodes) 17.690000
best mean reward 17.720000
running time 25181.556578
Train_EnvstepsSoFar : 4320001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 25012.01460790634
Done logging...



Beginning logging procedure...
Timestep 4330001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 25071.654411
Train_EnvstepsSoFar : 4390001
Train_AverageReturn : 17.69
Train_BestReturn : 17.72
TimeSinceStart : 25181.55657839775
Done logging...



Beginning logging procedure...
Timestep 4400001
mean reward (100 episodes) 17.730000
best mean reward 17.730000
running time 25241.169250
Train_EnvstepsSoFar : 4320001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 25075.180075645447
Done logging...



Beginning logging procedure...
Timestep 4330001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 25136.428999
Train_EnvstepsSoFar : 4330001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 25071.65441083908
Done logging...



Beginning logging procedure...
Timestep 4340001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 25131.769230
Train_EnvstepsSoFar : 4400001
Train_AverageReturn : 17.73
Train_BestReturn : 17.73
TimeSinceStart : 25241.169249773026
Done logging...



Beginning logging procedure...
Timestep 4410001
mean reward (100 episodes) 17.820000
best mean reward 17.820000
running time 25300.655253
Train_EnvstepsSoFar : 4330001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 25136.428998708725
Done logging...



Beginning logging procedure...
Timestep 4340001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 25196.881496
Train_EnvstepsSoFar : 4340001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 25131.769230127335
Done logging...



Beginning logging procedure...
Timestep 4350001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 25191.614203
Train_EnvstepsSoFar : 4410001
Train_AverageReturn : 17.82
Train_BestReturn : 17.82
TimeSinceStart : 25300.65525341034
Done logging...



Beginning logging procedure...
Timestep 4420001
mean reward (100 episodes) 17.920000
best mean reward 17.920000
running time 25360.288114
Train_EnvstepsSoFar : 4340001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 25196.881495952606
Done logging...



Beginning logging procedure...
Timestep 4350001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 25258.297653
Train_EnvstepsSoFar : 4350001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 25191.61420273781
Done logging...



Beginning logging procedure...
Timestep 4360001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 25251.618432
Train_EnvstepsSoFar : 4420001
Train_AverageReturn : 17.92
Train_BestReturn : 17.92
TimeSinceStart : 25360.288114070892
Done logging...



Beginning logging procedure...
Timestep 4430001
mean reward (100 episodes) 18.010000
best mean reward 18.010000
running time 25419.635194
Train_EnvstepsSoFar : 4350001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 25258.297653198242
Done logging...



Beginning logging procedure...
Timestep 4360001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 25319.683586
Train_EnvstepsSoFar : 4360001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 25251.618431568146
Done logging...



Beginning logging procedure...
Timestep 4370001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 25312.097437
Train_EnvstepsSoFar : 4430001
Train_AverageReturn : 18.01
Train_BestReturn : 18.01
TimeSinceStart : 25419.63519358635
Done logging...



Beginning logging procedure...
Timestep 4440001
mean reward (100 episodes) 17.940000
best mean reward 18.010000
running time 25478.962095
Train_EnvstepsSoFar : 4360001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 25319.683585882187
Done logging...



Beginning logging procedure...
Timestep 4370001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 25380.187464
Train_EnvstepsSoFar : 4370001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 25312.097436904907
Done logging...



Beginning logging procedure...
Timestep 4380001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 25372.288366
Train_EnvstepsSoFar : 4440001
Train_AverageReturn : 17.94
Train_BestReturn : 18.01
TimeSinceStart : 25478.962094783783
Done logging...



Beginning logging procedure...
Timestep 4450001
mean reward (100 episodes) 17.980000
best mean reward 18.010000
running time 25538.566675
Train_EnvstepsSoFar : 4370001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 25380.187463760376
Done logging...



Beginning logging procedure...
Timestep 4380001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 25440.705895
Train_EnvstepsSoFar : 4380001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 25372.28836631775
Done logging...



Beginning logging procedure...
Timestep 4390001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 25432.380494
Train_EnvstepsSoFar : 4450001
Train_AverageReturn : 17.98
Train_BestReturn : 18.01
TimeSinceStart : 25538.56667470932
Done logging...



Beginning logging procedure...
Timestep 4460001
mean reward (100 episodes) 17.940000
best mean reward 18.010000
running time 25598.090337
Train_EnvstepsSoFar : 4380001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 25440.705894947052
Done logging...



Beginning logging procedure...
Timestep 4390001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 25501.351284
Train_EnvstepsSoFar : 4390001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 25432.380494356155
Done logging...



Beginning logging procedure...
Timestep 4400001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 25492.359751
Train_EnvstepsSoFar : 4460001
Train_AverageReturn : 17.94
Train_BestReturn : 18.01
TimeSinceStart : 25598.09033679962
Done logging...



Beginning logging procedure...
Timestep 4470001
mean reward (100 episodes) 18.010000
best mean reward 18.010000
running time 25657.867484
Train_EnvstepsSoFar : 4390001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 25501.351283550262
Done logging...



Beginning logging procedure...
Timestep 4400001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 25562.071511
Train_EnvstepsSoFar : 4400001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 25492.359751462936
Done logging...



Beginning logging procedure...
Timestep 4410001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 25552.254313
Train_EnvstepsSoFar : 4470001
Train_AverageReturn : 18.01
Train_BestReturn : 18.01
TimeSinceStart : 25657.867483615875
Done logging...



Beginning logging procedure...
Timestep 4480001
mean reward (100 episodes) 18.020000
best mean reward 18.020000
running time 25718.210140
Train_EnvstepsSoFar : 4400001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 25562.071511030197
Done logging...



Beginning logging procedure...
Timestep 4410001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 25622.741727
Train_EnvstepsSoFar : 4410001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 25552.25431251526
Done logging...



Beginning logging procedure...
Timestep 4420001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 25611.822144
Train_EnvstepsSoFar : 4480001
Train_AverageReturn : 18.02
Train_BestReturn : 18.02
TimeSinceStart : 25718.210139513016
Done logging...



Beginning logging procedure...
Timestep 4490001
mean reward (100 episodes) 18.070000
best mean reward 18.070000
running time 25778.548210
Train_EnvstepsSoFar : 4410001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 25622.741726636887
Done logging...



Beginning logging procedure...
Timestep 4420001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 25682.707540
Train_EnvstepsSoFar : 4420001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 25611.822144031525
Done logging...



Beginning logging procedure...
Timestep 4430001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 25671.322469
Train_EnvstepsSoFar : 4490001
Train_AverageReturn : 18.07
Train_BestReturn : 18.07
TimeSinceStart : 25778.54821038246
Done logging...



Beginning logging procedure...
Timestep 4500001
mean reward (100 episodes) 18.100000
best mean reward 18.100000
running time 25838.882491
Train_EnvstepsSoFar : 4420001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 25682.707540273666
Done logging...



Beginning logging procedure...
Timestep 4430001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 25743.154851
Train_EnvstepsSoFar : 4430001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 25671.322469234467
Done logging...



Beginning logging procedure...
Timestep 4440001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 25731.371350
Train_EnvstepsSoFar : 4500001
Train_AverageReturn : 18.1
Train_BestReturn : 18.1
TimeSinceStart : 25838.882491111755
Done logging...



Beginning logging procedure...
Timestep 4510001
mean reward (100 episodes) 18.030000
best mean reward 18.100000
running time 25898.963931
Train_EnvstepsSoFar : 4430001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 25743.154851436615
Done logging...



Beginning logging procedure...
Timestep 4440001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 25803.763696
Train_EnvstepsSoFar : 4440001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 25731.371349811554
Done logging...



Beginning logging procedure...
Timestep 4450001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 25792.138438
Train_EnvstepsSoFar : 4510001
Train_AverageReturn : 18.03
Train_BestReturn : 18.1
TimeSinceStart : 25898.963930606842
Done logging...



Beginning logging procedure...
Timestep 4520001
mean reward (100 episodes) 18.030000
best mean reward 18.100000
running time 25958.404852
Train_EnvstepsSoFar : 4440001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 25803.763696432114
Done logging...



Beginning logging procedure...
Timestep 4450001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 25863.906199
Train_EnvstepsSoFar : 4450001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 25792.138437747955
Done logging...



Beginning logging procedure...
Timestep 4460001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 25852.041145
Train_EnvstepsSoFar : 4520001
Train_AverageReturn : 18.03
Train_BestReturn : 18.1
TimeSinceStart : 25958.404851675034
Done logging...



Beginning logging procedure...
Timestep 4530001
mean reward (100 episodes) 18.110000
best mean reward 18.110000
running time 26018.438990
Train_EnvstepsSoFar : 4450001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 25863.906198978424
Done logging...



Beginning logging procedure...
Timestep 4460001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 25924.829126
Train_EnvstepsSoFar : 4460001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 25852.04114484787
Done logging...



Beginning logging procedure...
Timestep 4470001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 25912.448772
Train_EnvstepsSoFar : 4530001
Train_AverageReturn : 18.11
Train_BestReturn : 18.11
TimeSinceStart : 26018.4389898777
Done logging...



Beginning logging procedure...
Timestep 4540001
mean reward (100 episodes) 18.210000
best mean reward 18.210000
running time 26078.363912
Train_EnvstepsSoFar : 4460001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 25924.829125881195
Done logging...



Beginning logging procedure...
Timestep 4470001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 25985.429211
Train_EnvstepsSoFar : 4470001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 25912.448771715164
Done logging...



Beginning logging procedure...
Timestep 4480001
mean reward (100 episodes) -20.750000
best mean reward -20.030000
running time 25972.597366
Train_EnvstepsSoFar : 4540001
Train_AverageReturn : 18.21
Train_BestReturn : 18.21
TimeSinceStart : 26078.36391210556
Done logging...



Beginning logging procedure...
Timestep 4550001
mean reward (100 episodes) 18.330000
best mean reward 18.330000
running time 26138.457084
Train_EnvstepsSoFar : 4470001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 25985.429211378098
Done logging...



Beginning logging procedure...
Timestep 4480001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 26045.309523
Train_EnvstepsSoFar : 4480001
Train_AverageReturn : -20.75
Train_BestReturn : -20.03
TimeSinceStart : 25972.59736585617
Done logging...



Beginning logging procedure...
Timestep 4490001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 26032.647294
Train_EnvstepsSoFar : 4550001
Train_AverageReturn : 18.33
Train_BestReturn : 18.33
TimeSinceStart : 26138.457083940506
Done logging...



Beginning logging procedure...
Timestep 4560001
mean reward (100 episodes) 18.270000
best mean reward 18.330000
running time 26198.434856
Train_EnvstepsSoFar : 4480001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 26045.309522628784
Done logging...



Beginning logging procedure...
Timestep 4490001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 26105.498686
Train_EnvstepsSoFar : 4560001
Train_AverageReturn : 18.27
Train_BestReturn : 18.33
TimeSinceStart : 26198.434855937958
Done logging...



Beginning logging procedure...
Timestep 4570001
mean reward (100 episodes) 18.250000
best mean reward 18.330000
running time 26258.176810
Train_EnvstepsSoFar : 4490001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 26032.647294282913
Done logging...



Beginning logging procedure...
Timestep 4500001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 26093.966744
Train_EnvstepsSoFar : 4490001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 26105.498685598373
Done logging...



Beginning logging procedure...
Timestep 4500001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 26166.022048
Train_EnvstepsSoFar : 4570001
Train_AverageReturn : 18.25
Train_BestReturn : 18.33
TimeSinceStart : 26258.176810264587
Done logging...



Beginning logging procedure...
Timestep 4580001
mean reward (100 episodes) 18.180000
best mean reward 18.330000
running time 26317.797545
Train_EnvstepsSoFar : 4500001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 26093.966743946075
Done logging...



Beginning logging procedure...
Timestep 4510001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 26154.844960
Train_EnvstepsSoFar : 4500001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 26166.02204799652
Done logging...



Beginning logging procedure...
Timestep 4510001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 26226.263311
Train_EnvstepsSoFar : 4580001
Train_AverageReturn : 18.18
Train_BestReturn : 18.33
TimeSinceStart : 26317.797545433044
Done logging...



Beginning logging procedure...
Timestep 4590001
mean reward (100 episodes) 18.200000
best mean reward 18.330000
running time 26377.116058
Train_EnvstepsSoFar : 4510001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 26154.844960212708
Done logging...



Beginning logging procedure...
Timestep 4520001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 26214.766374
Train_EnvstepsSoFar : 4510001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 26226.26331138611
Done logging...



Beginning logging procedure...
Timestep 4520001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 26287.390393
Train_EnvstepsSoFar : 4590001
Train_AverageReturn : 18.2
Train_BestReturn : 18.33
TimeSinceStart : 26377.11605834961
Done logging...



Beginning logging procedure...
Timestep 4600001
mean reward (100 episodes) 18.200000
best mean reward 18.330000
running time 26436.868663
Train_EnvstepsSoFar : 4520001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 26214.766374111176
Done logging...



Beginning logging procedure...
Timestep 4530001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 26275.300823
Train_EnvstepsSoFar : 4520001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 26287.39039325714
Done logging...



Beginning logging procedure...
Timestep 4530001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 26348.308801
Train_EnvstepsSoFar : 4600001
Train_AverageReturn : 18.2
Train_BestReturn : 18.33
TimeSinceStart : 26436.868663072586
Done logging...



Beginning logging procedure...
Timestep 4610001
mean reward (100 episodes) 18.150000
best mean reward 18.330000
running time 26496.683671
Train_EnvstepsSoFar : 4530001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 26275.30082345009
Done logging...



Beginning logging procedure...
Timestep 4540001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 26336.037968
Train_EnvstepsSoFar : 4530001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 26348.308800935745
Done logging...



Beginning logging procedure...
Timestep 4540001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 26408.287329
Train_EnvstepsSoFar : 4610001
Train_AverageReturn : 18.15
Train_BestReturn : 18.33
TimeSinceStart : 26496.6836707592
Done logging...



Beginning logging procedure...
Timestep 4620001
mean reward (100 episodes) 18.190000
best mean reward 18.330000
running time 26556.372337
Train_EnvstepsSoFar : 4540001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 26336.037968158722
Done logging...



Beginning logging procedure...
Timestep 4550001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 26395.443595
Train_EnvstepsSoFar : 4540001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 26408.28732919693
Done logging...



Beginning logging procedure...
Timestep 4550001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 26469.034898
Train_EnvstepsSoFar : 4620001
Train_AverageReturn : 18.19
Train_BestReturn : 18.33
TimeSinceStart : 26556.37233710289
Done logging...



Beginning logging procedure...
Timestep 4630001
mean reward (100 episodes) 18.190000
best mean reward 18.330000
running time 26616.221102
Train_EnvstepsSoFar : 4550001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 26395.443595170975
Done logging...



Beginning logging procedure...
Timestep 4560001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 26455.834382
Train_EnvstepsSoFar : 4550001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 26469.03489756584
Done logging...



Beginning logging procedure...
Timestep 4560001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 26528.812860
Train_EnvstepsSoFar : 4630001
Train_AverageReturn : 18.19
Train_BestReturn : 18.33
TimeSinceStart : 26616.22110247612
Done logging...



Beginning logging procedure...
Timestep 4640001
mean reward (100 episodes) 18.250000
best mean reward 18.330000
running time 26676.571583
Train_EnvstepsSoFar : 4560001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 26455.83438229561
Done logging...



Beginning logging procedure...
Timestep 4570001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 26516.236817
Train_EnvstepsSoFar : 4560001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 26528.81286048889
Done logging...



Beginning logging procedure...
Timestep 4570001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 26589.838089
Train_EnvstepsSoFar : 4640001
Train_AverageReturn : 18.25
Train_BestReturn : 18.33
TimeSinceStart : 26676.571583032608
Done logging...



Beginning logging procedure...
Timestep 4650001
mean reward (100 episodes) 18.330000
best mean reward 18.330000
running time 26735.455885
Train_EnvstepsSoFar : 4570001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 26516.23681664467
Done logging...



Beginning logging procedure...
Timestep 4580001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 26577.271513
Train_EnvstepsSoFar : 4570001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 26589.838089227676
Done logging...



Beginning logging procedure...
Timestep 4580001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 26651.293571
Train_EnvstepsSoFar : 4650001
Train_AverageReturn : 18.33
Train_BestReturn : 18.33
TimeSinceStart : 26735.45588541031
Done logging...



Beginning logging procedure...
Timestep 4660001
mean reward (100 episodes) 18.340000
best mean reward 18.340000
running time 26794.873020
Train_EnvstepsSoFar : 4580001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 26577.271512508392
Done logging...



Beginning logging procedure...
Timestep 4590001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 26637.139932
Train_EnvstepsSoFar : 4580001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 26651.293570756912
Done logging...



Beginning logging procedure...
Timestep 4590001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 26712.240227
Train_EnvstepsSoFar : 4660001
Train_AverageReturn : 18.34
Train_BestReturn : 18.34
TimeSinceStart : 26794.873019695282
Done logging...



Beginning logging procedure...
Timestep 4670001
mean reward (100 episodes) 18.350000
best mean reward 18.350000
running time 26854.300352
Train_EnvstepsSoFar : 4590001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 26637.139932394028
Done logging...



Beginning logging procedure...
Timestep 4600001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 26695.813254
Train_EnvstepsSoFar : 4590001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 26712.24022746086
Done logging...



Beginning logging procedure...
Timestep 4600001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 26772.988448
Train_EnvstepsSoFar : 4670001
Train_AverageReturn : 18.35
Train_BestReturn : 18.35
TimeSinceStart : 26854.30035161972
Done logging...



Beginning logging procedure...
Timestep 4680001
mean reward (100 episodes) 18.300000
best mean reward 18.350000
running time 26913.638398
Train_EnvstepsSoFar : 4600001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 26695.81325364113
Done logging...



Beginning logging procedure...
Timestep 4610001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 26755.361291
Train_EnvstepsSoFar : 4600001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 26772.988448143005
Done logging...



Beginning logging procedure...
Timestep 4610001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 26833.655423
Train_EnvstepsSoFar : 4680001
Train_AverageReturn : 18.3
Train_BestReturn : 18.35
TimeSinceStart : 26913.63839840889
Done logging...



Beginning logging procedure...
Timestep 4690001
mean reward (100 episodes) 18.210000
best mean reward 18.350000
running time 26972.481206
Train_EnvstepsSoFar : 4610001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 26755.361290693283
Done logging...



Beginning logging procedure...
Timestep 4620001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 26815.495712
Train_EnvstepsSoFar : 4610001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 26833.655423164368
Done logging...



Beginning logging procedure...
Timestep 4620001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 26894.533015
Train_EnvstepsSoFar : 4690001
Train_AverageReturn : 18.21
Train_BestReturn : 18.35
TimeSinceStart : 26972.481205701828
Done logging...



Beginning logging procedure...
Timestep 4700001
mean reward (100 episodes) 18.190000
best mean reward 18.350000
running time 27031.725639
Train_EnvstepsSoFar : 4620001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 26815.495712041855
Done logging...



Beginning logging procedure...
Timestep 4630001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 26874.873966
Train_EnvstepsSoFar : 4620001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 26894.53301525116
Done logging...



Beginning logging procedure...
Timestep 4630001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 26955.337689
Train_EnvstepsSoFar : 4700001
Train_AverageReturn : 18.19
Train_BestReturn : 18.35
TimeSinceStart : 27031.725639104843
Done logging...



Beginning logging procedure...
Timestep 4710001
mean reward (100 episodes) 18.260000
best mean reward 18.350000
running time 27091.095785
Train_EnvstepsSoFar : 4630001
Train_AverageReturn : -20.67
Train_BestReturn : -20.03
TimeSinceStart : 26874.873965740204
Done logging...



Beginning logging procedure...
Timestep 4640001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 26935.473199
Train_EnvstepsSoFar : 4630001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 26955.33768939972
Done logging...



Beginning logging procedure...
Timestep 4640001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 27015.908013
Train_EnvstepsSoFar : 4710001
Train_AverageReturn : 18.26
Train_BestReturn : 18.35
TimeSinceStart : 27091.09578514099
Done logging...



Beginning logging procedure...
Timestep 4720001
mean reward (100 episodes) 18.230000
best mean reward 18.350000
running time 27150.161933
Train_EnvstepsSoFar : 4640001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 26935.473199129105
Done logging...



Beginning logging procedure...
Timestep 4650001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 26995.808263
Train_EnvstepsSoFar : 4640001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 27015.90801334381
Done logging...



Beginning logging procedure...
Timestep 4650001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 27076.111280
Train_EnvstepsSoFar : 4720001
Train_AverageReturn : 18.23
Train_BestReturn : 18.35
TimeSinceStart : 27150.16193318367
Done logging...



Beginning logging procedure...
Timestep 4730001
mean reward (100 episodes) 18.300000
best mean reward 18.350000
running time 27209.717898
Train_EnvstepsSoFar : 4650001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 26995.80826330185
Done logging...



Beginning logging procedure...
Timestep 4660001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 27056.255523
Train_EnvstepsSoFar : 4650001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 27076.111280202866
Done logging...



Beginning logging procedure...
Timestep 4660001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 27136.645462
Train_EnvstepsSoFar : 4730001
Train_AverageReturn : 18.3
Train_BestReturn : 18.35
TimeSinceStart : 27209.717898368835
Done logging...



Beginning logging procedure...
Timestep 4740001
mean reward (100 episodes) 18.160000
best mean reward 18.350000
running time 27269.332443
Train_EnvstepsSoFar : 4660001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 27056.255522966385
Done logging...



Beginning logging procedure...
Timestep 4670001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 27116.667971
Train_EnvstepsSoFar : 4660001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 27136.645461559296
Done logging...



Beginning logging procedure...
Timestep 4670001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 27197.401341
Train_EnvstepsSoFar : 4740001
Train_AverageReturn : 18.16
Train_BestReturn : 18.35
TimeSinceStart : 27269.332442760468
Done logging...



Beginning logging procedure...
Timestep 4750001
mean reward (100 episodes) 18.140000
best mean reward 18.350000
running time 27328.977322
Train_EnvstepsSoFar : 4670001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 27116.667971372604
Done logging...



Beginning logging procedure...
Timestep 4680001
mean reward (100 episodes) -20.610000
best mean reward -20.030000
running time 27177.156533
Train_EnvstepsSoFar : 4670001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 27197.401341438293
Done logging...



Beginning logging procedure...
Timestep 4680001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 27258.257890
Train_EnvstepsSoFar : 4750001
Train_AverageReturn : 18.14
Train_BestReturn : 18.35
TimeSinceStart : 27328.977321863174
Done logging...



Beginning logging procedure...
Timestep 4760001
mean reward (100 episodes) 18.200000
best mean reward 18.350000
running time 27388.327169
Train_EnvstepsSoFar : 4680001
Train_AverageReturn : -20.61
Train_BestReturn : -20.03
TimeSinceStart : 27177.156533241272
Done logging...



Beginning logging procedure...
Timestep 4690001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 27237.240041
Train_EnvstepsSoFar : 4680001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 27258.257890462875
Done logging...



Beginning logging procedure...
Timestep 4690001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 27319.555041
Train_EnvstepsSoFar : 4760001
Train_AverageReturn : 18.2
Train_BestReturn : 18.35
TimeSinceStart : 27388.327168941498
Done logging...



Beginning logging procedure...
Timestep 4770001
mean reward (100 episodes) 18.170000
best mean reward 18.350000
running time 27448.041924
Train_EnvstepsSoFar : 4690001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 27237.240040779114
Done logging...



Beginning logging procedure...
Timestep 4700001
mean reward (100 episodes) -20.630000
best mean reward -20.030000
running time 27297.678319
Train_EnvstepsSoFar : 4690001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 27319.555040836334
Done logging...



Beginning logging procedure...
Timestep 4700001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 27380.561343
Train_EnvstepsSoFar : 4770001
Train_AverageReturn : 18.17
Train_BestReturn : 18.35
TimeSinceStart : 27448.041924238205
Done logging...



Beginning logging procedure...
Timestep 4780001
mean reward (100 episodes) 18.220000
best mean reward 18.350000
running time 27507.729821
Train_EnvstepsSoFar : 4700001
Train_AverageReturn : -20.63
Train_BestReturn : -20.03
TimeSinceStart : 27297.67831850052
Done logging...



Beginning logging procedure...
Timestep 4710001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 27357.388840
Train_EnvstepsSoFar : 4700001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 27380.561342716217
Done logging...



Beginning logging procedure...
Timestep 4710001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 27440.680504
Train_EnvstepsSoFar : 4780001
Train_AverageReturn : 18.22
Train_BestReturn : 18.35
TimeSinceStart : 27507.729820728302
Done logging...



Beginning logging procedure...
Timestep 4790001
mean reward (100 episodes) 18.230000
best mean reward 18.350000
running time 27567.289910
Train_EnvstepsSoFar : 4710001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 27357.38883972168
Done logging...



Beginning logging procedure...
Timestep 4720001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 27418.351520
Train_EnvstepsSoFar : 4710001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 27440.680504322052
Done logging...



Beginning logging procedure...
Timestep 4720001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 27501.011663
Train_EnvstepsSoFar : 4790001
Train_AverageReturn : 18.23
Train_BestReturn : 18.35
TimeSinceStart : 27567.289910316467
Done logging...



Beginning logging procedure...
Timestep 4800001
mean reward (100 episodes) 18.280000
best mean reward 18.350000
running time 27627.181970
Train_EnvstepsSoFar : 4720001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 27418.351519823074
Done logging...



Beginning logging procedure...
Timestep 4730001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 27478.691363
Train_EnvstepsSoFar : 4720001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 27501.011662960052
Done logging...



Beginning logging procedure...
Timestep 4730001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 27561.730273
Train_EnvstepsSoFar : 4800001
Train_AverageReturn : 18.28
Train_BestReturn : 18.35
TimeSinceStart : 27627.18196964264
Done logging...



Beginning logging procedure...
Timestep 4810001
mean reward (100 episodes) 18.280000
best mean reward 18.350000
running time 27686.986947
Train_EnvstepsSoFar : 4730001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 27478.6913626194
Done logging...



Beginning logging procedure...
Timestep 4740001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 27538.682859
Train_EnvstepsSoFar : 4730001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 27561.730273246765
Done logging...



Beginning logging procedure...
Timestep 4740001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 27622.416850
Train_EnvstepsSoFar : 4810001
Train_AverageReturn : 18.28
Train_BestReturn : 18.35
TimeSinceStart : 27686.986946821213
Done logging...



Beginning logging procedure...
Timestep 4820001
mean reward (100 episodes) 18.270000
best mean reward 18.350000
running time 27746.871043
Train_EnvstepsSoFar : 4740001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 27538.682859182358
Done logging...



Beginning logging procedure...
Timestep 4750001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 27598.562238
Train_EnvstepsSoFar : 4740001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 27622.416850090027
Done logging...



Beginning logging procedure...
Timestep 4750001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 27683.237611
Train_EnvstepsSoFar : 4820001
Train_AverageReturn : 18.27
Train_BestReturn : 18.35
TimeSinceStart : 27746.871042728424
Done logging...



Beginning logging procedure...
Timestep 4830001
mean reward (100 episodes) 18.280000
best mean reward 18.350000
running time 27807.035145
Train_EnvstepsSoFar : 4750001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 27598.562237739563
Done logging...



Beginning logging procedure...
Timestep 4760001
mean reward (100 episodes) -20.740000
best mean reward -20.030000
running time 27659.051785
Train_EnvstepsSoFar : 4750001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 27683.237611055374
Done logging...



Beginning logging procedure...
Timestep 4760001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 27744.183829
Train_EnvstepsSoFar : 4830001
Train_AverageReturn : 18.28
Train_BestReturn : 18.35
TimeSinceStart : 27807.035145282745
Done logging...



Beginning logging procedure...
Timestep 4840001
mean reward (100 episodes) 18.290000
best mean reward 18.350000
running time 27866.945961
Train_EnvstepsSoFar : 4760001
Train_AverageReturn : -20.74
Train_BestReturn : -20.03
TimeSinceStart : 27659.05178451538
Done logging...



Beginning logging procedure...
Timestep 4770001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 27719.045274
Train_EnvstepsSoFar : 4760001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 27744.183829307556
Done logging...



Beginning logging procedure...
Timestep 4770001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 27805.174585
Train_EnvstepsSoFar : 4840001
Train_AverageReturn : 18.29
Train_BestReturn : 18.35
TimeSinceStart : 27866.945960998535
Done logging...



Beginning logging procedure...
Timestep 4850001
mean reward (100 episodes) 18.280000
best mean reward 18.350000
running time 27926.912467
Train_EnvstepsSoFar : 4770001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 27719.045273542404
Done logging...



Beginning logging procedure...
Timestep 4780001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 27779.406973
Train_EnvstepsSoFar : 4770001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 27805.17458486557
Done logging...



Beginning logging procedure...
Timestep 4780001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 27866.128058
Train_EnvstepsSoFar : 4850001
Train_AverageReturn : 18.28
Train_BestReturn : 18.35
TimeSinceStart : 27926.91246652603
Done logging...



Beginning logging procedure...
Timestep 4860001
mean reward (100 episodes) 18.210000
best mean reward 18.350000
running time 27986.272792
Train_EnvstepsSoFar : 4780001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 27779.406972646713
Done logging...



Beginning logging procedure...
Timestep 4790001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 27838.977834
Train_EnvstepsSoFar : 4780001
Train_AverageReturn : -20.76
Train_BestReturn : -18.29
TimeSinceStart : 27866.128058433533
Done logging...



Beginning logging procedure...
Timestep 4790001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 27926.911914
Train_EnvstepsSoFar : 4860001
Train_AverageReturn : 18.21
Train_BestReturn : 18.35
TimeSinceStart : 27986.27279162407
Done logging...



Beginning logging procedure...
Timestep 4870001
mean reward (100 episodes) 18.200000
best mean reward 18.350000
running time 28046.207459
Train_EnvstepsSoFar : 4790001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 27838.97783446312
Done logging...



Beginning logging procedure...
Timestep 4800001
mean reward (100 episodes) -20.810000
best mean reward -20.030000
running time 27899.459141
Train_EnvstepsSoFar : 4790001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 27926.911913633347
Done logging...



Beginning logging procedure...
Timestep 4800001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 27987.101430
Train_EnvstepsSoFar : 4870001
Train_AverageReturn : 18.2
Train_BestReturn : 18.35
TimeSinceStart : 28046.207459449768
Done logging...



Beginning logging procedure...
Timestep 4880001
mean reward (100 episodes) 18.300000
best mean reward 18.350000
running time 28105.811474
Train_EnvstepsSoFar : 4800001
Train_AverageReturn : -20.81
Train_BestReturn : -20.03
TimeSinceStart : 27899.459141492844
Done logging...



Beginning logging procedure...
Timestep 4810001
mean reward (100 episodes) -20.810000
best mean reward -20.030000
running time 27960.021438
Train_EnvstepsSoFar : 4800001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 27987.101430416107
Done logging...



Beginning logging procedure...
Timestep 4810001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 28047.987952
Train_EnvstepsSoFar : 4880001
Train_AverageReturn : 18.3
Train_BestReturn : 18.35
TimeSinceStart : 28105.811474084854
Done logging...



Beginning logging procedure...
Timestep 4890001
mean reward (100 episodes) 18.380000
best mean reward 18.380000
running time 28165.960991
Train_EnvstepsSoFar : 4810001
Train_AverageReturn : -20.81
Train_BestReturn : -20.03
TimeSinceStart : 27960.021438360214
Done logging...



Beginning logging procedure...
Timestep 4820001
mean reward (100 episodes) -20.790000
best mean reward -20.030000
running time 28020.330661
Train_EnvstepsSoFar : 4810001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 28047.987951517105
Done logging...



Beginning logging procedure...
Timestep 4820001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 28108.058256
Train_EnvstepsSoFar : 4890001
Train_AverageReturn : 18.38
Train_BestReturn : 18.38
TimeSinceStart : 28165.9609913826
Done logging...



Beginning logging procedure...
Timestep 4900001
mean reward (100 episodes) 18.290000
best mean reward 18.380000
running time 28226.380778
Train_EnvstepsSoFar : 4820001
Train_AverageReturn : -20.79
Train_BestReturn : -20.03
TimeSinceStart : 28020.33066058159
Done logging...



Beginning logging procedure...
Timestep 4830001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 28081.090025
Train_EnvstepsSoFar : 4820001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 28108.058255672455
Done logging...



Beginning logging procedure...
Timestep 4830001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 28168.399038
Train_EnvstepsSoFar : 4900001
Train_AverageReturn : 18.29
Train_BestReturn : 18.38
TimeSinceStart : 28226.380777597427
Done logging...



Beginning logging procedure...
Timestep 4910001
mean reward (100 episodes) 18.250000
best mean reward 18.380000
running time 28286.207479
Train_EnvstepsSoFar : 4830001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 28081.09002494812
Done logging...



Beginning logging procedure...
Timestep 4840001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 28141.270057
Train_EnvstepsSoFar : 4830001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 28168.3990380764
Done logging...



Beginning logging procedure...
Timestep 4840001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 28229.663994
Train_EnvstepsSoFar : 4910001
Train_AverageReturn : 18.25
Train_BestReturn : 18.38
TimeSinceStart : 28286.207478761673
Done logging...



Beginning logging procedure...
Timestep 4920001
mean reward (100 episodes) 18.170000
best mean reward 18.380000
running time 28345.780531
Train_EnvstepsSoFar : 4840001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 28141.27005672455
Done logging...



Beginning logging procedure...
Timestep 4850001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 28200.546327
Train_EnvstepsSoFar : 4840001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 28229.663994073868
Done logging...



Beginning logging procedure...
Timestep 4850001
mean reward (100 episodes) -20.880000
best mean reward -18.290000
running time 28290.414247
Train_EnvstepsSoFar : 4920001
Train_AverageReturn : 18.17
Train_BestReturn : 18.38
TimeSinceStart : 28345.780530929565
Done logging...



Beginning logging procedure...
Timestep 4930001
mean reward (100 episodes) 18.370000
best mean reward 18.380000
running time 28405.586709
Train_EnvstepsSoFar : 4850001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 28200.546327114105
Done logging...



Beginning logging procedure...
Timestep 4860001
mean reward (100 episodes) -20.800000
best mean reward -20.030000
running time 28261.098566
Train_EnvstepsSoFar : 4850001
Train_AverageReturn : -20.88
Train_BestReturn : -18.29
TimeSinceStart : 28290.414246559143
Done logging...



Beginning logging procedure...
Timestep 4860001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 28351.447179
Train_EnvstepsSoFar : 4930001
Train_AverageReturn : 18.37
Train_BestReturn : 18.38
TimeSinceStart : 28405.586708784103
Done logging...



Beginning logging procedure...
Timestep 4940001
mean reward (100 episodes) 18.480000
best mean reward 18.480000
running time 28465.498794
Train_EnvstepsSoFar : 4860001
Train_AverageReturn : -20.8
Train_BestReturn : -20.03
TimeSinceStart : 28261.09856557846
Done logging...



Beginning logging procedure...
Timestep 4870001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 28321.777474
Train_EnvstepsSoFar : 4860001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 28351.447178840637
Done logging...



Beginning logging procedure...
Timestep 4870001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 28411.998613
Train_EnvstepsSoFar : 4940001
Train_AverageReturn : 18.48
Train_BestReturn : 18.48
TimeSinceStart : 28465.49879384041
Done logging...



Beginning logging procedure...
Timestep 4950001
mean reward (100 episodes) 18.490000
best mean reward 18.490000
running time 28525.265543
Train_EnvstepsSoFar : 4870001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 28321.777474164963
Done logging...



Beginning logging procedure...
Timestep 4880001
mean reward (100 episodes) -20.850000
best mean reward -20.030000
running time 28382.404259
Train_EnvstepsSoFar : 4870001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 28411.998612880707
Done logging...



Beginning logging procedure...
Timestep 4880001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 28472.654727
Train_EnvstepsSoFar : 4950001
Train_AverageReturn : 18.49
Train_BestReturn : 18.49
TimeSinceStart : 28525.265543222427
Done logging...



Beginning logging procedure...
Timestep 4960001
mean reward (100 episodes) 18.530000
best mean reward 18.530000
running time 28584.686354
Train_EnvstepsSoFar : 4880001
Train_AverageReturn : -20.85
Train_BestReturn : -20.03
TimeSinceStart : 28382.404259204865
Done logging...



Beginning logging procedure...
Timestep 4890001
mean reward (100 episodes) -20.820000
best mean reward -20.030000
running time 28443.094770
Train_EnvstepsSoFar : 4880001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 28472.654727458954
Done logging...



Beginning logging procedure...
Timestep 4890001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 28533.331300
Train_EnvstepsSoFar : 4960001
Train_AverageReturn : 18.53
Train_BestReturn : 18.53
TimeSinceStart : 28584.68635392189
Done logging...



Beginning logging procedure...
Timestep 4970001
mean reward (100 episodes) 18.470000
best mean reward 18.530000
running time 28644.741775
Train_EnvstepsSoFar : 4890001
Train_AverageReturn : -20.82
Train_BestReturn : -20.03
TimeSinceStart : 28443.0947701931
Done logging...



Beginning logging procedure...
Timestep 4900001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 28504.016970
Train_EnvstepsSoFar : 4890001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 28533.331300258636
Done logging...



Beginning logging procedure...
Timestep 4900001
mean reward (100 episodes) -20.800000
best mean reward -18.290000
running time 28593.246553
Train_EnvstepsSoFar : 4970001
Train_AverageReturn : 18.47
Train_BestReturn : 18.53
TimeSinceStart : 28644.741775274277
Done logging...



Beginning logging procedure...
Timestep 4980001
mean reward (100 episodes) 18.530000
best mean reward 18.530000
running time 28704.818111
Train_EnvstepsSoFar : 4900001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 28504.016970396042
Done logging...



Beginning logging procedure...
Timestep 4910001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 28564.415261
Train_EnvstepsSoFar : 4900001
Train_AverageReturn : -20.8
Train_BestReturn : -18.29
TimeSinceStart : 28593.246552944183
Done logging...



Beginning logging procedure...
Timestep 4910001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 28652.998557
Train_EnvstepsSoFar : 4980001
Train_AverageReturn : 18.53
Train_BestReturn : 18.53
TimeSinceStart : 28704.81811094284
Done logging...



Beginning logging procedure...
Timestep 4990001
mean reward (100 episodes) 18.410000
best mean reward 18.530000
running time 28764.803560
Train_EnvstepsSoFar : 4910001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 28564.415261268616
Done logging...



Beginning logging procedure...
Timestep 4920001
mean reward (100 episodes) -20.780000
best mean reward -20.030000
running time 28624.519899
Train_EnvstepsSoFar : 4910001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 28652.998557329178
Done logging...



Beginning logging procedure...
Timestep 4920001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 28713.642061
Train_EnvstepsSoFar : 4990001
Train_AverageReturn : 18.41
Train_BestReturn : 18.53
TimeSinceStart : 28764.803559541702
Done logging...



Beginning logging procedure...
Timestep 5000001
mean reward (100 episodes) 18.320000
best mean reward 18.530000
running time 28824.315349
Train_EnvstepsSoFar : 4920001
Train_AverageReturn : -20.78
Train_BestReturn : -20.03
TimeSinceStart : 28624.51989889145
Done logging...



Beginning logging procedure...
Timestep 4930001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 28684.950280
Train_EnvstepsSoFar : 4920001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 28713.642060518265
Done logging...



Beginning logging procedure...
Timestep 4930001
mean reward (100 episodes) -20.780000
best mean reward -18.290000
running time 28775.055069
Train_EnvstepsSoFar : 5000001
Train_AverageReturn : 18.32
Train_BestReturn : 18.53
TimeSinceStart : 28824.315348625183
Done logging...



Beginning logging procedure...
Timestep 5010001
mean reward (100 episodes) 18.310000
best mean reward 18.530000
running time 28884.361096
Train_EnvstepsSoFar : 4930001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 28684.950280189514
Done logging...



Beginning logging procedure...
Timestep 4940001
mean reward (100 episodes) -20.760000
best mean reward -20.030000
running time 28745.305548
Train_EnvstepsSoFar : 4930001
Train_AverageReturn : -20.78
Train_BestReturn : -18.29
TimeSinceStart : 28775.055068731308
Done logging...



Beginning logging procedure...
Timestep 4940001
mean reward (100 episodes) -20.790000
best mean reward -18.290000
running time 28835.410791
Train_EnvstepsSoFar : 5010001
Train_AverageReturn : 18.31
Train_BestReturn : 18.53
TimeSinceStart : 28884.361095666885
Done logging...



Beginning logging procedure...
Timestep 5020001
mean reward (100 episodes) 18.360000
best mean reward 18.530000
running time 28944.361042
Train_EnvstepsSoFar : 4940001
Train_AverageReturn : -20.76
Train_BestReturn : -20.03
TimeSinceStart : 28745.305547952652
Done logging...



Beginning logging procedure...
Timestep 4950001
mean reward (100 episodes) -20.770000
best mean reward -20.030000
running time 28804.469403
Train_EnvstepsSoFar : 4940001
Train_AverageReturn : -20.79
Train_BestReturn : -18.29
TimeSinceStart : 28835.410790920258
Done logging...



Beginning logging procedure...
Timestep 4950001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 28896.318132
Train_EnvstepsSoFar : 5020001
Train_AverageReturn : 18.36
Train_BestReturn : 18.53
TimeSinceStart : 28944.361042022705
Done logging...



Beginning logging procedure...
Timestep 5030001
mean reward (100 episodes) 18.380000
best mean reward 18.530000
running time 29004.137298
Train_EnvstepsSoFar : 4950001
Train_AverageReturn : -20.77
Train_BestReturn : -20.03
TimeSinceStart : 28804.46940255165
Done logging...



Beginning logging procedure...
Timestep 4960001
mean reward (100 episodes) -20.720000
best mean reward -20.030000
running time 28864.806806
Train_EnvstepsSoFar : 4950001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 28896.318132400513
Done logging...



Beginning logging procedure...
Timestep 4960001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 28957.010296
Train_EnvstepsSoFar : 5030001
Train_AverageReturn : 18.38
Train_BestReturn : 18.53
TimeSinceStart : 29004.13729786873
Done logging...



Beginning logging procedure...
Timestep 5040001
mean reward (100 episodes) 18.360000
best mean reward 18.530000
running time 29064.168293
Train_EnvstepsSoFar : 4960001
Train_AverageReturn : -20.72
Train_BestReturn : -20.03
TimeSinceStart : 28864.806805610657
Done logging...



Beginning logging procedure...
Timestep 4970001
mean reward (100 episodes) -20.650000
best mean reward -20.030000
running time 28924.464304
Train_EnvstepsSoFar : 4960001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 28957.0102956295
Done logging...



Beginning logging procedure...
Timestep 4970001
mean reward (100 episodes) -20.820000
best mean reward -18.290000
running time 29017.498717
Train_EnvstepsSoFar : 5040001
Train_AverageReturn : 18.36
Train_BestReturn : 18.53
TimeSinceStart : 29064.168293476105
Done logging...



Beginning logging procedure...
Timestep 5050001
mean reward (100 episodes) 18.370000
best mean reward 18.530000
running time 29124.000088
Train_EnvstepsSoFar : 4970001
Train_AverageReturn : -20.65
Train_BestReturn : -20.03
TimeSinceStart : 28924.46430373192
Done logging...



Beginning logging procedure...
Timestep 4980001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 28985.198858
Train_EnvstepsSoFar : 5050001
Train_AverageReturn : 18.37
Train_BestReturn : 18.53
TimeSinceStart : 29124.000087976456
Done logging...



Beginning logging procedure...
Timestep 5060001
mean reward (100 episodes) 18.310000
best mean reward 18.530000
running time 29183.521263
Train_EnvstepsSoFar : 4970001
Train_AverageReturn : -20.82
Train_BestReturn : -18.29
TimeSinceStart : 29017.498717069626
Done logging...



Beginning logging procedure...
Timestep 4980001
mean reward (100 episodes) -20.850000
best mean reward -18.290000
running time 29078.250493
Train_EnvstepsSoFar : 4980001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 28985.198857545853
Done logging...



Beginning logging procedure...
Timestep 4990001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 29045.728608
Train_EnvstepsSoFar : 5060001
Train_AverageReturn : 18.31
Train_BestReturn : 18.53
TimeSinceStart : 29183.52126288414
Done logging...



Beginning logging procedure...
Timestep 5070001
mean reward (100 episodes) 18.290000
best mean reward 18.530000
running time 29243.013588
Train_EnvstepsSoFar : 4980001
Train_AverageReturn : -20.85
Train_BestReturn : -18.29
TimeSinceStart : 29078.25049328804
Done logging...



Beginning logging procedure...
Timestep 4990001
mean reward (100 episodes) -20.870000
best mean reward -18.290000
running time 29138.619452
Train_EnvstepsSoFar : 4990001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 29045.72860789299
Done logging...



Beginning logging procedure...
Timestep 5000001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 29106.295065
Train_EnvstepsSoFar : 5070001
Train_AverageReturn : 18.29
Train_BestReturn : 18.53
TimeSinceStart : 29243.01358795166
Done logging...



Beginning logging procedure...
Timestep 5080001
mean reward (100 episodes) 18.260000
best mean reward 18.530000
running time 29302.259135
Train_EnvstepsSoFar : 4990001
Train_AverageReturn : -20.87
Train_BestReturn : -18.29
TimeSinceStart : 29138.6194524765
Done logging...



Beginning logging procedure...
Timestep 5000001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 29198.982520
Train_EnvstepsSoFar : 5000001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 29106.295064926147
Done logging...



Beginning logging procedure...
Timestep 5010001
mean reward (100 episodes) -20.730000
best mean reward -20.030000
running time 29166.659200
Train_EnvstepsSoFar : 5080001
Train_AverageReturn : 18.26
Train_BestReturn : 18.53
TimeSinceStart : 29302.259135246277
Done logging...



Beginning logging procedure...
Timestep 5090001
mean reward (100 episodes) 18.340000
best mean reward 18.530000
running time 29361.861147
Train_EnvstepsSoFar : 5000001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 29198.982520103455
Done logging...



Beginning logging procedure...
Timestep 5010001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 29259.540289
Train_EnvstepsSoFar : 5010001
Train_AverageReturn : -20.73
Train_BestReturn : -20.03
TimeSinceStart : 29166.65919971466
Done logging...



Beginning logging procedure...
Timestep 5020001
mean reward (100 episodes) -20.690000
best mean reward -20.030000
running time 29226.278466
Train_EnvstepsSoFar : 5090001
Train_AverageReturn : 18.34
Train_BestReturn : 18.53
TimeSinceStart : 29361.86114668846
Done logging...



Beginning logging procedure...
Timestep 5100001
mean reward (100 episodes) 18.380000
best mean reward 18.530000
running time 29421.639330
Train_EnvstepsSoFar : 5010001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 29259.54028916359
Done logging...



Beginning logging procedure...
Timestep 5020001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 29319.498210
Train_EnvstepsSoFar : 5020001
Train_AverageReturn : -20.69
Train_BestReturn : -20.03
TimeSinceStart : 29226.278465747833
Done logging...



Beginning logging procedure...
Timestep 5030001
mean reward (100 episodes) -20.680000
best mean reward -20.030000
running time 29286.508113
Train_EnvstepsSoFar : 5100001
Train_AverageReturn : 18.38
Train_BestReturn : 18.53
TimeSinceStart : 29421.639330148697
Done logging...



Beginning logging procedure...
Timestep 5110001
mean reward (100 episodes) 18.440000
best mean reward 18.530000
running time 29481.165130
Train_EnvstepsSoFar : 5020001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 29319.49820971489
Done logging...



Beginning logging procedure...
Timestep 5030001
mean reward (100 episodes) -20.860000
best mean reward -18.290000
running time 29380.306949
Train_EnvstepsSoFar : 5030001
Train_AverageReturn : -20.68
Train_BestReturn : -20.03
TimeSinceStart : 29286.50811266899
Done logging...



Beginning logging procedure...
Timestep 5040001
mean reward (100 episodes) -20.620000
best mean reward -20.030000
running time 29346.864008
Train_EnvstepsSoFar : 5110001
Train_AverageReturn : 18.44
Train_BestReturn : 18.53
TimeSinceStart : 29481.16512966156
Done logging...



Beginning logging procedure...
Timestep 5120001
mean reward (100 episodes) 18.360000
best mean reward 18.530000
running time 29540.849197
Train_EnvstepsSoFar : 5030001
Train_AverageReturn : -20.86
Train_BestReturn : -18.29
TimeSinceStart : 29380.306948900223
Done logging...



Beginning logging procedure...
Timestep 5040001
mean reward (100 episodes) -20.840000
best mean reward -18.290000
running time 29440.817151
Train_EnvstepsSoFar : 5040001
Train_AverageReturn : -20.62
Train_BestReturn : -20.03
TimeSinceStart : 29346.864008188248
Done logging...



Beginning logging procedure...
Timestep 5050001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 29406.765188
Train_EnvstepsSoFar : 5120001
Train_AverageReturn : 18.36
Train_BestReturn : 18.53
TimeSinceStart : 29540.849197387695
Done logging...



Beginning logging procedure...
Timestep 5130001
mean reward (100 episodes) 18.310000
best mean reward 18.530000
running time 29600.951715
Train_EnvstepsSoFar : 5040001
Train_AverageReturn : -20.84
Train_BestReturn : -18.29
TimeSinceStart : 29440.817150592804
Done logging...



Beginning logging procedure...
Timestep 5050001
mean reward (100 episodes) -20.830000
best mean reward -18.290000
running time 29500.996845
Train_EnvstepsSoFar : 5050001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 29406.765188217163
Done logging...



Beginning logging procedure...
Timestep 5060001
mean reward (100 episodes) -20.710000
best mean reward -20.030000
running time 29467.039378
Train_EnvstepsSoFar : 5130001
Train_AverageReturn : 18.31
Train_BestReturn : 18.53
TimeSinceStart : 29600.95171546936
Done logging...



Beginning logging procedure...
Timestep 5140001
mean reward (100 episodes) 18.280000
best mean reward 18.530000
running time 29660.598823
Train_EnvstepsSoFar : 5050001
Train_AverageReturn : -20.83
Train_BestReturn : -18.29
TimeSinceStart : 29500.996844530106
Done logging...



Beginning logging procedure...
Timestep 5060001
mean reward (100 episodes) -20.810000
best mean reward -18.290000
running time 29561.496565
Train_EnvstepsSoFar : 5060001
Train_AverageReturn : -20.71
Train_BestReturn : -20.03
TimeSinceStart : 29467.0393781662
Done logging...



Beginning logging procedure...
Timestep 5070001
mean reward (100 episodes) -20.660000
best mean reward -20.030000
running time 29527.313513
Train_EnvstepsSoFar : 5140001
Train_AverageReturn : 18.28
Train_BestReturn : 18.53
TimeSinceStart : 29660.59882259369
Done logging...



Beginning logging procedure...
Timestep 5150001
mean reward (100 episodes) 18.320000
best mean reward 18.530000
running time 29720.283996
Train_EnvstepsSoFar : 5060001
Train_AverageReturn : -20.81
Train_BestReturn : -18.29
TimeSinceStart : 29561.49656534195
Done logging...



Beginning logging procedure...
Timestep 5070001
mean reward (100 episodes) -20.770000
best mean reward -18.290000
running time 29621.647872
Train_EnvstepsSoFar : 5070001
Train_AverageReturn : -20.66
Train_BestReturn : -20.03
TimeSinceStart : 29527.313512563705
Done logging...



Beginning logging procedure...
Timestep 5080001
mean reward (100 episodes) -20.700000
best mean reward -20.030000
running time 29587.758696
Train_EnvstepsSoFar : 5150001
Train_AverageReturn : 18.32
Train_BestReturn : 18.53
TimeSinceStart : 29720.283995628357
Done logging...



Beginning logging procedure...
Timestep 5160001
mean reward (100 episodes) 18.360000
best mean reward 18.530000
running time 29780.219760
Train_EnvstepsSoFar : 5070001
Train_AverageReturn : -20.77
Train_BestReturn : -18.29
TimeSinceStart : 29621.64787197113
Done logging...



Beginning logging procedure...
Timestep 5080001
mean reward (100 episodes) -20.750000
best mean reward -18.290000
running time 29681.716408
Train_EnvstepsSoFar : 5080001
Train_AverageReturn : -20.7
Train_BestReturn : -20.03
TimeSinceStart : 29587.758696079254
Done logging...



Beginning logging procedure...
Timestep 5090001
mean reward (100 episodes) -20.670000
best mean reward -20.030000
running time 29647.784272
Train_EnvstepsSoFar : 5160001
Train_AverageReturn : 18.36
Train_BestReturn : 18.53
TimeSinceStart : 29780.21976041794
Done logging...



Beginning logging procedure...
Timestep 5170001
mean reward (100 episodes) 18.350000
best mean reward 18.530000
running time 29840.149278
Train_EnvstepsSoFar : 5080001
Train_AverageReturn : -20.75
Train_BestReturn : -18.29
TimeSinceStart : 29681.71640777588
Done logging...



Beginning logging procedure...
Timestep 5090001
mean reward (100 episodes) -20.760000
best mean reward -18.290000
running time 29742.289623
